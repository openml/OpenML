INSERT INTO `estimation_procedure_type` (`name`, `description`) VALUES
('crossvalidation', 'Cross-validation is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a test set to evaluate it. <br><br>\r\n\r\nIn k-fold cross-validation, the original sample is randomly partitioned into k equal size subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k-1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged (or otherwise combined) to produce a single estimation. The advantage of this method is that all observations are used for both training and validation, and each observation is used for validation exactly once.<br><br>\r\n\r\nFor classification problems, one typically uses stratified k-fold cross-validation, in which the folds are selected so that each fold contains roughly the same proportions of class labels.<br><br>\r\n\r\nIn repeated cross-validation, the cross-validation procedure is repeated n times, yielding n random partitions of the original sample. The n results are again  averaged (or otherwise combined) to produce a single estimation.<br><br>\r\n\r\nOpenML generates train-test splits given the number of folds and repeats, so that different users can evaluate their models with the same splits. Stratification is applied by default for classification problems (unless otherwise specified). The splits are given as part of the task description as an ARFF file with the row id, fold number, repeat number and the class (TRAIN or TEST). The uploaded predictions should be labeled with the fold and repeat number of the test instance, so that the results can be properly evaluated and aggregated. OpenML stores both the per fold/repeat results and the aggregated scores.'),
('customholdout', 'A custom holdout partitions a set of observations into a training set and a test set in a predefined way. This is typically done in order to compare the performance of different predictive algorithms on the same data, as part of  a data mining competition or by the researcher who first uses the dataset.\r\n\r\n'),
('holdout', 'Holdout or random subsampling is a technique to evaluate predictive models by partitioning the original sample into a training set to train the model, and a test set to evaluate it. <br>\r\n\r\nIn a k% holdout, the original sample is randomly partitioned into a test set containing k% of the input sample size, and a 1-k% training set. Sampling is done without replacement. This holdout is usually repeated n times, yielding n random partitions of the original sample. The n results are averaged (or otherwise combined) to produce a single estimation.<br><br>\r\n\r\nFor classification problems, one typically uses stratified sampling, so that the test set contains roughly the same proportions of class labels as the original sample.<br><br>\r\n\r\nOpenML generates train-test splits given the percentage size of the holdout and the number of repeats, so that different users can evaluate their models with the same splits. Stratification is applied by default for classification problems (unless otherwise specified). The splits are given as part of the task description as an ARFF file with the row id, fold number (0/1), repeat number and the class (TRAIN or TEST). The uploaded predictions should be labeled with the fold and repeat number of the test instance, so that the results can be properly evaluated and aggregated. OpenML stores both the per fold/repeat results and the aggregated scores.'),
('learningcurve', 'Description to be added'),
('leaveoneout', 'Leave-on-out is a special case of cross-validation where the number of folds equals the number of instances. Thus, models are always evaluated on one instance and trained on all others.<br><br>\r\n\r\nLeave-one-out is deterministic, bias-free, and does not require repeats or stratification. However, it is very computationally intensive and thus only advised for small data sets.<br><br>\r\n\r\nFor leave-one-out, OpenML does not provide a train-test split file, but does require that the uploaded predictions are labeled with the row id of the test instance, so that the results can be properly evaluated and aggregated. OpenML stores both the per fold/repeat results and the aggregated scores.'),
('testthentrain', 'Description to be added');
