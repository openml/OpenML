{
    "docs": [
        {
            "location": "/",
            "text": "An open, collaborative, \nfrictionless\n, automated machine learning environment.\n\n\n\n \nData sets\n automatically analyzed, annotated, and organized online\n\n\n\n \nMachine learning pipelines\n automatically shared from many libraries.\n\n\n\n \nExtensive APIs\n to integrate OpenML into your own tools and scripts\n\n\n\n \nReproducible results\n (e.g. models, evaluations) for easy comparison and reuse\n\n\n\n Collaborate in real time, right from your existing tools\n\n\n\n Make your work more visible, reusable, and easily citable\n\n\n\n Open source tools to automate experimentation and model building\n\n\n\nConcepts\n\u00b6\n\n\nOpenML operates on a number of core concepts which are important to understand:  \n\n\n Datasets\n\nDatasets are pretty straight-forward. They simply consist of a number of rows, also called \ninstances\n, usually in tabular form.\n\n\nExample: The \niris dataset\n\n\n Tasks\n\nA task consists of a dataset, together with a machine learning task to perform, such as classification or clustering and an evaluation method. For\nsupervised tasks, this also specifies the target column in the data.\n\n\nExample: \nClassifying different iris species\n from other attributes and evaluate using 10-fold cross-validation.\n\n\n Flows\n\nA flow identifies a particular machine learning algorithm from a particular library or framework such as Weka, mlr or scikit-learn.\n\n\nExample: \nWEKA's RandomForest\n\n\n Runs\n\nA run is a particular flow, that is algorithm, with a particular parameter setting, applied to a particular task.\n\n\nExample: \nClassifying irises with WEKA's RandomForest\n\n\nData\n\u00b6\n\n\nYou can upload and download datasets through the \nwebsite\n, or \nAPI\n. Data hosted\nelsewhere can be referenced by URL.\n\n\nData consists of columns, also known as features or covariates, each of\nwhich is either numeric, nominal or a string, and has a unique name. A column\ncan also contain any number of missing values.\n\n\n\n\nMost datasets have a \"default target attribute\" which denotes the column that\nis usually the target, also known as dependent variable, in supervised learning tasks. The default\ntarget column is denoted by \"(target)\" in the web interface. Not all datasets\nhave such a column, though, and a supervised task can pick any column as the\ntarget (as long as it is of the appropriate type).\n\n\n\nExample: The default target variable for the \nMNIST\n data is to predict the class from\npixel values, OpenML also allows you to create a task that tries to predict the\nvalue of pixel257 given all the other pixel values and the class column. As such,\nthe class is also considered a feature in OpenML terminology.\n\n\n\nOpenML automatically analyzes the data, checks for problems, visualizes it,\nand computes \ndata\ncharacteristics\n, also called data qualities (including simple ones like number of features, but also\nmore complex statistics like kurtosis or the AUC of a decision tree of depth 3).\nThese data qualities can be useful to find and compare datasets.\n\n\n\nEvery dataset gets a dedicated page with all known information (check out\n\nzoo\n), including a wiki, visualizations, statistics, user\ndiscussions, and the \ntasks\n in which it is used.\n\n\n\n\n\nInfo\n\n\nOpenML currently only supports uploading of ARFF files. We aim to extend this in the near future, and allow conversions between the main data types.\n\n\n\n\nDataset ID and versions\n\u00b6\n\n\nA dataset can be uniquely identified by its dataset ID, which you can find\nin the URL of the dataset page, such as 62 for \nzoo\n. Each\ndataset also has a name, but several dataset can have the same name. When several datasets\nhave the same name, they are called \"versions\" of the same dataset (although\nthat is not necessarily true). The version number is assigned according to the order\nof upload. Different versions of a dataset can be accessed through the drop\ndown menu at the top right of the dataset page.\n\n\n\n\nDataset status\n\u00b6\n\n\nEach dataset has a status, which can be \"active\", \"deactivated\" or\n\"in_preparation\". When you upload a dataset, it will be marked \"in_preparation\"\nuntil it is approved by a site administrator.  Once it is approved, the dataset\nwill become \"active\". If a severe issue has been found with a dataset, it can\nbecome \"deactivated\". By default, the search will only display datasets that are\n\"active\", but you can access and download datasets with any status.\n\n\nIgnored features\n\u00b6\n\n\nFeatures in datasets can be tagged as \"ignored\" or \"row id\". Those features will not be\nconsidered by programming interfaces, and excluded from any tasks.\n\n\nTasks\n\u00b6\n\n\nTasks describe what to do with the data. OpenML covers several \ntask types\n, such as classification and\nclustering. You can \ncreate tasks\n\nonline.\n\n\nTasks are little containers including the data and other information such as\ntrain/test splits, and define what needs to be returned.\n\n\nTasks are machine-readable so that machine learning environments know what\nto do, and you can focus on finding the best algorithm. You can run algorithms\non your own machine(s) and upload the results. OpenML evaluates and organizes\nall solutions online.\n\n\n\n\nTasks are \nreal-time, collaborative\n data mining challenges (e.g. see\n\nthis one\n): you can study, discuss and learn from\nall submissions (code has to be shared), while OpenML keeps track of who was\nfirst.\n\n\n\n\n\nMore concretely, tasks specify the dataset, the kind of machine learning\ntask (i.e. regression), the target attribute (i.e. which column in the dataset\nshould be predicted), the number of splits for cross-validated evaluation and\nthe exact dataset splits, as well as an optional evaluation metric (i.e. mean\nsquared error). Given this specification, a task can be solved using any of the\nintegrated machine learning tools, like Weka, mlr and scikit-learn.\n\n\n\n\nNote\n\n\nYou can also supply hidden test sets for the evaluation of solutions. Novel ways of ranking solutions will be added in the near future.\n\n\n\n\nFlows\n\u00b6\n\n\nFlows are algorithms, workflows, or scripts solving tasks. You can upload\nthem through the \nwebsite\n, or \nAPIs\n.\nCode hosted elsewhere (e.g., GitHub) can be\nreferenced by URL, though typically they are generated automatically by machine learning environments.\n\n\nFlows contain all the information necessary to apply a particular workflow\nor algorithm to a new task. Usually a flow is specific to a task-type, i.e.\nyou can not run a classification model on a clustering task.\n\n\nEvery flow gets a dedicated page with all known information (check out \nWEKA's RandomForest\n), including a wiki, hyperparameters,\nevaluations on all tasks, and user discussions.\n\n\n\n\n\n\nNote\n\n\nEach flow specifies requirements and dependencies, and you need to install these locally to execute\na flow on a specific task. We aim to add support for VMs so that flows can be easily (re)run in any environment.\n\n\n\n\nRuns\n\u00b6\n\n\nRuns are applications of flows to a specific task. They are typically\nsubmitted automatically by machine learning\nenvironments (through the OpenML \nAPIs\n), with the goal of creating a\nreproducible experiment (though exactly reproducing experiments across machines\nmight not be possible because of changes in numeric libraries and operating\nsystems).\n\n\nOpenML organizes all runs online, linked to the underlying data, flows,\nparameter settings, people, and other details. OpenML also independently\nevaluates the results contained in the run given the provided predictions.\n\n\nYou can search and compare everyone's runs online, download all results into\nyour favorite machine learning environment, and relate evaluations to known\nproperties of the data and algorithms.\n\n\n\n\nOpenML stores and analyzes results in fine detail, up to the level of\nindividual instances.\n\n\n!!! Want to read more?\n    A more detailed description can be found \nin this blogpost\n.\n\n\nAuthentication\n\u00b6\n\n\nYou can download and inspect all datasets, tasks, flows and runs through the\nwebsite or the API without creating an account. However, if you want to upload\ndatasets or experiments, you need to \ncreate an account\n\nor sign in and \ncreate an API key\n.\nThis key can then be used with any of the \nOpenML APIs\n.\n\n\nIntegrations\n\u00b6\n\n\nOpenML is deeply integrated in several popular machine learning environments. Given a task, these integrations will\nautomatically download the data into the environments, allow you to run any\nalgorithm/flow, and automatically upload all runs.\n\n\n\n\nProgramming APIs\n\u00b6\n\n\nIf you want to integrate OpenML into your own tools, we offer several \nLanguage-specific APIs\n, so you can easily interact with\nOpenML to list, download and upload datasets, tasks, flows and runs.\n\n\nWith these APIs you can download a task, run an algorithm, and upload the\nresults in just a few lines of code.\n\n\nOpenML also offers a \nREST API\n which allows you to talk to OpenML directly.\n\n\n\n\nTags\n\u00b6\n\n\nDatasets, tasks, runs and flows can be assigned tags, either via the web\ninterface or the API. These tags can be used to search and annotated datasets.\nFor example the tag \nOpenML100\n refers to\nbenchmark machine learning algorithms used as a benchmark suite. Anyone can add\nor remove tags on any entity.\n\n\nStudies (under construction)\n\u00b6\n\n\nYou can combine datasets, flows and runs into studies, to collaborate with others online, or simply keep a log of your work.\n\n\nEach project gets its own page, which can be linked to publications so that others can find all the details online.\n\n\nTo link data sets, task, flows or runs to a certain study, you can use tags. By adding the tag \"study_XYZ\" the object will automatically be linked and appear on the page of study XYZ (with XYZ being the study id, which can e.g. be found in the URL of the study).\n\n\nCircles (under construction)\n\u00b6\n\n\nYou can create circles of trusted researchers in which data can be shared that is not yet ready for publication.",
            "title": "Get started"
        },
        {
            "location": "/#concepts",
            "text": "OpenML operates on a number of core concepts which are important to understand:     Datasets \nDatasets are pretty straight-forward. They simply consist of a number of rows, also called  instances , usually in tabular form.  Example: The  iris dataset   Tasks \nA task consists of a dataset, together with a machine learning task to perform, such as classification or clustering and an evaluation method. For\nsupervised tasks, this also specifies the target column in the data.  Example:  Classifying different iris species  from other attributes and evaluate using 10-fold cross-validation.   Flows \nA flow identifies a particular machine learning algorithm from a particular library or framework such as Weka, mlr or scikit-learn.  Example:  WEKA's RandomForest   Runs \nA run is a particular flow, that is algorithm, with a particular parameter setting, applied to a particular task.  Example:  Classifying irises with WEKA's RandomForest",
            "title": "Concepts"
        },
        {
            "location": "/#data",
            "text": "You can upload and download datasets through the  website , or  API . Data hosted\nelsewhere can be referenced by URL.  Data consists of columns, also known as features or covariates, each of\nwhich is either numeric, nominal or a string, and has a unique name. A column\ncan also contain any number of missing values.   Most datasets have a \"default target attribute\" which denotes the column that\nis usually the target, also known as dependent variable, in supervised learning tasks. The default\ntarget column is denoted by \"(target)\" in the web interface. Not all datasets\nhave such a column, though, and a supervised task can pick any column as the\ntarget (as long as it is of the appropriate type).  Example: The default target variable for the  MNIST  data is to predict the class from\npixel values, OpenML also allows you to create a task that tries to predict the\nvalue of pixel257 given all the other pixel values and the class column. As such,\nthe class is also considered a feature in OpenML terminology.  OpenML automatically analyzes the data, checks for problems, visualizes it,\nand computes  data\ncharacteristics , also called data qualities (including simple ones like number of features, but also\nmore complex statistics like kurtosis or the AUC of a decision tree of depth 3).\nThese data qualities can be useful to find and compare datasets.  Every dataset gets a dedicated page with all known information (check out zoo ), including a wiki, visualizations, statistics, user\ndiscussions, and the  tasks  in which it is used.   Info  OpenML currently only supports uploading of ARFF files. We aim to extend this in the near future, and allow conversions between the main data types.",
            "title": "Data"
        },
        {
            "location": "/#dataset-id-and-versions",
            "text": "A dataset can be uniquely identified by its dataset ID, which you can find\nin the URL of the dataset page, such as 62 for  zoo . Each\ndataset also has a name, but several dataset can have the same name. When several datasets\nhave the same name, they are called \"versions\" of the same dataset (although\nthat is not necessarily true). The version number is assigned according to the order\nof upload. Different versions of a dataset can be accessed through the drop\ndown menu at the top right of the dataset page.",
            "title": "Dataset ID and versions"
        },
        {
            "location": "/#dataset-status",
            "text": "Each dataset has a status, which can be \"active\", \"deactivated\" or\n\"in_preparation\". When you upload a dataset, it will be marked \"in_preparation\"\nuntil it is approved by a site administrator.  Once it is approved, the dataset\nwill become \"active\". If a severe issue has been found with a dataset, it can\nbecome \"deactivated\". By default, the search will only display datasets that are\n\"active\", but you can access and download datasets with any status.",
            "title": "Dataset status"
        },
        {
            "location": "/#ignored-features",
            "text": "Features in datasets can be tagged as \"ignored\" or \"row id\". Those features will not be\nconsidered by programming interfaces, and excluded from any tasks.",
            "title": "Ignored features"
        },
        {
            "location": "/#tasks",
            "text": "Tasks describe what to do with the data. OpenML covers several  task types , such as classification and\nclustering. You can  create tasks \nonline.  Tasks are little containers including the data and other information such as\ntrain/test splits, and define what needs to be returned.  Tasks are machine-readable so that machine learning environments know what\nto do, and you can focus on finding the best algorithm. You can run algorithms\non your own machine(s) and upload the results. OpenML evaluates and organizes\nall solutions online.   Tasks are  real-time, collaborative  data mining challenges (e.g. see this one ): you can study, discuss and learn from\nall submissions (code has to be shared), while OpenML keeps track of who was\nfirst.   More concretely, tasks specify the dataset, the kind of machine learning\ntask (i.e. regression), the target attribute (i.e. which column in the dataset\nshould be predicted), the number of splits for cross-validated evaluation and\nthe exact dataset splits, as well as an optional evaluation metric (i.e. mean\nsquared error). Given this specification, a task can be solved using any of the\nintegrated machine learning tools, like Weka, mlr and scikit-learn.   Note  You can also supply hidden test sets for the evaluation of solutions. Novel ways of ranking solutions will be added in the near future.",
            "title": "Tasks"
        },
        {
            "location": "/#flows",
            "text": "Flows are algorithms, workflows, or scripts solving tasks. You can upload\nthem through the  website , or  APIs .\nCode hosted elsewhere (e.g., GitHub) can be\nreferenced by URL, though typically they are generated automatically by machine learning environments.  Flows contain all the information necessary to apply a particular workflow\nor algorithm to a new task. Usually a flow is specific to a task-type, i.e.\nyou can not run a classification model on a clustering task.  Every flow gets a dedicated page with all known information (check out  WEKA's RandomForest ), including a wiki, hyperparameters,\nevaluations on all tasks, and user discussions.    Note  Each flow specifies requirements and dependencies, and you need to install these locally to execute\na flow on a specific task. We aim to add support for VMs so that flows can be easily (re)run in any environment.",
            "title": "Flows"
        },
        {
            "location": "/#runs",
            "text": "Runs are applications of flows to a specific task. They are typically\nsubmitted automatically by machine learning\nenvironments (through the OpenML  APIs ), with the goal of creating a\nreproducible experiment (though exactly reproducing experiments across machines\nmight not be possible because of changes in numeric libraries and operating\nsystems).  OpenML organizes all runs online, linked to the underlying data, flows,\nparameter settings, people, and other details. OpenML also independently\nevaluates the results contained in the run given the provided predictions.  You can search and compare everyone's runs online, download all results into\nyour favorite machine learning environment, and relate evaluations to known\nproperties of the data and algorithms.   OpenML stores and analyzes results in fine detail, up to the level of\nindividual instances.  !!! Want to read more?\n    A more detailed description can be found  in this blogpost .",
            "title": "Runs"
        },
        {
            "location": "/#authentication",
            "text": "You can download and inspect all datasets, tasks, flows and runs through the\nwebsite or the API without creating an account. However, if you want to upload\ndatasets or experiments, you need to  create an account \nor sign in and  create an API key .\nThis key can then be used with any of the  OpenML APIs .",
            "title": "Authentication"
        },
        {
            "location": "/#integrations",
            "text": "OpenML is deeply integrated in several popular machine learning environments. Given a task, these integrations will\nautomatically download the data into the environments, allow you to run any\nalgorithm/flow, and automatically upload all runs.",
            "title": "Integrations"
        },
        {
            "location": "/#programming-apis",
            "text": "If you want to integrate OpenML into your own tools, we offer several  Language-specific APIs , so you can easily interact with\nOpenML to list, download and upload datasets, tasks, flows and runs.  With these APIs you can download a task, run an algorithm, and upload the\nresults in just a few lines of code.  OpenML also offers a  REST API  which allows you to talk to OpenML directly.",
            "title": "Programming APIs"
        },
        {
            "location": "/#tags",
            "text": "Datasets, tasks, runs and flows can be assigned tags, either via the web\ninterface or the API. These tags can be used to search and annotated datasets.\nFor example the tag  OpenML100  refers to\nbenchmark machine learning algorithms used as a benchmark suite. Anyone can add\nor remove tags on any entity.",
            "title": "Tags"
        },
        {
            "location": "/#studies-under-construction",
            "text": "You can combine datasets, flows and runs into studies, to collaborate with others online, or simply keep a log of your work.  Each project gets its own page, which can be linked to publications so that others can find all the details online.  To link data sets, task, flows or runs to a certain study, you can use tags. By adding the tag \"study_XYZ\" the object will automatically be linked and appear on the page of study XYZ (with XYZ being the study id, which can e.g. be found in the URL of the study).",
            "title": "Studies (under construction)"
        },
        {
            "location": "/#circles-under-construction",
            "text": "You can create circles of trusted researchers in which data can be shared that is not yet ready for publication.",
            "title": "Circles (under construction)"
        },
        {
            "location": "/sklearn/",
            "text": "scikit-learn\n\u00b6\n\n\nOpenML is readily integrated with scikit-learn through the \nPython API\n.\n\n\n\n\nExample\n\n\nfrom\n \nsklearn\n \nimport\n \nensemble\n\n\nfrom\n \nopenml\n \nimport\n \ntasks\n,\n \nflows\n,\n \nRuns\n\n\n\ntask\n \n=\n \ntasks\n.\nget_task\n(\n3954\n)\n\n\nclf\n \n=\n \nensemble\n.\nRandomForestClassifier\n()\n\n\nflow\n \n=\n \nflows\n.\nsklearn_to_flow\n(\nclf\n)\n\n\nrun\n \n=\n \nruns\n.\nrun_flow_on_task\n(\ntask\n,\n \nflow\n)\n\n\nresult\n \n=\n \nrun\n.\npublish\n()\n\n\n\n\n\n\n\nKey features:  \n\n\n\n\nQuery and download OpenML datasets and use them however you like  \n\n\nBuild any sklearn estimator or pipeline and convert to OpenML flows  \n\n\nRun any flow on any task and save the experiment as run objects  \n\n\nUpload your runs for collaboration or publishing  \n\n\nQuery, download and reuse all shared runs  \n\n\n\n\nFor many more details and examples, see the \nPython tutorial\n.",
            "title": "scikit-learn"
        },
        {
            "location": "/sklearn/#scikit-learn",
            "text": "OpenML is readily integrated with scikit-learn through the  Python API .   Example  from   sklearn   import   ensemble  from   openml   import   tasks ,   flows ,   Runs  task   =   tasks . get_task ( 3954 )  clf   =   ensemble . RandomForestClassifier ()  flow   =   flows . sklearn_to_flow ( clf )  run   =   runs . run_flow_on_task ( task ,   flow )  result   =   run . publish ()    Key features:     Query and download OpenML datasets and use them however you like    Build any sklearn estimator or pipeline and convert to OpenML flows    Run any flow on any task and save the experiment as run objects    Upload your runs for collaboration or publishing    Query, download and reuse all shared runs     For many more details and examples, see the  Python tutorial .",
            "title": "scikit-learn"
        },
        {
            "location": "/mlr/",
            "text": "Machine Learning in R (mlr)\n\u00b6\n\n\nOpenML is readily integrated with mlr through the \nR API\n.\n\n\n\n\nExample\n\n\nlibrary\n(\nOpenML\n)\n\n\nlibrary\n(\nmlr\n)\n\n\ntask \n=\n getOMLTask\n(\n10\n)\n\nlrn \n=\n makeLearner\n(\n\"classif.rpart\"\n)\n\nrun \n=\n runTaskMlr\n(\ntask\n,\n lrn\n)\n\nrun.id \n=\n uploadOMLRun\n(\nrun\n)\n\n\n\n\n\n\n\nKey features:  \n\n\n\n\nQuery and download OpenML datasets and use them however you like  \n\n\nBuild any mlr learner, run it on any task and save the experiment as run objects  \n\n\nUpload your runs for collaboration or publishing  \n\n\nQuery, download and reuse all shared runs  \n\n\n\n\nFor many more details and examples, see the \nR tutorial\n.",
            "title": "mlr"
        },
        {
            "location": "/mlr/#machine-learning-in-r-mlr",
            "text": "OpenML is readily integrated with mlr through the  R API .   Example  library ( OpenML )  library ( mlr ) \n\ntask  =  getOMLTask ( 10 ) \nlrn  =  makeLearner ( \"classif.rpart\" ) \nrun  =  runTaskMlr ( task ,  lrn ) \nrun.id  =  uploadOMLRun ( run )    Key features:     Query and download OpenML datasets and use them however you like    Build any mlr learner, run it on any task and save the experiment as run objects    Upload your runs for collaboration or publishing    Query, download and reuse all shared runs     For many more details and examples, see the  R tutorial .",
            "title": "Machine Learning in R (mlr)"
        },
        {
            "location": "/Weka/",
            "text": "OpenML is integrated in the Weka (Waikato Environment for Knowledge Analysis) Experimenter and the Command Line Interface.\n\n\nInstallation\n\u00b6\n\n\nOpenML is available as a weka extension in the package manager:\n\n\n\n\nDownload the latest version\n (3.7.13 or higher).\n\n\nLaunch Weka, or start from commandline:\n\n\njava -jar weka.jar\n\n\n\n\n\n\nIf you need more memory (e.g. 1GB), start as follows:\n\n\njava -Xmx1G -jar weka.jar\n\n\n\n\n\n\nOpen the package manager (Under 'Tools')\n\n\nSelect package \nOpenmlWeka\n and click install. Afterwards, restart WEKA.\n\n\nFrom the Tools menu, open the 'OpenML Experimenter'.\n\n\n\n\nGraphical Interface\n\u00b6\n\n\n\n\nYou can solve OpenML Tasks in the Weka Experimenter, and automatically upload your experiments to OpenML (or store them locally).  \n\n\n\n\nFrom the Tools menu, open the 'OpenML Experimenter'.\n\n\nEnter your \nAPI key\n in the top field (log in first). You can also store this in a config file (see below).\n\n\nIn the 'Tasks' panel, click the 'Add New' button to add new tasks. Insert the task id's as comma-separated values (e.g., '1,2,3,4,5'). Use the search function on OpenML to find interesting tasks and click the ID icon to list the ID's. In the future this search will also be integrated in WEKA.\n\n\nAdd algorithms in the \"Algorithm\" panel.\n\n\nGo to the \"Run\" tab, and click on the \"Start\" button.\n\n\nThe experiment will be executed and sent to OpenML.org.\n\n\nThe runs will now appear on OpenML.org. You can follow their progress and check for errors on your profile page under 'Runs'.\n\n\n\n\nCommandLine Interface\n\u00b6\n\n\nThe Command Line interface is useful for running experiments automatically on a server, without using a GUI.\n\n\n\n\nCreate a config file called \nopenml.conf\n in a new directory called \n.openml\n in your home dir. It should contain the following line:\n\n\napi_key = YOUR_KEY\n\n\n\n\n\n\nExecute the following command:\n\n\njava -cp weka.jar openml.experiment.TaskBasedExperiment -T \n -C \n -- \n\n\n\n\n\n\nFor example, the following command will run Weka's J48 algorithm on Task 1:\n\n\njava -cp OpenWeka.beta.jar openml.experiment.TaskBasedExperiment -T 1 -C weka.classifiers.trees.J48\n\n\n\n\n\n\nThe following suffix will set some parameters of this classifier:\n\n\n-- -C 0.25 -M 2\n\n\n\n\n\n\n\n\nIssues\n\u00b6\n\n\nPlease report any bugs that you may encounter in the issue tracker: \nhttps://github.com/openml/openml-weka\n\nOr email to \nj.n.van.rijn@liacs.leidenuniv.nl",
            "title": "WEKA"
        },
        {
            "location": "/Weka/#installation",
            "text": "OpenML is available as a weka extension in the package manager:   Download the latest version  (3.7.13 or higher).  Launch Weka, or start from commandline:  java -jar weka.jar    If you need more memory (e.g. 1GB), start as follows:  java -Xmx1G -jar weka.jar    Open the package manager (Under 'Tools')  Select package  OpenmlWeka  and click install. Afterwards, restart WEKA.  From the Tools menu, open the 'OpenML Experimenter'.",
            "title": "Installation"
        },
        {
            "location": "/Weka/#graphical-interface",
            "text": "You can solve OpenML Tasks in the Weka Experimenter, and automatically upload your experiments to OpenML (or store them locally).     From the Tools menu, open the 'OpenML Experimenter'.  Enter your  API key  in the top field (log in first). You can also store this in a config file (see below).  In the 'Tasks' panel, click the 'Add New' button to add new tasks. Insert the task id's as comma-separated values (e.g., '1,2,3,4,5'). Use the search function on OpenML to find interesting tasks and click the ID icon to list the ID's. In the future this search will also be integrated in WEKA.  Add algorithms in the \"Algorithm\" panel.  Go to the \"Run\" tab, and click on the \"Start\" button.  The experiment will be executed and sent to OpenML.org.  The runs will now appear on OpenML.org. You can follow their progress and check for errors on your profile page under 'Runs'.",
            "title": "Graphical Interface"
        },
        {
            "location": "/Weka/#commandline-interface",
            "text": "The Command Line interface is useful for running experiments automatically on a server, without using a GUI.   Create a config file called  openml.conf  in a new directory called  .openml  in your home dir. It should contain the following line:  api_key = YOUR_KEY    Execute the following command:  java -cp weka.jar openml.experiment.TaskBasedExperiment -T   -C   --     For example, the following command will run Weka's J48 algorithm on Task 1:  java -cp OpenWeka.beta.jar openml.experiment.TaskBasedExperiment -T 1 -C weka.classifiers.trees.J48    The following suffix will set some parameters of this classifier:  -- -C 0.25 -M 2",
            "title": "CommandLine Interface"
        },
        {
            "location": "/Weka/#issues",
            "text": "Please report any bugs that you may encounter in the issue tracker:  https://github.com/openml/openml-weka \nOr email to  j.n.van.rijn@liacs.leidenuniv.nl",
            "title": "Issues"
        },
        {
            "location": "/MOA/",
            "text": "OpenML features extensive support for MOA. However currently this is implemented as a stand alone MOA compilation, using the latest version (as of May, 2014).\n\n\nDownload MOA for OpenML\n\n\nQuick Start\n\u00b6\n\n\n\n\n\n\nDownload the standalone MOA environment above.\n\n\nFind your \nAPI key\n in your profile (log in first). Create a config file called \nopenml.conf\n in a \n.openml\n directory in your home dir. It should contain the following lines:\n\n\napi_key = YOUR_KEY\n\n\n\n\n\n\nLaunch the JAR file by double clicking on it, or launch from command-line using the following command:\n\n\njava -cp openmlmoa.beta.jar moa.gui.GUI\n\n\n\n\n\n\nSelect the task \nmoa.tasks.openml.OpenmlDataStreamClassification\n to evaluate a classifier on an OpenML task, and send the results to OpenML.\n\n\nOptionally, you can generate new streams using the Bayesian Network Generator: select the \nmoa.tasks.WriteStreamToArff\n task, with \nmoa.streams.generators.BayesianNetworkGenerator\n.",
            "title": "MOA"
        },
        {
            "location": "/MOA/#quick-start",
            "text": "Download the standalone MOA environment above.  Find your  API key  in your profile (log in first). Create a config file called  openml.conf  in a  .openml  directory in your home dir. It should contain the following lines:  api_key = YOUR_KEY    Launch the JAR file by double clicking on it, or launch from command-line using the following command:  java -cp openmlmoa.beta.jar moa.gui.GUI    Select the task  moa.tasks.openml.OpenmlDataStreamClassification  to evaluate a classifier on an OpenML task, and send the results to OpenML.  Optionally, you can generate new streams using the Bayesian Network Generator: select the  moa.tasks.WriteStreamToArff  task, with  moa.streams.generators.BayesianNetworkGenerator .",
            "title": "Quick Start"
        },
        {
            "location": "/Contributing/",
            "text": "Project vision\n\u00b6\n\n\nWe want to make machine learning and data analysis \nsimple\n, \naccessible\n, \ncollaborative\n and \nopen\n with an optimal \ndivision of labour\n between computers and humans.\n\n\nWant to get involved?\n\u00b6\n\n\nAwesome, we're happy to have you! \n\n\nOpenML is dependent on the community. If you want to help, please email us (\nopenmlHQ@googlegroups.com\n). If you feel already comfortable you can help by opening issues or make a pull request on GitHub. We also have regular workshops you can join (they are announced on openml.org).\n\n\nWho are we?\n\u00b6\n\n\nWe are a group of friendly people who are excited about open science and machine learning. A list of people currently involved can be found \nhere\n.\n\n\nWe need help!\n\u00b6\n\n\nWe are currently looking for help with:  \n\n\n\n\nUser feedback (best via GitHub issues, but email is also fine)\n\n\nFrontend / UX / Design of the website\n\n\nBackend / API\n\n\nOutreach / making OpenML better known (especially in non-ML-communities, where people have data but no analysis experise)\n\n\nHelping with the interfaces (\nPython\n, \nWEKA\n, \nMOA\n, \nRapidMiner\n, \nJava\n, \nR\n; find the links to GitHub repos \nhere\n)\n\n\nHelping with documenting the interfaces or the API\n\n\nWhat could we do better to get new users started? Help us to figure out what is difficult to understand about OpenML. If you \nare\n a new user, you are the perfect person for this!\n\n\n\n\nBeginner issues\n\u00b6\n\n\nCheck out the issues labeled \nGood first issue\n or \nhelp wanted\n (you need to be logged into GitHub to see these)\n\n\nChange the world\n\u00b6\n\n\nIf you have your own ideas on how you want to contribute, please \nget in touch\n! We are very friendly and open to new ideas",
            "title": "How to contribute"
        },
        {
            "location": "/Contributing/#project-vision",
            "text": "We want to make machine learning and data analysis  simple ,  accessible ,  collaborative  and  open  with an optimal  division of labour  between computers and humans.",
            "title": "Project vision"
        },
        {
            "location": "/Contributing/#want-to-get-involved",
            "text": "Awesome, we're happy to have you!   OpenML is dependent on the community. If you want to help, please email us ( openmlHQ@googlegroups.com ). If you feel already comfortable you can help by opening issues or make a pull request on GitHub. We also have regular workshops you can join (they are announced on openml.org).",
            "title": "Want to get involved?"
        },
        {
            "location": "/Contributing/#who-are-we",
            "text": "We are a group of friendly people who are excited about open science and machine learning. A list of people currently involved can be found  here .",
            "title": "Who are we?"
        },
        {
            "location": "/Contributing/#we-need-help",
            "text": "We are currently looking for help with:     User feedback (best via GitHub issues, but email is also fine)  Frontend / UX / Design of the website  Backend / API  Outreach / making OpenML better known (especially in non-ML-communities, where people have data but no analysis experise)  Helping with the interfaces ( Python ,  WEKA ,  MOA ,  RapidMiner ,  Java ,  R ; find the links to GitHub repos  here )  Helping with documenting the interfaces or the API  What could we do better to get new users started? Help us to figure out what is difficult to understand about OpenML. If you  are  a new user, you are the perfect person for this!",
            "title": "We need help!"
        },
        {
            "location": "/Contributing/#beginner-issues",
            "text": "Check out the issues labeled  Good first issue  or  help wanted  (you need to be logged into GitHub to see these)",
            "title": "Beginner issues"
        },
        {
            "location": "/Contributing/#change-the-world",
            "text": "If you have your own ideas on how you want to contribute, please  get in touch ! We are very friendly and open to new ideas",
            "title": "Change the world"
        },
        {
            "location": "/Communication-Channels/",
            "text": "We have several communication channels set up for different purposes:\n\n\nGitHub\n\u00b6\n\n\nhttps://github.com/openml\n\n* Issues (members and users can complain)\n\n* Request new features  \n\n\nAnyone with a GitHub account can write issues. We are happy if people get involved by writing issues, so don't be shy \n\n\nSlack\n\u00b6\n\n\nhttps://openml.slack.com\n\n* Informal communication  \n\n\nWe use slack for day to day discussions and news. If you want to join the OpenML slack chat, please message us (\nopenmlHQ@googlegroups.com\n).\n\n\nMailing List\n\u00b6\n\n\nhttps://groups.google.com/forum/#!forum/openml\n\n* Information on upcoming workshop\n\n* Other major information\n\n* Urgent or important issues  \n\n\nIf you want to receive information on major news or upcoming events, sign up for the \nmailing list\n. There is a privat mailing list for \nOpenML core members\n which you can contact by sending an e-mail to \nopenmlHQ@googlegroups.com\n.\n\n\nTwitter (@open_ml)\n\u00b6\n\n\nhttps://twitter.com/open_ml\n\n* News\n\n* Publicly relevant information  \n\n\nBlog\n\u00b6\n\n\nhttps://medium.com/open-machine-learning/archive\n\n* Tutorials\n\n* News\n\n* Info about papers  \n\n\nHangout (google)\n\u00b6\n\n\n\n\nCalls",
            "title": "Communication Channels"
        },
        {
            "location": "/Communication-Channels/#github",
            "text": "https://github.com/openml \n* Issues (members and users can complain) \n* Request new features    Anyone with a GitHub account can write issues. We are happy if people get involved by writing issues, so don't be shy",
            "title": "GitHub"
        },
        {
            "location": "/Communication-Channels/#slack",
            "text": "https://openml.slack.com \n* Informal communication    We use slack for day to day discussions and news. If you want to join the OpenML slack chat, please message us ( openmlHQ@googlegroups.com ).",
            "title": "Slack"
        },
        {
            "location": "/Communication-Channels/#mailing-list",
            "text": "https://groups.google.com/forum/#!forum/openml \n* Information on upcoming workshop \n* Other major information \n* Urgent or important issues    If you want to receive information on major news or upcoming events, sign up for the  mailing list . There is a privat mailing list for  OpenML core members  which you can contact by sending an e-mail to  openmlHQ@googlegroups.com .",
            "title": "Mailing List"
        },
        {
            "location": "/Communication-Channels/#twitter-open_ml",
            "text": "https://twitter.com/open_ml \n* News \n* Publicly relevant information",
            "title": "Twitter (@open_ml)"
        },
        {
            "location": "/Communication-Channels/#blog",
            "text": "https://medium.com/open-machine-learning/archive \n* Tutorials \n* News \n* Info about papers",
            "title": "Blog"
        },
        {
            "location": "/Communication-Channels/#hangout-google",
            "text": "Calls",
            "title": "Hangout (google)"
        },
        {
            "location": "/Core-team/",
            "text": "OpenML has many amazing contributors, which you can find on out \nteam website\n. Should you be a contributor, but not on this page, let us know!\n\n\nCurrent members of the core team are:\n\n\n\n\nJoaquin Vanschoren\n\n\nJan van Rijn\n\n\nBernd Bischl\n\n\nGiuseppe Casaliccio\n\n\nMatthias Feurer\n\n\nHeidi Seibold\n\n\n\n\nYou can contact us by emailing to \nopenmlHQ@googlegroups.com\n.\n\n\nTo get in touch with the broader community check out our \ncommunication channels\n.",
            "title": "Core team"
        },
        {
            "location": "/benchmark/",
            "text": "Benchmarking suites\n\u00b6\n\n\nMachine learning research depends on objectively interpretable, comparable, and reproducible algorithm benchmarks. OpenML aims to facilitate the creation of curated, comprehensive \nsuites\n of machine learning tasks, covering precise sets of conditions.\n\n\nSeamlessly integrated into the OpenML platform, benchmark suites standardize the setup, execution, analysis, and reporting of benchmarks. Moreover, they make benchmarking a whole lot easier:\n\n\n\n\nall datasets are uniformly formatted in standardized data formats\n\n\nthey can be easily downloaded programmatically through \nAPIs and client libraries\n\n\nthey come with machine-readable \nmeta-information\n, such as the occurrence of missing values, to train algorithms correctly\n\n\nstandardized train-test splits are provided to ensure that results can be objectively compared\n\n\nresults can be shared in a reproducible way through the \nAPIs\n\n\nresults from other users can be easily downloaded and reused\n\n\n\n\nSoftware interfaces\n\u00b6\n\n\nTo use OpenML Benchmark suites, you can use bindings in several programming languages. These all interface with the OpenML REST API. The default endpoint for this is \nhttps://www.openml.org/api/v1/\n, but this can change when later versions of the API are released. To use the code examples below, you only need a recent version of one of the following libraries:\n\n\n\n\nOpenML Java ApiConnector\n (version \n1.0.22\n and up).\n\n\nOpenML Weka\n (version \n0.9.6\n and up). This package adds a Weka Integration.\n\n\nOpenML Python\n (version \n0.8.0\n and up)\n\n\nOpenML R\n (version \n1.8\n and up)\n\n\n\n\nUsing OpenML Benchmark Suites\n\u00b6\n\n\nBelow are walk-through instructions for common use cases, as well as code examples. These illustrations use the reference \nOpenML-CC18\n benchmark suite, but you can replace it with any other benchmark suite. Note that a benchmark suite is a set of OpenML \ntasks\n, which envelop not only a specific dataset, but also the train-test splits and (for predictive tasks) the target feature.\n\n\nTerminology and current status\nBenchmark suites are sets of OpenML tasks that you can create and manage yourself. At the same time, it is often useful to also share the set of experiments (runs) with the ensuing benchmarking results. For legacy reasons, such sets of tasks or runs are called \nstudies\n in the OpenML REST API. In the OpenML bindings (Python, R, Java,...) these are called either \nsets\n or \nstudies\n.\nWhen benchmarking, you will probably use two types of sets:\nSets of tasks. These can be created, edited, downloaded or deleted via the OpenML API. Website forms will be added soon. Also the set of underlying datasets can be easily retrieved via the API.\nSets of runs. Likewise, these can be created, edited, downloaded or deleted via the OpenML API. On the website, these are currently simply called 'studies'. Also the set of underlying tasks, datasets and flows can be easily retrieved. It is possible to link a set of runs to a benchmark study, aimed to collect future runs on that specific set of tasks. Additional information on these will be provided in a separate page.\nListing the benchmark suites\n\u00b6\n\n\nThe current list of benchmark suites is explicitly listed on the bottom of this page. The list of all sets of tasks can also be fetched programmatically. This list includes the suite's ID (and optionally an alias), which can be used to fetch further details.\n\n\nREST (under development)\n[https://www.openml.org/api/v1/xml/study/list/main_entity_type/task](https://www.openml.org/api/v1/xml/study/list/main_entity_type/task)\n\n[Check out the API docs](https://www.openml.org/api_docs/#!/study/get_study_list_filters)\n\n\n\nPython example (requires the development version)\nimport\n \nopenml\n\n\n\n# using the main entity type task, only benchmark suites are returned\n\n\n# each benchmark suite has an ID, some also have an alias. These can be\n\n\n# used to obtain the full details. \n\n\nstudies\n \n=\n \nopenml\n.\nstudy\n.\nlist_studies\n(\nmain_entity_type\n=\n'task'\n)\n\n\n\n\nJava example\npublic\n \nvoid\n \nlistBenchmarksuites\n()\n \nthrows\n \nException\n \n{\n\n    \nOpenmlConnector\n \nopenml\n \n=\n \nnew\n \nOpenmlConnector\n();\n\n    \nMap\n<\nString\n,\n \nString\n>\n \nfilters\n \n=\n \nnew\n \nTreeMap\n<\nString\n,\n \nString\n>();\n\n    \nfilters\n.\nput\n(\n\"status\"\n,\n \n\"all\"\n);\n\n    \nfilters\n.\nput\n(\n\"main_entity_type\"\n,\n \n\"task\"\n);\n\n    \nfilters\n.\nput\n(\n\"limit\"\n,\n \n\"20\"\n);\n\n    \nStudyList\n \nlist\n \n=\n \nopenml\n.\nstudyList\n(\nfilters\n);\n\n\n}\n\n\n\n\nR example\nTODO\n\n\n\nFetching details\n\u00b6\n\n\nUsing the ID or alias of a benchmark suite, you can retrieve a description and the full list of tasks and the underlying datasets.\n\n\nREST\n[https://www.openml.org/api/v1/xml/study/OpenML-CC18](https://www.openml.org/api/v1/xml/study/OpenML-CC18)\n\n[Check out the API docs](https://www.openml.org/api_docs/#!/study/get_study_id)\n\n\n\nPython example\nimport\n \nopenml\n\n\nbenchmark_suite\n \n=\n \nopenml\n.\nstudy\n.\nget_study\n(\n'OpenML-CC18'\n,\n'tasks'\n)\n \n# obtain the benchmark suite\n\n\nfor\n \ntask_id\n \nin\n \nbenchmark_suite\n.\ntasks\n:\n \n# iterate over all tasks\n\n    \ntask\n \n=\n \nopenml\n.\ntasks\n.\nget_task\n(\ntask_id\n)\n \n# download the OpenML task\n\n    \nX\n,\n \ny\n \n=\n \ntask\n.\nget_X_and_y\n()\n \n# get the data\n\n\n\n\nJava example\npublic\n \nvoid\n \ndownloadDatasets\n()\n \nthrows\n \nException\n \n{\n\n    \nOpenmlConnector\n \nopenml\n \n=\n \nnew\n \nOpenmlConnector\n();\n\n    \nStudy\n \nbenchmarksuite\n \n=\n \nopenml\n.\nstudyGet\n(\n\"OpenML-CC18\"\n,\n \n\"tasks\"\n);\n\n    \nfor\n \n(\nInteger\n \ntaskId\n \n:\n \nbenchmarksuite\n.\ngetTasks\n())\n \n{\n \n// iterate over all tasks\n\n        \nTask\n \nt\n \n=\n \nopenml\n.\ntaskGet\n(\ntaskId\n);\n \n// download the OpenML task\n\n        \n// note that InstanceHelper is part of the OpenML-weka package\n\n        \nInstances\n \nd\n \n=\n \nInstancesHelper\n.\ngetDatasetFromTask\n(\nopenml\n,\n \nt\n);\n \n// obtain the dataset\n\n    \n}\n\n\n}\n\n\n\n\nR example\nlibrary\n(\nOpenML\n)\n\ntask.ids \n=\n getOMLStudy\n(\n'OpenML-CC18'\n)\n$\ntasks\n$\ntask.id \n# obtain the list of suggested tasks\n\n\nfor\n \n(\ntask.id \nin\n task.ids\n)\n \n{\n \n# iterate over all tasks\n\n  task \n=\n getOMLTask\n(\ntask.id\n)\n \n# download single OML task\n\n  data \n=\n \nas.data.frame\n(\ntask\n)\n \n# obtain raw data set\n\n\n\n\nRunning and sharing benchmarks\n\u00b6\n\n\nThe code below demonstrates how OpenML benchmarking suites can be conveniently imported for benchmarking using the Python, Java and R APIs.\n\n\nFirst, the list of tasks is downloaded as already illustrated above. Next, a specific algorithm (or pipeline) can be run on each of them. The OpenML API will automatically evaluate the algorithm using the pre-set train-test splits and store the predictions and scores in a run object. This run object can then be immediately published, pushing the results to the OpenML server, so that they can be compared against all others on the same benchmark set. Uploading results requires an OpenML API key, which can be found in your account details after logging into the OpenML website.\n\n\nREST\nRequires POST requests:  \n[Attaching a new run to a benchmark_study](https://www.openml.org/api_docs/#!/study/post_study_id_attach)  \n[Detaching a run from benchmark_study](https://www.openml.org/api_docs/#!/study/post_study_id_detach)  \n\n\n\nPython example\nimport\n \nopenml\n\n\nimport\n \nsklearn\n\n\nopenml\n.\nconfig\n.\napikey\n \n=\n \n'FILL_IN_OPENML_API_KEY'\n \n# set the OpenML Api Key\n\n\nbenchmark_suite\n \n=\n \nopenml\n.\nstudy\n.\nget_study\n(\n'OpenML-CC18'\n,\n'tasks'\n)\n \n# obtain the benchmark suite\n\n\n# build a sklearn classifier\n\n\nclf\n \n=\n \nsklearn\n.\npipeline\n.\nmake_pipeline\n(\nsklearn\n.\npreprocessing\n.\nImputer\n(),\n\n                                     \nsklearn\n.\ntree\n.\nDecisionTreeClassifier\n())\n\n\nfor\n \ntask_id\n \nin\n \nbenchmark_suite\n.\ntasks\n:\n \n# iterate over all tasks\n\n  \ntask\n \n=\n \nopenml\n.\ntasks\n.\nget_task\n(\ntask_id\n)\n \n# download the OpenML task\n\n  \nX\n,\n \ny\n \n=\n \ntask\n.\nget_X_and_y\n()\n \n# get the data (not used in this example)\n\n  \n# run classifier on splits (requires API key)\n\n  \nrun\n \n=\n \nopenml\n.\nruns\n.\nrun_model_on_task\n(\nclf\n,\n \ntask\n)\n\n  \nscore\n \n=\n \nrun\n.\nget_metric_score\n(\nsklearn\n.\nmetrics\n.\naccuracy_score\n)\n \n# print accuracy score\n\n  \nprint\n(\n'Data set: \n%s\n; Accuracy: \n%0.2f\n'\n \n%\n \n(\ntask\n.\nget_dataset\n()\n.\nname\n,\nscore\n.\nmean\n()))\n\n  \nrun\n.\npublish\n()\n \n# publish the experiment on OpenML (optional)\n\n  \nprint\n(\n'URL for run: \n%s\n/run/\n%d\n'\n \n%\n(\nopenml\n.\nconfig\n.\nserver\n,\nrun\n.\nrun_id\n))\n\n\n\n\nJava example\npublic\n \nstatic\n \nvoid\n \nrunTasksAndUpload\n()\n \nthrows\n \nException\n \n{\n\n  \nOpenmlConnector\n \nopenml\n \n=\n \nnew\n \nOpenmlConnector\n();\n\n  \nopenml\n.\nsetApiKey\n(\n\"FILL_IN_OPENML_API_KEY\"\n);\n\n  \n// obtain the benchmark suite\n\n  \nStudy\n \nbenchmarksuite\n \n=\n \nopenml\n.\nstudyGet\n(\n\"OpenML-CC18\"\n,\n \n\"tasks\"\n);\n\n  \nClassifier\n \ntree\n \n=\n \nnew\n \nREPTree\n();\n \n// build a Weka classifier\n\n  \nfor\n \n(\nInteger\n \ntaskId\n \n:\n \nbenchmarksuite\n.\ngetTasks\n())\n \n{\n \n// iterate over all tasks\n\n    \nTask\n \nt\n \n=\n \nopenml\n.\ntaskGet\n(\ntaskId\n);\n \n// download the OpenML task\n\n    \nInstances\n \nd\n \n=\n \nInstancesHelper\n.\ngetDatasetFromTask\n(\nopenml\n,\n \nt\n);\n \n// obtain the dataset\n\n    \nint\n \nrunId\n \n=\n \nRunOpenmlJob\n.\nexecuteTask\n(\nopenml\n,\n \nnew\n \nWekaConfig\n(),\n \ntaskId\n,\n \ntree\n);\n\n    \nRun\n \nrun\n \n=\n \nopenml\n.\nrunGet\n(\nrunId\n);\n   \n// retrieve the uploaded run\n\n  \n}\n\n\n}\n\n\n\n\nR example\nlibrary\n(\nOpenML\n)\n\nsetOMLConfig\n(\napikey \n=\n \n'FILL_IN_OPENML_API_KEY'\n)\n\nlrn \n=\n makeLearner\n(\n'classif.rpart'\n)\n \n# construct a simple CART classifier\n\ntask.ids \n=\n getOMLStudy\n(\n'OpenML-CC18'\n)\n$\ntasks\n$\ntask.id \n# obtain the list of suggested tasks\n\n\nfor\n \n(\ntask.id \nin\n task.ids\n)\n \n{\n \n# iterate over all tasks\n\n  task \n=\n getOMLTask\n(\ntask.id\n)\n \n# download single OML task\n\n  data \n=\n \nas.data.frame\n(\ntask\n)\n \n# obtain raw data set\n\n  run \n=\n runTaskMlr\n(\ntask\n,\n learner \n=\n lrn\n)\n \n# run constructed learner\n\n  upload \n=\n uploadOMLRun\n(\nrun\n)\n \n# upload and tag the run\n\n\n}\n\n\n\n\nRetrieving runs on a benchmarking suites:\n\u00b6\n\n\nOnce a benchmark suite has been created, the listing functions can be used to \nobtain all results on the benchmark suite. Note that there are several other\nways to select and bundle runs together. This will be featured in \na separate article on reproducible benchmarks. \n\n\nREST (TODO)\n[https://www.openml.org/api/v1/xml/run/list/study/OpenML-CC18](https://www.openml.org/api/v1/xml/run/list/study/OpenML-CC18)\n\n[Check out the API docs](https://www.openml.org/api_docs/#!/run/get_run_list_filters)\n\n\n\nPython example\nbenchmark_suite\n \n=\n \nopenml\n.\nstudy\n.\nget_study\n(\n'OpenML-CC18'\n,\n \n'tasks'\n)\n\n\nruns\n \n=\n \nopenml\n.\nruns\n.\nlist_runs\n(\ntask\n=\nbenchmark_suite\n.\ntasks\n,\n \nlimit\n=\n1000\n)\n\n\n\n\nJava example\npublic\n \nvoid\n \ndownloadResultsBenchmarkSuite\n()\n  \nthrows\n \nException\n \n{\n\n    \nStudy\n \nbenchmarkSuite\n \n=\n \nopenml\n.\nstudyGet\n(\n\"OpenML100\"\n,\n \n\"tasks\"\n);\n\n\n    \nMap\n<\nString\n,\n \nList\n<\nInteger\n>>\n \nfilters\n \n=\n \nnew\n \nTreeMap\n<\nString\n,\n \nList\n<\nInteger\n>>();\n\n    \nfilters\n.\nput\n(\n\"task\"\n,\n \nArrays\n.\nasList\n(\nbenchmarkSuite\n.\ngetTasks\n()));\n\n    \nRunList\n \nrl\n \n=\n \nopenml\n.\nrunList\n(\nfilters\n,\n \n200\n,\n \nnull\n);\n\n\n    \nassertTrue\n(\nrl\n.\ngetRuns\n().\nlength\n \n>\n \n0\n);\n \n\n}\n\n\n\n\nR example\nTODO\n\n\n\nCreating new benchmark suites\n\u00b6\n\n\nAdditional OpenML benchmark suites can be created by defining the precise set of tasks, as well as a textual description. New datasets first need to be \nregistered on OpenML\n and tasks need to be created on them.\n\n\nWe have provided \na GitHub repository\n with additional tools and scripts to build new benchmark studies, e.g. to select all datasets adhering to strict conditions, and to analyse bencharking results.\n\n\nREST\nRequires POST requests:  \n[Creating a benchmark suite](https://www.openml.org/api_docs/#!/study/post_study)  \n\n\n\nPython example\nimport\n \nopenml\n\n\n\n# find 250 tasks that we are interested in, e.g., the tasks that have between\n\n\n# 100 and 10000 instances and between 4 and 20 attributes\n\n\ntasks\n \n=\n \nopenml\n.\ntasks\n.\nlist_tasks\n(\nnumber_instances\n=\n'100..10000'\n,\n \nnumber_features\n=\n'4..20'\n,\n \nsize\n=\n250\n)\n\n\ntask_ids\n \n=\n \nlist\n(\ntasks\n.\nkeys\n())\n\n\n\n# create the benchmark suite\n\n\n# the arguments are the alias, name, description, and list of task_ids, respectively.\n\n\nstudy\n \n=\n \nopenml\n.\nstudy\n.\ncreate_benchmark_suite\n(\nNone\n,\n \n\"MidSize Suite\"\n,\n \n\"illustrating how to create a benchmark suite\"\n,\n \ntask_ids\n)\n\n\nstudy_id\n \n=\n \nstudy\n.\npublish\n()\n\n\n\n\nJava example\npublic\n \nvoid\n \ncreateBenchmarkSuite\n()\n \nthrows\n \nException\n \n{\n\n    \nOpenmlConnector\n \nopenml\n \n=\n \nnew\n \nOpenmlConnector\n(\n\"FILL_IN_OPENML_API_KEY\"\n);\n\n    \n// find 250 tasks that we are interested in, e.g., the tasks that have between\n\n    \n// 100 and 10000 instances and between 4 and 20 attributes\n\n    \nMap\n<\nString\n,\n \nString\n>\n \nfiltersOrig\n \n=\n \nnew\n \nTreeMap\n<\nString\n,\n \nString\n>();\n\n    \nfiltersOrig\n.\nput\n(\n\"number_instances\"\n,\n \n\"100..10000\"\n);\n\n    \nfiltersOrig\n.\nput\n(\n\"number_features\"\n,\n \n\"4..20\"\n);\n\n    \nfiltersOrig\n.\nput\n(\n\"limit\"\n,\n \n\"250\"\n);\n\n    \nTasks\n \ntasksOrig\n \n=\n \nclient_write_test\n.\ntaskList\n(\nfiltersOrig\n);\n\n\n    \n// create the study\n\n    \nStudy\n \nstudy\n \n=\n \nnew\n \nStudy\n(\nnull\n,\n \n\"test\"\n,\n \n\"test\"\n,\n \nnull\n,\n \ntasksOrig\n.\ngetTaskIds\n(),\n \nnull\n);\n\n    \nint\n \nstudyId\n \n=\n \nopenml\n.\nstudyUpload\n(\nstudy\n);\n\n\n}\n\n\n\n\nR example\nTODO\n\n\n\nUpdating a benchmark suite\n\u00b6\n\n\nYou can add tasks to a benchmark suite, or remove them.\n\n\nREST\nRequires POST requests:  \n[Attaching a new task](https://www.openml.org/api_docs/#!/study/post_study_id_attach)  \n[Detaching a task](https://www.openml.org/api_docs/#!/study/post_study_id_detach)  \n\n\n\nPython example\nimport\n \nopenml\n\n\n\n# find 250 tasks that we are interested in, e.g., the tasks that have between\n\n\n# 100 and 10000 instances and between 4 and 20 attributes\n\n\ntasks\n \n=\n \nopenml\n.\ntasks\n.\nlist_tasks\n(\nnumber_instances\n=\n'100..10000'\n,\n \nnumber_features\n=\n'4..20'\n,\n \nsize\n=\n250\n)\n\n\ntask_ids\n \n=\n \nlist\n(\ntasks\n.\nkeys\n())\n\n\n\n# create the benchmark suite\n\n\nstudy\n \n=\n \nopenml\n.\nstudy\n.\ncreate_benchmark_suite\n(\nNone\n,\n \n\"MidSize Suite\"\n,\n \n\"illustrating how to create a benchmark suite\"\n,\n \ntask_ids\n)\n\n\nstudy_id\n \n=\n \nstudy\n.\npublish\n()\n\n\n\n# download the study from the server, for verification purposes\n\n\nstudy\n \n=\n \nopenml\n.\nstudy\n.\nget_study\n(\nstudy_id\n)\n\n\n\n# until the benchmark suite is activated, we can also add some more tasks. Search for the letter dataset:\n\n\ntasks_new\n \n=\n \nopenml\n.\ntasks\n.\nlist_tasks\n(\ndata_name\n=\n'letter'\n,\n \nsize\n=\n1\n)\n\n\ntask_ids_new\n \n=\n \nlist\n(\ntasks_new\n.\nkeys\n())\n\n\nopenml\n.\nstudy\n.\nattach_to_study\n(\nstudy_id\n,\n \ntask_ids_new\n)\n\n\n\n# or even remove these again\n\n\nopenml\n.\nstudy\n.\ndetach_from_study\n(\nstudy_id\n,\n \ntask_ids_new\n)\n\n\n\n# redownload the study\n\n\nstudy_prime\n \n=\n \nopenml\n.\nstudy\n.\nget_study\n(\nstudy_id\n)\n\n\n\nassert\n(\nstudy\n.\ntasks\n \n==\n \nstudy_prime\n.\ntasks\n)\n\n\nassert\n(\nstudy\n.\ndata\n \n==\n \nstudy_prime\n.\ndata\n)\n\n\n\n\nJava example\npublic\n \nvoid\n \nattachDetachStudy\n()\n  \nthrows\n \nException\n \n{\n\n    \nOpenmlConnector\n \nopenml\n \n=\n \nnew\n \nOpenmlConnector\n(\n\"FILL_IN_OPENML_API_KEY\"\n);\n\n    \n// find 250 tasks that we are interested in, e.g., the tasks that have between\n\n    \n// 100 and 10000 instances and between 4 and 20 attributes\n\n    \nMap\n<\nString\n,\n \nString\n>\n \nfiltersOrig\n \n=\n \nnew\n \nTreeMap\n<\nString\n,\n \nString\n>();\n\n    \nfiltersOrig\n.\nput\n(\n\"number_instances\"\n,\n \n\"100..10000\"\n);\n\n    \nfiltersOrig\n.\nput\n(\n\"number_features\"\n,\n \n\"4..20\"\n);\n\n    \nfiltersOrig\n.\nput\n(\n\"limit\"\n,\n \n\"250\"\n);\n\n    \nTasks\n \ntasksOrig\n \n=\n \nopenml\n.\ntaskList\n(\nfiltersOrig\n);\n\n\n    \n// create the study\n\n    \nStudy\n \nstudy\n \n=\n \nnew\n \nStudy\n(\nnull\n,\n \n\"test\"\n,\n \n\"test\"\n,\n \nnull\n,\n \ntasksOrig\n.\ngetTaskIds\n(),\n \nnull\n);\n\n    \nint\n \nstudyId\n \n=\n \nopenml\n.\nstudyUpload\n(\nstudy\n);\n\n\n    \n// until the benchmark suite is activated, we can also add some more tasks. Search for the letter dataset:\n\n    \nMap\n<\nString\n,\n \nString\n>\n \nfiltersAdd\n \n=\n \nnew\n \nTreeMap\n<\nString\n,\n \nString\n>();\n\n    \nfiltersAdd\n.\nput\n(\n\"data_name\"\n,\n \n\"letter\"\n);\n\n    \nfiltersAdd\n.\nput\n(\n\"limit\"\n,\n \n\"1\"\n);\n\n    \nTasks\n \ntasksAdd\n \n=\n \nopenml\n.\ntaskList\n(\nfiltersAdd\n);\n\n    \nopenml\n.\nstudyAttach\n(\nstudyId\n,\n \nArrays\n.\nasList\n(\ntasksAdd\n.\ngetTaskIds\n()));\n\n\n    \n// or even remove these again\n\n    \nopenml\n.\nstudyDetach\n(\nstudyId\n,\n \nArrays\n.\nasList\n(\ntasksAdd\n.\ngetTaskIds\n()));\n\n\n    \n// download the study\n\n    \nStudy\n \nstudyDownloaded\n \n=\n \nopenml\n.\nstudyGet\n(\nstudyId\n);\n\n    \nassertArrayEquals\n(\ntasksOrig\n.\ngetTaskIds\n(),\n \nstudyDownloaded\n.\ngetTasks\n());\n\n\n}\n\n\n\n\nR example\nTODO\n\n\n\nFurther code examples and use cases\n\u00b6\n\n\nAs mentioned above, we host \na GitHub repository\n with additional tools and scripts to easily create and use new benchmark studies. It includes:\n\n\n\n\nA Jupyter Notebook that builds a new benchmark suite with datasets that adhere to strict and complex conditions, as well as automated tests to remove tasks that are too easy for proper benchmarking.\n\n\nA Jupyter Notebook that shows how to pull in the latest state-of-the-art results for any of the benchmark suites\n\n\nA Jupyter Notebook that does a detailed analysis of all results in a benchmark suite, and an example run on the OpenML-CC18. It includes a wide range of plots and rankings to get a deeper insight into the benchmark results.\n\n\nScripts in Python and R to facilitate common subtasks.\n\n\n\n\nWe very much welcome new scripts and notebooks, or improvements to the existing ones, that help others to create benchmark suites and analyse benchmarking results.\n\n\nList of benchmarking suites\n\u00b6\n\n\nOpenML-CC18\n\u00b6\n\n\nThe \nOpenML-CC18\n suite contains all OpenML datasets from mid-2018 that satisfy a large set of clear requirements for thorough yet practical benchmarking. It includes datasets frequently used in benchmarks published over the last years, so it can be used as a drop-in replacement for many benchmarking setups.\n\n\nList of datasets and properties\n\n\nThe suite is defined as the set of all verified OpenML datasets that satisfy the following requirements:\n\n\n\n\nthe number of observations are between 500 and 100000 to focus on medium-sized datasets, that are not too small and not too big,\n\n\nthe number of features does not exceed 5000 features to keep the runtime of algorithms low,\n\n\nthe target attribute has at least two classes\n\n\nhave classes with less than 20 observations\n\n\nthe ratio of the minority class and the majority class is above 0.05, to eliminate highly imbalanced datasets which require special treatment for both algorithms and evaluation measures.\n\n\n\n\nWe excluded datasets which:\n\n\n\n\nare artificially generated (not to confuse with simulated)\n\n\ncannot be randomized via a 10-fold cross-validation due to grouped samples or because they are time series or data streams\n\n\nare a subset of a larger dataset\n\n\nhave no source or reference available\n\n\ncan be perfectly classified by a single attribute or a decision stump\n\n\nallow a decision tree to achieve 100% accuracy on a 10-fold cross-validation task\n\n\nhave more than 5000 features after one-hot-encoding categorical features\n\n\nare created by binarization of regression tasks or multiclass classification tasks, or\n\n\nare sparse data (e.g., text mining data sets)\n\n\n\n\nDetailed motivation of these decisions\nWe chose the CC18 datasets to allow for practical benchmarking based on the characteristics that might be problematic based on our experience, and to avoid common pitfalls that may invalidate benchmark studies:  \nWe used at least 500 data points to allow performing cross-validation while still having a large-enough test split.\nWe limited the datasets to 100.000 data points to allow the algorithms to train machine learning models in a reasonable amount of time.\nWe limited the number of features to 5000 to allow the usage of algorithms which scale unfavourably in the number of features. This limitation, together with the two limitations above aims to allow running all \u201cstandard\u201d machine learning algorithms (naive bayes, linear models, support vector machines, tree-based ensemble methods and neural networks) on the benchmark suite.\nWe required each dataset to have at least two classes to be able to work in a supervised classification setting.\nWe require each class to have at least 20 observations to be able to perform stratified cross-validation where there is at least one observation from each class in each split. We have found that not having all classes present in all training and test sets can make several machine learning packages fail.\nWe require a certain balancedness (ratio of minority class to majority class) to prevent cases where only predicting the majority class would be beneficial. This is most likely the restriction which is most debatable, but we found it very helpful to apply a large set of machine learning algorithms across several libraries to the study. We expect that future studies focus more on imbalanced datasets. \nFurthermore, we aimed to have the dataset collection as general as possible, rule out as few algorithms as possible and have it usable as easily as possible:\nWe strived to remove artificial datasets as they, for example, come from textbooks and it is hard to reliably assess their difficulty. We admit that there is a blurred line between artificial and simulated datasets and do not have a perfect distinction between them (for example, a lot of phenomena can be simulated, but the outcome might be like a simple, artificial dataset). Therefore, we removed datasets if we were in doubt of whether they are simulated or artificial. \nWe removed datasets which require grouped sampling because they are time series or data streams which should be treated with special care by machine learning algorithms (i.e., taking the time aspect into account). To be on the safe side, we also removed datasets where each sample constitutes a single data stream.\nWe removed datasets which are a subset of larger datasets. Allowing subsets would be very subjective as there is no objective choice of a dataset subset size or a subset of the variables or classes. Therefore, creating dataset subsets would open a Pandora\u2019s Box.\nWe removed datasets which have no source or reference available to potentially learn more about these datasets if we observe unexpected behavior in future studies. In contrast, we would not be able to learn more about the background of a dataset which has no description and publication attached, leaving us with a complete black box.\nWe removed datasets which can be perfectly classified by a single attribute or a decision stump as they do not allow to meaningfully compare machine learning algorithms (they all achieve 100% accuracy unless the hyperparameters are set in a bogus way).\nWe removed datasets where a decision tree could achieve 100% accuracy on a 10-fold cross-validation task to remove datasets which can be solved by a simple algorithm which is prone to overfitting training data. We found that this is a good indicator of too easy datasets. Obviously, other datasets will appear easy for several algorithms, and we aim to learn more about the characteristics of such datasets in future studies.\nWe removed datasets which have more than 5000 features after one-hot-encoding categorical features. One-hot-encoding is the most frequent way to deal with categorical variables across the different machine learning libraries MLR, scikit-learn and WEKA. In order to limit the number of features to 5000 as explained above, we imposed the additional constraint that this should be counted after one-hot-encoding to allow wide applicability of the benchmark suite.\nWe removed datasets which were created by binarization of regression tasks or multiclass classification task for similar reasons as for forbidding dataset subsets.\nWe did not include sparse datasets because not all machine learning libraries (i.e., all machine learning models) can handle them gracefully, which is in contrast to our goal which is wide applicability.\nOpenML100\n\u00b6\n\n\nThe \nOpenML100\n was a predecessor of the OpenML-CC18, consisting of \n100 classification datasets\n. We recommend that you use the \nOpenML-CC18\n instead, because the OpenML100 suffers from some teething issues in the design of benchmark suites. For instance, it contains several datasets that are too easy to model with today's machine learning algorithms, as well as datasets that represent time series analysis problems. These do not invalidate benchmarks run on the OpenML100, but may obfuscate the interpretation of results. The 'OpenML-CC18' handle is also more descriptive and allows easier versioning.\nThe OpenML100 was first published in the Arxiv preprint \nOpenML Benchmarking Suites and the OpenML100\n.\n\n\nList of datasets and properties\n\n\nFor reference, the OpenML100 included datasets satisfying the following requirements:\n\n\n\n\nthe number of observations are between 500 and 100000 to focus on medium-sized datasets, that are not too small for proper training and not too big for practical experimentation\n\n\nthe number of features does not exceed 5000 features to keep the runtime of algorithms low\n\n\nthe target attribute has at least two classes\n\n\nhe ratio of the minority class and the majority class is above 0.05 to eliminate highly imbalanced datasets that would obfuscate a clear analysis\n\n\n\n\nIt excluded datasets which:\n\n\n\n\ncannot be randomized via a 10-fold cross-validation due to grouped samples\n\n\nhave an unknown origin or no clearly defined task\n\n\nare variants of other datasets (e.g. binarized regression tasks)\n\n\ninclude sparse data (e.g., text mining data sets)\n\n\n\n\nNeed help?\n\u00b6\n\n\nWe are happy to answer to any suggestion or question you may have. For general questions or issues, please open an issue in the \nbenchmarking issue tracker\n. If the issue lies with one of the language-specific bindings, please post an issue \nin the appropriate issue tracker\n.",
            "title": "Benchmarking"
        },
        {
            "location": "/benchmark/#benchmarking-suites",
            "text": "Machine learning research depends on objectively interpretable, comparable, and reproducible algorithm benchmarks. OpenML aims to facilitate the creation of curated, comprehensive  suites  of machine learning tasks, covering precise sets of conditions.  Seamlessly integrated into the OpenML platform, benchmark suites standardize the setup, execution, analysis, and reporting of benchmarks. Moreover, they make benchmarking a whole lot easier:   all datasets are uniformly formatted in standardized data formats  they can be easily downloaded programmatically through  APIs and client libraries  they come with machine-readable  meta-information , such as the occurrence of missing values, to train algorithms correctly  standardized train-test splits are provided to ensure that results can be objectively compared  results can be shared in a reproducible way through the  APIs  results from other users can be easily downloaded and reused",
            "title": "Benchmarking suites"
        },
        {
            "location": "/benchmark/#software-interfaces",
            "text": "To use OpenML Benchmark suites, you can use bindings in several programming languages. These all interface with the OpenML REST API. The default endpoint for this is  https://www.openml.org/api/v1/ , but this can change when later versions of the API are released. To use the code examples below, you only need a recent version of one of the following libraries:   OpenML Java ApiConnector  (version  1.0.22  and up).  OpenML Weka  (version  0.9.6  and up). This package adds a Weka Integration.  OpenML Python  (version  0.8.0  and up)  OpenML R  (version  1.8  and up)",
            "title": "Software interfaces"
        },
        {
            "location": "/benchmark/#using-openml-benchmark-suites",
            "text": "Below are walk-through instructions for common use cases, as well as code examples. These illustrations use the reference  OpenML-CC18  benchmark suite, but you can replace it with any other benchmark suite. Note that a benchmark suite is a set of OpenML  tasks , which envelop not only a specific dataset, but also the train-test splits and (for predictive tasks) the target feature.  Terminology and current status Benchmark suites are sets of OpenML tasks that you can create and manage yourself. At the same time, it is often useful to also share the set of experiments (runs) with the ensuing benchmarking results. For legacy reasons, such sets of tasks or runs are called  studies  in the OpenML REST API. In the OpenML bindings (Python, R, Java,...) these are called either  sets  or  studies . When benchmarking, you will probably use two types of sets: Sets of tasks. These can be created, edited, downloaded or deleted via the OpenML API. Website forms will be added soon. Also the set of underlying datasets can be easily retrieved via the API. Sets of runs. Likewise, these can be created, edited, downloaded or deleted via the OpenML API. On the website, these are currently simply called 'studies'. Also the set of underlying tasks, datasets and flows can be easily retrieved. It is possible to link a set of runs to a benchmark study, aimed to collect future runs on that specific set of tasks. Additional information on these will be provided in a separate page.",
            "title": "Using OpenML Benchmark Suites"
        },
        {
            "location": "/benchmark/#listing-the-benchmark-suites",
            "text": "The current list of benchmark suites is explicitly listed on the bottom of this page. The list of all sets of tasks can also be fetched programmatically. This list includes the suite's ID (and optionally an alias), which can be used to fetch further details.  REST (under development) [https://www.openml.org/api/v1/xml/study/list/main_entity_type/task](https://www.openml.org/api/v1/xml/study/list/main_entity_type/task)\n\n[Check out the API docs](https://www.openml.org/api_docs/#!/study/get_study_list_filters)  Python example (requires the development version) import   openml  # using the main entity type task, only benchmark suites are returned  # each benchmark suite has an ID, some also have an alias. These can be  # used to obtain the full details.   studies   =   openml . study . list_studies ( main_entity_type = 'task' )   Java example public   void   listBenchmarksuites ()   throws   Exception   { \n     OpenmlConnector   openml   =   new   OpenmlConnector (); \n     Map < String ,   String >   filters   =   new   TreeMap < String ,   String >(); \n     filters . put ( \"status\" ,   \"all\" ); \n     filters . put ( \"main_entity_type\" ,   \"task\" ); \n     filters . put ( \"limit\" ,   \"20\" ); \n     StudyList   list   =   openml . studyList ( filters );  }   R example TODO",
            "title": "Listing the benchmark suites"
        },
        {
            "location": "/benchmark/#fetching-details",
            "text": "Using the ID or alias of a benchmark suite, you can retrieve a description and the full list of tasks and the underlying datasets.  REST [https://www.openml.org/api/v1/xml/study/OpenML-CC18](https://www.openml.org/api/v1/xml/study/OpenML-CC18)\n\n[Check out the API docs](https://www.openml.org/api_docs/#!/study/get_study_id)  Python example import   openml  benchmark_suite   =   openml . study . get_study ( 'OpenML-CC18' , 'tasks' )   # obtain the benchmark suite  for   task_id   in   benchmark_suite . tasks :   # iterate over all tasks \n     task   =   openml . tasks . get_task ( task_id )   # download the OpenML task \n     X ,   y   =   task . get_X_and_y ()   # get the data   Java example public   void   downloadDatasets ()   throws   Exception   { \n     OpenmlConnector   openml   =   new   OpenmlConnector (); \n     Study   benchmarksuite   =   openml . studyGet ( \"OpenML-CC18\" ,   \"tasks\" ); \n     for   ( Integer   taskId   :   benchmarksuite . getTasks ())   {   // iterate over all tasks \n         Task   t   =   openml . taskGet ( taskId );   // download the OpenML task \n         // note that InstanceHelper is part of the OpenML-weka package \n         Instances   d   =   InstancesHelper . getDatasetFromTask ( openml ,   t );   // obtain the dataset \n     }  }   R example library ( OpenML ) \ntask.ids  =  getOMLStudy ( 'OpenML-CC18' ) $ tasks $ task.id  # obtain the list of suggested tasks  for   ( task.id  in  task.ids )   {   # iterate over all tasks \n  task  =  getOMLTask ( task.id )   # download single OML task \n  data  =   as.data.frame ( task )   # obtain raw data set",
            "title": "Fetching details"
        },
        {
            "location": "/benchmark/#running-and-sharing-benchmarks",
            "text": "The code below demonstrates how OpenML benchmarking suites can be conveniently imported for benchmarking using the Python, Java and R APIs.  First, the list of tasks is downloaded as already illustrated above. Next, a specific algorithm (or pipeline) can be run on each of them. The OpenML API will automatically evaluate the algorithm using the pre-set train-test splits and store the predictions and scores in a run object. This run object can then be immediately published, pushing the results to the OpenML server, so that they can be compared against all others on the same benchmark set. Uploading results requires an OpenML API key, which can be found in your account details after logging into the OpenML website.  REST Requires POST requests:  \n[Attaching a new run to a benchmark_study](https://www.openml.org/api_docs/#!/study/post_study_id_attach)  \n[Detaching a run from benchmark_study](https://www.openml.org/api_docs/#!/study/post_study_id_detach)    Python example import   openml  import   sklearn  openml . config . apikey   =   'FILL_IN_OPENML_API_KEY'   # set the OpenML Api Key  benchmark_suite   =   openml . study . get_study ( 'OpenML-CC18' , 'tasks' )   # obtain the benchmark suite  # build a sklearn classifier  clf   =   sklearn . pipeline . make_pipeline ( sklearn . preprocessing . Imputer (), \n                                      sklearn . tree . DecisionTreeClassifier ())  for   task_id   in   benchmark_suite . tasks :   # iterate over all tasks \n   task   =   openml . tasks . get_task ( task_id )   # download the OpenML task \n   X ,   y   =   task . get_X_and_y ()   # get the data (not used in this example) \n   # run classifier on splits (requires API key) \n   run   =   openml . runs . run_model_on_task ( clf ,   task ) \n   score   =   run . get_metric_score ( sklearn . metrics . accuracy_score )   # print accuracy score \n   print ( 'Data set:  %s ; Accuracy:  %0.2f '   %   ( task . get_dataset () . name , score . mean ())) \n   run . publish ()   # publish the experiment on OpenML (optional) \n   print ( 'URL for run:  %s /run/ %d '   % ( openml . config . server , run . run_id ))   Java example public   static   void   runTasksAndUpload ()   throws   Exception   { \n   OpenmlConnector   openml   =   new   OpenmlConnector (); \n   openml . setApiKey ( \"FILL_IN_OPENML_API_KEY\" ); \n   // obtain the benchmark suite \n   Study   benchmarksuite   =   openml . studyGet ( \"OpenML-CC18\" ,   \"tasks\" ); \n   Classifier   tree   =   new   REPTree ();   // build a Weka classifier \n   for   ( Integer   taskId   :   benchmarksuite . getTasks ())   {   // iterate over all tasks \n     Task   t   =   openml . taskGet ( taskId );   // download the OpenML task \n     Instances   d   =   InstancesHelper . getDatasetFromTask ( openml ,   t );   // obtain the dataset \n     int   runId   =   RunOpenmlJob . executeTask ( openml ,   new   WekaConfig (),   taskId ,   tree ); \n     Run   run   =   openml . runGet ( runId );     // retrieve the uploaded run \n   }  }   R example library ( OpenML ) \nsetOMLConfig ( apikey  =   'FILL_IN_OPENML_API_KEY' ) \nlrn  =  makeLearner ( 'classif.rpart' )   # construct a simple CART classifier \ntask.ids  =  getOMLStudy ( 'OpenML-CC18' ) $ tasks $ task.id  # obtain the list of suggested tasks  for   ( task.id  in  task.ids )   {   # iterate over all tasks \n  task  =  getOMLTask ( task.id )   # download single OML task \n  data  =   as.data.frame ( task )   # obtain raw data set \n  run  =  runTaskMlr ( task ,  learner  =  lrn )   # run constructed learner \n  upload  =  uploadOMLRun ( run )   # upload and tag the run  }",
            "title": "Running and sharing benchmarks"
        },
        {
            "location": "/benchmark/#retrieving-runs-on-a-benchmarking-suites",
            "text": "Once a benchmark suite has been created, the listing functions can be used to \nobtain all results on the benchmark suite. Note that there are several other\nways to select and bundle runs together. This will be featured in \na separate article on reproducible benchmarks.   REST (TODO) [https://www.openml.org/api/v1/xml/run/list/study/OpenML-CC18](https://www.openml.org/api/v1/xml/run/list/study/OpenML-CC18)\n\n[Check out the API docs](https://www.openml.org/api_docs/#!/run/get_run_list_filters)  Python example benchmark_suite   =   openml . study . get_study ( 'OpenML-CC18' ,   'tasks' )  runs   =   openml . runs . list_runs ( task = benchmark_suite . tasks ,   limit = 1000 )   Java example public   void   downloadResultsBenchmarkSuite ()    throws   Exception   { \n     Study   benchmarkSuite   =   openml . studyGet ( \"OpenML100\" ,   \"tasks\" ); \n\n     Map < String ,   List < Integer >>   filters   =   new   TreeMap < String ,   List < Integer >>(); \n     filters . put ( \"task\" ,   Arrays . asList ( benchmarkSuite . getTasks ())); \n     RunList   rl   =   openml . runList ( filters ,   200 ,   null ); \n\n     assertTrue ( rl . getRuns (). length   >   0 );   }   R example TODO",
            "title": "Retrieving runs on a benchmarking suites:"
        },
        {
            "location": "/benchmark/#creating-new-benchmark-suites",
            "text": "Additional OpenML benchmark suites can be created by defining the precise set of tasks, as well as a textual description. New datasets first need to be  registered on OpenML  and tasks need to be created on them.  We have provided  a GitHub repository  with additional tools and scripts to build new benchmark studies, e.g. to select all datasets adhering to strict conditions, and to analyse bencharking results.  REST Requires POST requests:  \n[Creating a benchmark suite](https://www.openml.org/api_docs/#!/study/post_study)    Python example import   openml  # find 250 tasks that we are interested in, e.g., the tasks that have between  # 100 and 10000 instances and between 4 and 20 attributes  tasks   =   openml . tasks . list_tasks ( number_instances = '100..10000' ,   number_features = '4..20' ,   size = 250 )  task_ids   =   list ( tasks . keys ())  # create the benchmark suite  # the arguments are the alias, name, description, and list of task_ids, respectively.  study   =   openml . study . create_benchmark_suite ( None ,   \"MidSize Suite\" ,   \"illustrating how to create a benchmark suite\" ,   task_ids )  study_id   =   study . publish ()   Java example public   void   createBenchmarkSuite ()   throws   Exception   { \n     OpenmlConnector   openml   =   new   OpenmlConnector ( \"FILL_IN_OPENML_API_KEY\" ); \n     // find 250 tasks that we are interested in, e.g., the tasks that have between \n     // 100 and 10000 instances and between 4 and 20 attributes \n     Map < String ,   String >   filtersOrig   =   new   TreeMap < String ,   String >(); \n     filtersOrig . put ( \"number_instances\" ,   \"100..10000\" ); \n     filtersOrig . put ( \"number_features\" ,   \"4..20\" ); \n     filtersOrig . put ( \"limit\" ,   \"250\" ); \n     Tasks   tasksOrig   =   client_write_test . taskList ( filtersOrig ); \n\n     // create the study \n     Study   study   =   new   Study ( null ,   \"test\" ,   \"test\" ,   null ,   tasksOrig . getTaskIds (),   null ); \n     int   studyId   =   openml . studyUpload ( study );  }   R example TODO",
            "title": "Creating new benchmark suites"
        },
        {
            "location": "/benchmark/#updating-a-benchmark-suite",
            "text": "You can add tasks to a benchmark suite, or remove them.  REST Requires POST requests:  \n[Attaching a new task](https://www.openml.org/api_docs/#!/study/post_study_id_attach)  \n[Detaching a task](https://www.openml.org/api_docs/#!/study/post_study_id_detach)    Python example import   openml  # find 250 tasks that we are interested in, e.g., the tasks that have between  # 100 and 10000 instances and between 4 and 20 attributes  tasks   =   openml . tasks . list_tasks ( number_instances = '100..10000' ,   number_features = '4..20' ,   size = 250 )  task_ids   =   list ( tasks . keys ())  # create the benchmark suite  study   =   openml . study . create_benchmark_suite ( None ,   \"MidSize Suite\" ,   \"illustrating how to create a benchmark suite\" ,   task_ids )  study_id   =   study . publish ()  # download the study from the server, for verification purposes  study   =   openml . study . get_study ( study_id )  # until the benchmark suite is activated, we can also add some more tasks. Search for the letter dataset:  tasks_new   =   openml . tasks . list_tasks ( data_name = 'letter' ,   size = 1 )  task_ids_new   =   list ( tasks_new . keys ())  openml . study . attach_to_study ( study_id ,   task_ids_new )  # or even remove these again  openml . study . detach_from_study ( study_id ,   task_ids_new )  # redownload the study  study_prime   =   openml . study . get_study ( study_id )  assert ( study . tasks   ==   study_prime . tasks )  assert ( study . data   ==   study_prime . data )   Java example public   void   attachDetachStudy ()    throws   Exception   { \n     OpenmlConnector   openml   =   new   OpenmlConnector ( \"FILL_IN_OPENML_API_KEY\" ); \n     // find 250 tasks that we are interested in, e.g., the tasks that have between \n     // 100 and 10000 instances and between 4 and 20 attributes \n     Map < String ,   String >   filtersOrig   =   new   TreeMap < String ,   String >(); \n     filtersOrig . put ( \"number_instances\" ,   \"100..10000\" ); \n     filtersOrig . put ( \"number_features\" ,   \"4..20\" ); \n     filtersOrig . put ( \"limit\" ,   \"250\" ); \n     Tasks   tasksOrig   =   openml . taskList ( filtersOrig ); \n\n     // create the study \n     Study   study   =   new   Study ( null ,   \"test\" ,   \"test\" ,   null ,   tasksOrig . getTaskIds (),   null ); \n     int   studyId   =   openml . studyUpload ( study ); \n\n     // until the benchmark suite is activated, we can also add some more tasks. Search for the letter dataset: \n     Map < String ,   String >   filtersAdd   =   new   TreeMap < String ,   String >(); \n     filtersAdd . put ( \"data_name\" ,   \"letter\" ); \n     filtersAdd . put ( \"limit\" ,   \"1\" ); \n     Tasks   tasksAdd   =   openml . taskList ( filtersAdd ); \n     openml . studyAttach ( studyId ,   Arrays . asList ( tasksAdd . getTaskIds ())); \n\n     // or even remove these again \n     openml . studyDetach ( studyId ,   Arrays . asList ( tasksAdd . getTaskIds ())); \n\n     // download the study \n     Study   studyDownloaded   =   openml . studyGet ( studyId ); \n     assertArrayEquals ( tasksOrig . getTaskIds (),   studyDownloaded . getTasks ());  }   R example TODO",
            "title": "Updating a benchmark suite"
        },
        {
            "location": "/benchmark/#further-code-examples-and-use-cases",
            "text": "As mentioned above, we host  a GitHub repository  with additional tools and scripts to easily create and use new benchmark studies. It includes:   A Jupyter Notebook that builds a new benchmark suite with datasets that adhere to strict and complex conditions, as well as automated tests to remove tasks that are too easy for proper benchmarking.  A Jupyter Notebook that shows how to pull in the latest state-of-the-art results for any of the benchmark suites  A Jupyter Notebook that does a detailed analysis of all results in a benchmark suite, and an example run on the OpenML-CC18. It includes a wide range of plots and rankings to get a deeper insight into the benchmark results.  Scripts in Python and R to facilitate common subtasks.   We very much welcome new scripts and notebooks, or improvements to the existing ones, that help others to create benchmark suites and analyse benchmarking results.",
            "title": "Further code examples and use cases"
        },
        {
            "location": "/benchmark/#list-of-benchmarking-suites",
            "text": "",
            "title": "List of benchmarking suites"
        },
        {
            "location": "/benchmark/#openml-cc18",
            "text": "The  OpenML-CC18  suite contains all OpenML datasets from mid-2018 that satisfy a large set of clear requirements for thorough yet practical benchmarking. It includes datasets frequently used in benchmarks published over the last years, so it can be used as a drop-in replacement for many benchmarking setups.  List of datasets and properties  The suite is defined as the set of all verified OpenML datasets that satisfy the following requirements:   the number of observations are between 500 and 100000 to focus on medium-sized datasets, that are not too small and not too big,  the number of features does not exceed 5000 features to keep the runtime of algorithms low,  the target attribute has at least two classes  have classes with less than 20 observations  the ratio of the minority class and the majority class is above 0.05, to eliminate highly imbalanced datasets which require special treatment for both algorithms and evaluation measures.   We excluded datasets which:   are artificially generated (not to confuse with simulated)  cannot be randomized via a 10-fold cross-validation due to grouped samples or because they are time series or data streams  are a subset of a larger dataset  have no source or reference available  can be perfectly classified by a single attribute or a decision stump  allow a decision tree to achieve 100% accuracy on a 10-fold cross-validation task  have more than 5000 features after one-hot-encoding categorical features  are created by binarization of regression tasks or multiclass classification tasks, or  are sparse data (e.g., text mining data sets)   Detailed motivation of these decisions We chose the CC18 datasets to allow for practical benchmarking based on the characteristics that might be problematic based on our experience, and to avoid common pitfalls that may invalidate benchmark studies:   We used at least 500 data points to allow performing cross-validation while still having a large-enough test split. We limited the datasets to 100.000 data points to allow the algorithms to train machine learning models in a reasonable amount of time. We limited the number of features to 5000 to allow the usage of algorithms which scale unfavourably in the number of features. This limitation, together with the two limitations above aims to allow running all \u201cstandard\u201d machine learning algorithms (naive bayes, linear models, support vector machines, tree-based ensemble methods and neural networks) on the benchmark suite. We required each dataset to have at least two classes to be able to work in a supervised classification setting. We require each class to have at least 20 observations to be able to perform stratified cross-validation where there is at least one observation from each class in each split. We have found that not having all classes present in all training and test sets can make several machine learning packages fail. We require a certain balancedness (ratio of minority class to majority class) to prevent cases where only predicting the majority class would be beneficial. This is most likely the restriction which is most debatable, but we found it very helpful to apply a large set of machine learning algorithms across several libraries to the study. We expect that future studies focus more on imbalanced datasets.  Furthermore, we aimed to have the dataset collection as general as possible, rule out as few algorithms as possible and have it usable as easily as possible: We strived to remove artificial datasets as they, for example, come from textbooks and it is hard to reliably assess their difficulty. We admit that there is a blurred line between artificial and simulated datasets and do not have a perfect distinction between them (for example, a lot of phenomena can be simulated, but the outcome might be like a simple, artificial dataset). Therefore, we removed datasets if we were in doubt of whether they are simulated or artificial.  We removed datasets which require grouped sampling because they are time series or data streams which should be treated with special care by machine learning algorithms (i.e., taking the time aspect into account). To be on the safe side, we also removed datasets where each sample constitutes a single data stream. We removed datasets which are a subset of larger datasets. Allowing subsets would be very subjective as there is no objective choice of a dataset subset size or a subset of the variables or classes. Therefore, creating dataset subsets would open a Pandora\u2019s Box. We removed datasets which have no source or reference available to potentially learn more about these datasets if we observe unexpected behavior in future studies. In contrast, we would not be able to learn more about the background of a dataset which has no description and publication attached, leaving us with a complete black box. We removed datasets which can be perfectly classified by a single attribute or a decision stump as they do not allow to meaningfully compare machine learning algorithms (they all achieve 100% accuracy unless the hyperparameters are set in a bogus way). We removed datasets where a decision tree could achieve 100% accuracy on a 10-fold cross-validation task to remove datasets which can be solved by a simple algorithm which is prone to overfitting training data. We found that this is a good indicator of too easy datasets. Obviously, other datasets will appear easy for several algorithms, and we aim to learn more about the characteristics of such datasets in future studies. We removed datasets which have more than 5000 features after one-hot-encoding categorical features. One-hot-encoding is the most frequent way to deal with categorical variables across the different machine learning libraries MLR, scikit-learn and WEKA. In order to limit the number of features to 5000 as explained above, we imposed the additional constraint that this should be counted after one-hot-encoding to allow wide applicability of the benchmark suite. We removed datasets which were created by binarization of regression tasks or multiclass classification task for similar reasons as for forbidding dataset subsets. We did not include sparse datasets because not all machine learning libraries (i.e., all machine learning models) can handle them gracefully, which is in contrast to our goal which is wide applicability.",
            "title": "OpenML-CC18"
        },
        {
            "location": "/benchmark/#openml100",
            "text": "The  OpenML100  was a predecessor of the OpenML-CC18, consisting of  100 classification datasets . We recommend that you use the  OpenML-CC18  instead, because the OpenML100 suffers from some teething issues in the design of benchmark suites. For instance, it contains several datasets that are too easy to model with today's machine learning algorithms, as well as datasets that represent time series analysis problems. These do not invalidate benchmarks run on the OpenML100, but may obfuscate the interpretation of results. The 'OpenML-CC18' handle is also more descriptive and allows easier versioning.\nThe OpenML100 was first published in the Arxiv preprint  OpenML Benchmarking Suites and the OpenML100 .  List of datasets and properties  For reference, the OpenML100 included datasets satisfying the following requirements:   the number of observations are between 500 and 100000 to focus on medium-sized datasets, that are not too small for proper training and not too big for practical experimentation  the number of features does not exceed 5000 features to keep the runtime of algorithms low  the target attribute has at least two classes  he ratio of the minority class and the majority class is above 0.05 to eliminate highly imbalanced datasets that would obfuscate a clear analysis   It excluded datasets which:   cannot be randomized via a 10-fold cross-validation due to grouped samples  have an unknown origin or no clearly defined task  are variants of other datasets (e.g. binarized regression tasks)  include sparse data (e.g., text mining data sets)",
            "title": "OpenML100"
        },
        {
            "location": "/benchmark/#need-help",
            "text": "We are happy to answer to any suggestion or question you may have. For general questions or issues, please open an issue in the  benchmarking issue tracker . If the issue lies with one of the language-specific bindings, please post an issue  in the appropriate issue tracker .",
            "title": "Need help?"
        },
        {
            "location": "/altmetrics/",
            "text": "To encourage open science, OpenML now includes a score system to track and reward scientific activity, reach and impact, and in the future will include further gamification features such as badges. Because the system is still experimental and very much in development, the details are subject to change. Below, the score system is described in more detailed followed by our rationale for this system for those interested. If anything is unclear or you have any feedback of the system do not hesitate to let us know.\n\n\nThe scores\n\u00b6\n\n\nAll scores are awarded to users and involve datasets, flows, tasks and runs, or knowledge pieces in short.\n\n\n    \n\n        \nActivity \n\n        \nActivity score is awarded to users for contributing to the knowledge base of OpenML. This includes uploading knowledge pieces, leaving likes and downloading new knowledge pieces. Uploads are rewarded strongest, with 3 activity, followed by likes, with 2 activity, and downloads are rewarded the least, with 1 activity.\n\n    \n\n    \n\n        \nReach \n\n        \nReach score is awarded to knowledge pieces and by extension their uploaders for the expressed interest of other users. It is increased by 2 for every user that leaves a like on a knowledge piece and increased by 1 for every user that downloads it for the first time.\n\n    \n\n    \n\n        \nImpact \n\n        \nImpact score is awarded to knowledge pieces and by extension their uploaders for the reuse of these knowledge pieces. A dataset is reused if when it is used as input in a task while flows and tasks are reused in runs. 1 Impact is awarded for every reuse by a user that is not the uploader. Impact of a reused knowledge piece is further increased by half of the acquired reach and half of the acquired impact of a reuse, usually rounded down. So the impact of a dataset that is used in a single task with reach 10 and impact 5, is 8 (\u230a1+0.5*10+0.5*5 \u230b).\n\n    \n\n\n\n\nThe rationale\n\u00b6\n\n\nOne of OpenML's core ideas is to create an open science environment for sharing and exploration of knowledge while getting credit for your work. The \nactivity\n score serves the encouragement of sharing and exploration. \nReach\n makes exploration easier (by finding well liked, and/or often downloaded knowledge pieces), while also providing a form of credit to the user. \nImpact\n is another form of credit that is closer in concept to citation scores.\n\n\nWhere to find it\n\u00b6\n\n\nThe number of likes and downloads as well as the reach and impact of knowledge pieces can be found on the top of their respective pages, for example the \nIris data set\n. In the top right you will also find the new Like button next to the already familiar download button.\n\n\nWhen searching for knowledge pieces on \nthe search page\n, you will now be able to see the statistics mentioned above as well. In addition you can sort the search results on their downloads, likes, reach or impact.\n\n\nOn user profiles you will find all statistics relevant to that user, as well as graphs of their progress on the three scores.\n\n\nBadges\n\u00b6\n\n\nBadges are intended to provide discrete goals for users to aim for. They are only in a conceptual phase, depending on the community's reaction they will be further developed.\n\n\nThe badges a user has acquired can be found on their user profile below the score graphs. The currently implemented badges are:\n\n\n\n\nClockwork Scientist \n For being active every day for a period of time.\n\nTeam Player \n For collaborating with other users; reusing a knowledge piece of someone who has reused a knowledge piece of yours.\n\nGood News Everyone \n For achieving a high reach on singular knowledge piece you uploaded.\n\n\n\n\nDownvotes\n\u00b6\n\n\nAlthough not part of the scores, downvotes have also been introduced. They are intended to indicate a flaw of a data set, flow, task or run that can be fixed, for example a missing description.\n\n\nIf you want to indicate something is wrong with a knowledge piece, click the number of issues statistic at the top the page. A panel will open where you either agree with an already raised issue anonymously or submit your own issue (not anonymously).\n\n\nYou can also sort search results by the number of downvotes, or issues on \nthe search page\n.\n\n\nOpting out\n\u00b6\n\n\nIf you really do not like the altmetrics you can opt-out by changing the setting on your profile. This hides your scores and badges from other users and hides their scores and badges from you. You will still be able to see the number of likes, downloads and downvotes on knowledge pieces, and your likes, downloads and downvotes will still be counted.",
            "title": "Altmetrics"
        },
        {
            "location": "/altmetrics/#the-scores",
            "text": "All scores are awarded to users and involve datasets, flows, tasks and runs, or knowledge pieces in short.",
            "title": "The scores"
        },
        {
            "location": "/altmetrics/#the-rationale",
            "text": "One of OpenML's core ideas is to create an open science environment for sharing and exploration of knowledge while getting credit for your work. The  activity  score serves the encouragement of sharing and exploration.  Reach  makes exploration easier (by finding well liked, and/or often downloaded knowledge pieces), while also providing a form of credit to the user.  Impact  is another form of credit that is closer in concept to citation scores.",
            "title": "The rationale"
        },
        {
            "location": "/altmetrics/#where-to-find-it",
            "text": "The number of likes and downloads as well as the reach and impact of knowledge pieces can be found on the top of their respective pages, for example the  Iris data set . In the top right you will also find the new Like button next to the already familiar download button.  When searching for knowledge pieces on  the search page , you will now be able to see the statistics mentioned above as well. In addition you can sort the search results on their downloads, likes, reach or impact.  On user profiles you will find all statistics relevant to that user, as well as graphs of their progress on the three scores.",
            "title": "Where to find it"
        },
        {
            "location": "/altmetrics/#badges",
            "text": "Badges are intended to provide discrete goals for users to aim for. They are only in a conceptual phase, depending on the community's reaction they will be further developed.  The badges a user has acquired can be found on their user profile below the score graphs. The currently implemented badges are:   Clockwork Scientist   For being active every day for a period of time. Team Player   For collaborating with other users; reusing a knowledge piece of someone who has reused a knowledge piece of yours. Good News Everyone   For achieving a high reach on singular knowledge piece you uploaded.",
            "title": "Badges"
        },
        {
            "location": "/altmetrics/#downvotes",
            "text": "Although not part of the scores, downvotes have also been introduced. They are intended to indicate a flaw of a data set, flow, task or run that can be fixed, for example a missing description.  If you want to indicate something is wrong with a knowledge piece, click the number of issues statistic at the top the page. A panel will open where you either agree with an already raised issue anonymously or submit your own issue (not anonymously).  You can also sort search results by the number of downvotes, or issues on  the search page .",
            "title": "Downvotes"
        },
        {
            "location": "/altmetrics/#opting-out",
            "text": "If you really do not like the altmetrics you can opt-out by changing the setting on your profile. This hides your scores and badges from other users and hides their scores and badges from you. You will still be able to see the number of likes, downloads and downvotes on knowledge pieces, and your likes, downloads and downvotes will still be counted.",
            "title": "Opting out"
        },
        {
            "location": "/terms/",
            "text": "Honor Code\n\u00b6\n\n\nBy \njoining OpenML\n, you join a special worldwide community of data scientists building on each other's results and connecting their minds as efficiently as possible. This community depends on your motivation to share data, tools and ideas, and to do so with honesty. In return, you will gain trust, visibility and reputation, igniting online collaborations and studies that otherwise may not have happened.\n\n\n\n\nBy using any part of OpenML, you agree to:\n\n\n\n\nGive credit where credit is due\n. Cite the authors whose work you are building on, or build collaborations where appropriate.\n\n\nGive back to the community\n by sharing your own data as openly and as soon as possible, or by helping the community in other ways. In doing so, you gain visibility and impact (citations).\n\n\nShare data according to your best efforts\n. \nEverybody\n make mistakes, but we trust you to correct them as soon as possible. Remove or flag data that cannot be trusted.\n\n\nBe polite and constructive\n in all discussions. Criticism of methods is welcomed, but personal criticisms should be avoided.\n\n\nRespect circles of trust\n. OpenML allows you to collaborate in 'circles' of trusted people to share unpublished results. Be considerate in sharing data with people outside this circle.\n\n\nDo not steal\n the work of people who openly share it. OpenML makes it easy to find all shared data (and when it was shared), thus everybody will know if you do this.\n\n\n\n\nTerms of Use\n\u00b6\n\n\nYou agree that you are responsible for your own use of OpenML.org and all content submitted by you, in accordance with the Honor Code and all applicable local, state, national and international laws.\n\n\nBy submitting or distributing content from OpenML.org, you affirm that you have the necessary rights, licenses, consents and/or permissions to reproduce and publish this content. You cannot upload sensitive or confidential data. You, and not the developers of OpenML.org, are solely responsible for your submissions.\n\n\nBy submitting content to OpenML.org, you grant OpenML.org the right to host, transfer, display and use this content, in accordance with your sharing settings and any licences granted by you. You also grant to each user a non-exclusive license to access and use this content for their own research purposes, in accordance with any licences granted by you.\n\n\nYou may maintain one user account and not let anyone else use your username and/or password. You may not impersonate other persons.\n\n\nYou will not intend to damage, disable, or impair any OpenML server or interfere with any other party's use and enjoyment of the service. You may not attempt to gain unauthorized access to the Site, other accounts, computer systems or networks connected to any OpenML server. You may not obtain or attempt to obtain any materials or information not intentionally made available through OpenML.\n\n\nStrictly prohibited are content that defames, harasses or threatens others, that infringes another's intellectual property, as well as indecent or unlawful content, advertising, or intentionally inaccurate information posted with the intent of misleading others. It is also prohibited to post code containing viruses, malware, spyware or any other similar software that may damage the operation of another's computer or property.",
            "title": "Terms"
        },
        {
            "location": "/terms/#honor-code",
            "text": "By  joining OpenML , you join a special worldwide community of data scientists building on each other's results and connecting their minds as efficiently as possible. This community depends on your motivation to share data, tools and ideas, and to do so with honesty. In return, you will gain trust, visibility and reputation, igniting online collaborations and studies that otherwise may not have happened.   By using any part of OpenML, you agree to:   Give credit where credit is due . Cite the authors whose work you are building on, or build collaborations where appropriate.  Give back to the community  by sharing your own data as openly and as soon as possible, or by helping the community in other ways. In doing so, you gain visibility and impact (citations).  Share data according to your best efforts .  Everybody  make mistakes, but we trust you to correct them as soon as possible. Remove or flag data that cannot be trusted.  Be polite and constructive  in all discussions. Criticism of methods is welcomed, but personal criticisms should be avoided.  Respect circles of trust . OpenML allows you to collaborate in 'circles' of trusted people to share unpublished results. Be considerate in sharing data with people outside this circle.  Do not steal  the work of people who openly share it. OpenML makes it easy to find all shared data (and when it was shared), thus everybody will know if you do this.",
            "title": "Honor Code"
        },
        {
            "location": "/terms/#terms-of-use",
            "text": "You agree that you are responsible for your own use of OpenML.org and all content submitted by you, in accordance with the Honor Code and all applicable local, state, national and international laws.  By submitting or distributing content from OpenML.org, you affirm that you have the necessary rights, licenses, consents and/or permissions to reproduce and publish this content. You cannot upload sensitive or confidential data. You, and not the developers of OpenML.org, are solely responsible for your submissions.  By submitting content to OpenML.org, you grant OpenML.org the right to host, transfer, display and use this content, in accordance with your sharing settings and any licences granted by you. You also grant to each user a non-exclusive license to access and use this content for their own research purposes, in accordance with any licences granted by you.  You may maintain one user account and not let anyone else use your username and/or password. You may not impersonate other persons.  You will not intend to damage, disable, or impair any OpenML server or interfere with any other party's use and enjoyment of the service. You may not attempt to gain unauthorized access to the Site, other accounts, computer systems or networks connected to any OpenML server. You may not obtain or attempt to obtain any materials or information not intentionally made available through OpenML.  Strictly prohibited are content that defames, harasses or threatens others, that infringes another's intellectual property, as well as indecent or unlawful content, advertising, or intentionally inaccurate information posted with the intent of misleading others. It is also prohibited to post code containing viruses, malware, spyware or any other similar software that may damage the operation of another's computer or property.",
            "title": "Terms of Use"
        },
        {
            "location": "/APIs/",
            "text": "OpenML offers a range of APIs to download and upload OpenML datasets, tasks, run algorithms on them, and share the results.\n\n\n REST\n\u00b6\n\n\nThe REST API allows you to talk directly to the OpenML server from any programming environment.\n\n\n\n\nREST Tutorial\n\n\nREST API Reference\n\n\n\n\n Python\n\u00b6\n\n\nDownload datasets into Python scripts, build models using Python machine learning libraries (e.g., \nscikit-learn\n), and share the results online, all in a few lines of code.\n\n\n\n\nPython Tutorial\n\n\nPython API Reference\n\n\nExample Jupyter notebook\n\n\nCheatsheet\n\n\nOnline Demo\n\n\n\n\n R\n\u00b6\n\n\nDownload datasets into R scripts, build models using R machine learning packages (e.g. \nmlr\n), and share the results online, again in a few lines of code.\n\n\n\n\nR Tutorial\n\n\nR API Reference\n\n\nCheatsheet\n\n\nuseR 2017 Tutorial\n\n\n\n\n Java\n\u00b6\n\n\nIf you are building machine learning systems in Java, there is also an API for that.\n\n\n\n\nJava Tutorial\n\n\nJava API Reference\n\n\n\n\n .NET (C#)\n\u00b6\n\n\nThe .NET library is under development, but already contains most of the functions available.\n\n\n\n\n.NET Tutorial\n\n\nGitHub repo\n\n\n\n\nEasy authentication\n\u00b6\n\n\nIn the interest of open science, we allow you to freely download all public resources, also through the APIs (rate limits apply when necessary).\nUploading and sharing new datasets, tasks, flows and runs (or accessing any shared/private resources) is also very easy, and requires only the API key that you can find \nin your profile\n (after logging in).\n\n\nIf you use any of the language-specific APIs, you only need to store this key in a config file and forget about it. For authenticating to the REST API, you can send your api key using Basic Auth, or by adding \n?api_key='your key'\n to your calls. If you are logged into OpenML.org, this will be done automatically.",
            "title": "OpenML APIs"
        },
        {
            "location": "/APIs/#rest",
            "text": "The REST API allows you to talk directly to the OpenML server from any programming environment.   REST Tutorial  REST API Reference",
            "title": "REST"
        },
        {
            "location": "/APIs/#python",
            "text": "Download datasets into Python scripts, build models using Python machine learning libraries (e.g.,  scikit-learn ), and share the results online, all in a few lines of code.   Python Tutorial  Python API Reference  Example Jupyter notebook  Cheatsheet  Online Demo",
            "title": "Python"
        },
        {
            "location": "/APIs/#r",
            "text": "Download datasets into R scripts, build models using R machine learning packages (e.g.  mlr ), and share the results online, again in a few lines of code.   R Tutorial  R API Reference  Cheatsheet  useR 2017 Tutorial",
            "title": "R"
        },
        {
            "location": "/APIs/#java",
            "text": "If you are building machine learning systems in Java, there is also an API for that.   Java Tutorial  Java API Reference",
            "title": "Java"
        },
        {
            "location": "/APIs/#net-c",
            "text": "The .NET library is under development, but already contains most of the functions available.   .NET Tutorial  GitHub repo",
            "title": ".NET (C#)"
        },
        {
            "location": "/APIs/#easy-authentication",
            "text": "In the interest of open science, we allow you to freely download all public resources, also through the APIs (rate limits apply when necessary).\nUploading and sharing new datasets, tasks, flows and runs (or accessing any shared/private resources) is also very easy, and requires only the API key that you can find  in your profile  (after logging in).  If you use any of the language-specific APIs, you only need to store this key in a config file and forget about it. For authenticating to the REST API, you can send your api key using Basic Auth, or by adding  ?api_key='your key'  to your calls. If you are logged into OpenML.org, this will be done automatically.",
            "title": "Easy authentication"
        },
        {
            "location": "/REST-tutorial/",
            "text": "REST tutorial\n\u00b6\n\n\nOpenML offers a RESTful Web API, with predictive URLs, for uploading and downloading machine learning resources. Try the \nAPI Documentation\n to see examples of all calls, and test them right in your browser.\n\n\nGetting started\n\u00b6\n\n\n\n\nREST services can be called using simple HTTP GET or POST actions.\n\n\nThe REST Endpoint URL is \nhttps://www.openml.org/api/v1/\n\n\nThe default endpoint returns data in XML. If you prefer JSON, use the endpoint \nhttps://www.openml.org/api/v1/json/\n. Note that, to upload content, you still need to use XML (at least for now).\n\n\n\n\nTesting\n\u00b6\n\n\nFor continuous integration and testing purposes, we have a test server offering the same API, but which does not affect the production server.\n\n\n\n\nThe test server REST Endpoint URL is \nhttps://test.openml.org/api/v1/\n\n\n\n\nError messages\n\u00b6\n\n\nError messages will look like this:\n\n\n<oml:error\n \nxmlns:oml=\n\"http://openml.org/error\"\n>\n\n\n<oml:code>\n100\n</oml:code>\n\n\n<oml:message>\nPlease invoke legal function\n</oml:message>\n\n\n<oml:additional_information>\nAdditional information, not always available.\n</oml:additional_information>\n\n\n</oml:error>\n\n\n\n\n\nAll error messages are listed in the API documentation. E.g. try to get a non-existing dataset:\n\n\n\n\nin XML: \nhttps://www.openml.org/api_new/v1/data/99999\n\n\nin JSON: \nhttps://www.openml.org/api_new/v1/json/data/99999\n\n\n\n\nExamples\n\u00b6\n\n\nYou need to be logged in for these examples to work.\n\n\nDownload a dataset\n\u00b6\n\n\n\n\n\n\nUser asks for a dataset using the \n/data/{id}\n service. The \ndataset id\n is typically part of a task, or can be found on OpenML.org.\n\n\nOpenML returns a description of the dataset as an XML file (or JSON). \nTry it now\n\n\nThe dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset.\n\n\nThe dataset is returned by the server hosting the dataset. This can be OpenML, but also any other data repository. \nTry it now\n\n\n\n\nDownload a flow\n\u00b6\n\n\n\n\n\n\nUser asks for a flow using the \n/flow/{id}\n service and a \nflow id\n. The \nflow id\n can be found on OpenML.org.\n\n\nOpenML returns a description of the flow as an XML file (or JSON). \nTry it now\n\n\nThe flow description contains the URL where the flow can be downloaded (e.g. GitHub), either as source, binary or both, as well as additional information on history, dependencies and licence. The user calls the right URL to download it.\n\n\nThe flow is returned by the server hosting it. This can be OpenML, but also any other code repository. \nTry it now\n\n\n\n\nDownload a task\n\u00b6\n\n\n\n\n\n\nUser asks for a task using the \n/task/{id}\n service and a \ntask id\n. The \ntask id\n is typically returned when searching for tasks.\n\n\nOpenML returns a description of the task as an XML file (or JSON). \nTry it now\n\n\nThe task description contains the \ndataset id\n(s) of the datasets involved in this task. The user asks for the dataset using the \n/data/{id}\n service and the \ndataset id\n.\n\n\nOpenML returns a description of the dataset as an XML file (or JSON). \nTry it now\n\n\nThe dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset.\n\n\nThe dataset is returned by the server hosting it. This can be OpenML, but also any other data repository. \nTry it now\n\n\nThe task description may also contain links to other resources, such as the train-test splits to be used in cross-validation. The user calls that URL to download the train-test splits.\n\n\nThe train-test splits are returned by OpenML. \nTry it now",
            "title": "Tutorial"
        },
        {
            "location": "/REST-tutorial/#rest-tutorial",
            "text": "OpenML offers a RESTful Web API, with predictive URLs, for uploading and downloading machine learning resources. Try the  API Documentation  to see examples of all calls, and test them right in your browser.",
            "title": "REST tutorial"
        },
        {
            "location": "/REST-tutorial/#getting-started",
            "text": "REST services can be called using simple HTTP GET or POST actions.  The REST Endpoint URL is  https://www.openml.org/api/v1/  The default endpoint returns data in XML. If you prefer JSON, use the endpoint  https://www.openml.org/api/v1/json/ . Note that, to upload content, you still need to use XML (at least for now).",
            "title": "Getting started"
        },
        {
            "location": "/REST-tutorial/#testing",
            "text": "For continuous integration and testing purposes, we have a test server offering the same API, but which does not affect the production server.   The test server REST Endpoint URL is  https://test.openml.org/api/v1/",
            "title": "Testing"
        },
        {
            "location": "/REST-tutorial/#error-messages",
            "text": "Error messages will look like this:  <oml:error   xmlns:oml= \"http://openml.org/error\" >  <oml:code> 100 </oml:code>  <oml:message> Please invoke legal function </oml:message>  <oml:additional_information> Additional information, not always available. </oml:additional_information>  </oml:error>   All error messages are listed in the API documentation. E.g. try to get a non-existing dataset:   in XML:  https://www.openml.org/api_new/v1/data/99999  in JSON:  https://www.openml.org/api_new/v1/json/data/99999",
            "title": "Error messages"
        },
        {
            "location": "/REST-tutorial/#examples",
            "text": "You need to be logged in for these examples to work.",
            "title": "Examples"
        },
        {
            "location": "/REST-tutorial/#download-a-dataset",
            "text": "User asks for a dataset using the  /data/{id}  service. The  dataset id  is typically part of a task, or can be found on OpenML.org.  OpenML returns a description of the dataset as an XML file (or JSON).  Try it now  The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset.  The dataset is returned by the server hosting the dataset. This can be OpenML, but also any other data repository.  Try it now",
            "title": "Download a dataset"
        },
        {
            "location": "/REST-tutorial/#download-a-flow",
            "text": "User asks for a flow using the  /flow/{id}  service and a  flow id . The  flow id  can be found on OpenML.org.  OpenML returns a description of the flow as an XML file (or JSON).  Try it now  The flow description contains the URL where the flow can be downloaded (e.g. GitHub), either as source, binary or both, as well as additional information on history, dependencies and licence. The user calls the right URL to download it.  The flow is returned by the server hosting it. This can be OpenML, but also any other code repository.  Try it now",
            "title": "Download a flow"
        },
        {
            "location": "/REST-tutorial/#download-a-task",
            "text": "User asks for a task using the  /task/{id}  service and a  task id . The  task id  is typically returned when searching for tasks.  OpenML returns a description of the task as an XML file (or JSON).  Try it now  The task description contains the  dataset id (s) of the datasets involved in this task. The user asks for the dataset using the  /data/{id}  service and the  dataset id .  OpenML returns a description of the dataset as an XML file (or JSON).  Try it now  The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset.  The dataset is returned by the server hosting it. This can be OpenML, but also any other data repository.  Try it now  The task description may also contain links to other resources, such as the train-test splits to be used in cross-validation. The user calls that URL to download the train-test splits.  The train-test splits are returned by OpenML.  Try it now",
            "title": "Download a task"
        },
        {
            "location": "/REST-API/",
            "text": "REST APIs\n\u00b6\n\n\nThe REST API allows you to talk directly to the OpenML server from any programming environment.\n\n\nThe REST API has two parts (with different endpoints):\n\n\nThe Main REST API\n\u00b6\n\n\nHas all main functions to download OpenML data or share your own.\n\n\nAPI Documentation\n\n\nThe File API\n\u00b6\n\n\nServes datasets and other files stored on OpenML servers.\n\n\nFile API Documentation",
            "title": "API Reference"
        },
        {
            "location": "/REST-API/#rest-apis",
            "text": "The REST API allows you to talk directly to the OpenML server from any programming environment.  The REST API has two parts (with different endpoints):",
            "title": "REST APIs"
        },
        {
            "location": "/REST-API/#the-main-rest-api",
            "text": "Has all main functions to download OpenML data or share your own.  API Documentation",
            "title": "The Main REST API"
        },
        {
            "location": "/REST-API/#the-file-api",
            "text": "Serves datasets and other files stored on OpenML servers.  File API Documentation",
            "title": "The File API"
        },
        {
            "location": "/Java-guide/",
            "text": "The Java API allows you connect to OpenML from Java applications.\n\n\nJava Docs\n\u00b6\n\n\nRead the full Java Docs\n\n\nDownload\n\u00b6\n\n\nStable releases of the Java API are available from \nMaven Central\n\nOr, you can check out the developer version from \nGitHub\n\n\nInclude the jar file in your projects as usual, or \ninstall via Maven\n.\n\n\nQuick Start\n\u00b6\n\n\n\n\nCreate an \nOpenmlConnector\n instance with your authentication details. This will create a client with all OpenML functionalities.\n\n\nOpenmlConnector client = new OpenmlConnector(\"api_key\")\n\n\n\n\n\n\n\n\nAll functions are described in the \nJava Docs\n.\n\n\nDownloading\n\u00b6\n\n\nTo download data, flows, tasks, runs, etc. you need the unique id of that resource. The id is shown on each item's webpage and in the corresponding url. For instance, let's download \nData set 1\n. The following returns a DataSetDescription object that contains all information about that data set.\n\n\nDataSetDescription data = client.dataGet(1);\n\n\n\n\nYou can also \nsearch\n for the items you need online, and click the icon to get all id's that match a search.\n\n\nUploading\n\u00b6\n\n\nTo upload data, flows, runs, etc. you need to provide a description of the object. We provide wrapper classes to provide this information, e.g. \nDataSetDescription\n, as well as to capture the server response, e.g. \nUploadDataSet\n, which always includes the generated id for reference:\n\n\nDataSetDescription description = new DataSetDescription( \"iris\", \"The famous iris dataset\", \"arff\", \"class\");\nUploadDataSet result = client.dataUpload( description, datasetFile );\nint data_id = result.getId();\n\n\n\n\nMore details are given in the corresponding functions below. Also see the \nJava Docs\n for all possible inputs and return values.\n\n\nData download\n\u00b6\n\n\ndataGet(int data_id)\n\u00b6\n\n\nRetrieves the description of a specified data set.\n\n\nDataSetDescription data = client.dataGet(1);\nString name = data.getName();\nString version = data.getVersion();\nString description = data.getDescription();\nString url = data.getUrl();\n\n\n\n\ndataFeatures(int data_id)\n\u00b6\n\n\nRetrieves the description of the features of a specified data set.\n\n\nDataFeature reponse = client.dataFeatures(1);\nDataFeature.Feature[] features = reponse.getFeatures();\nString name = features[0].getName();\nString type = features[0].getDataType();\nboolean isTarget = features[0].getIs_target();\n\n\n\n\ndataQuality(int data_id)\n\u00b6\n\n\nRetrieves the description of the qualities (meta-features) of a specified data set.\n\n\n    DataQuality response = client.dataQuality(1);\n    DataQuality.Quality[] qualities = reponse.getQualities();\n    String name = qualities[0].getName();\n    String value = qualities[0].getValue();\n\n\n\n\ndataQuality(int data_id, int start, int end, int interval_size)\n\u00b6\n\n\nFor data streams. Retrieves the description of the qualities (meta-features) of a specified portion of a data stream.\n\n\n    DataQuality qualities = client.dataQuality(1,0,10000,null);\n\n\n\n\ndataQualityList()\n\u00b6\n\n\nRetrieves a list of all data qualities known to OpenML.\n\n\n    DataQualityList response = client.dataQualityList();\n    String[] qualities = response.getQualities();\n\n\n\n\nData upload\n\u00b6\n\n\ndataUpload(DataSetDescription description, File dataset)\n\u00b6\n\n\nUploads a data set file to OpenML given a description. Throws an exception if the upload failed, see \nopenml.data.upload\n for error codes.\n\n\n    DataSetDescription dataset = new DataSetDescription( \"iris\", \"The iris dataset\", \"arff\", \"class\");\n    UploadDataSet data = client.dataUpload( dataset, new File(\"data/path\"));\n    int data_id = result.getId();\n\n\n\n\ndataUpload(DataSetDescription description)\n\u00b6\n\n\nRegisters an existing dataset (hosted elsewhere). The description needs to include the url of the data set. Throws an exception if the upload failed, see \nopenml.data.upload\n for error codes.\n\n\n    DataSetDescription description = new DataSetDescription( \"iris\", \"The iris dataset\", \"arff\", \"class\");\n    description.setUrl(\"http://datarepository.org/mydataset\");\n    UploadDataSet data = client.dataUpload( description );\n    int data_id = result.getId();\n\n\n\n\nFlow download\n\u00b6\n\n\nflowGet(int flow_id)\n\u00b6\n\n\nRetrieves the description of the flow/implementation with the given id.\n\n\n    Implementation flow = client.flowGet(100);\n    String name = flow.getName();\n    String version = flow.getVersion();\n    String description = flow.getDescription();\n    String binary_url = flow.getBinary_url();\n    String source_url = flow.getSource_url();\n    Parameter[] parameters = flow.getParameter();\n\n\n\n\nFlow management\n\u00b6\n\n\nflowOwned()\n\u00b6\n\n\nRetrieves an array of id's of all flows/implementations owned by you.\n\n\n    ImplementationOwned response = client.flowOwned();\n    Integer[] ids = response.getIds();\n\n\n\n\nflowExists(String name, String version)\n\u00b6\n\n\nChecks whether an implementation with the given name and version is already registered on OpenML.\n\n\n    ImplementationExists check = client.flowExists(\"weka.j48\", \"3.7.12\");\n    boolean exists = check.exists();\n    int flow_id = check.getId();\n\n\n\n\nflowDelete(int id)\n\u00b6\n\n\nRemoves the flow with the given id (if you are its owner).\n\n\n    ImplementationDelete response = client.openmlImplementationDelete(100);\n\n\n\n\nFlow upload\n\u00b6\n\n\nflowUpload(Implementation description, File binary, File source)\n\u00b6\n\n\nUploads implementation files (binary and/or source) to OpenML given a description.\n\n\n    Implementation flow = new Implementation(\"weka.J48\", \"3.7.12\", \"description\", \"Java\", \"WEKA 3.7.12\")\n    UploadImplementation response = client.flowUpload( flow, new File(\"code.jar\"), new File(\"source.zip\"));\n    int flow_id = response.getId();\n\n\n\n\nTask download\n\u00b6\n\n\ntaskGet(int task_id)\n\u00b6\n\n\nRetrieves the description of the task with the given id.\n\n\n    Task task = client.taskGet(1);\n    String task_type = task.getTask_type();\n    Input[] inputs = task.getInputs();\n    Output[] outputs = task.getOutputs();\n\n\n\n\ntaskEvaluations(int task_id)\n\u00b6\n\n\nRetrieves all evaluations for the task with the given id.\n\n\n    TaskEvaluations response = client.taskEvaluations(1);\n    Evaluation[] evaluations = response.getEvaluation();\n\n\n\n\ntaskEvaluations(int task_id, int start, int end, int interval_size)\n\u00b6\n\n\nFor data streams. Retrieves all evaluations for the task over the specified window of the stream.\n\n\n    TaskEvaluations response = client.taskEvaluations(1);\n    Evaluation[] evaluations = response.getEvaluation();\n\n\n\n\nRun download\n\u00b6\n\n\nrunGet(int run_id)\n\u00b6\n\n\nRetrieves the description of the run with the given id.\n\n\n    Run run = client.runGet(1);\n    int task_id = run.getTask_id();\n    int flow_id = run.getImplementation_id();\n    Parameter_setting[] settings = run.getParameter_settings()\n    EvaluationScore[] scores = run.getOutputEvaluation();\n\n\n\n\nRun management\n\u00b6\n\n\nrunDelete(int run_id)\n\u00b6\n\n\nDeletes the run with the given id (if you are its owner).\n\n\n    RunDelete response = client.runDelete(1);\n\n\n\n\nRun upload\n\u00b6\n\n\nrunUpload(Run description, Map<String,File> output_files)\n\u00b6\n\n\nUploads a run to OpenML, including a description and a set of output files depending on the task type.\n\n\n    Run.Parameter_setting[] parameter_settings = new Run.Parameter_setting[1];\n    parameter_settings[0] = Run.Parameter_setting(null, \"M\", \"2\");\n    Run run = new Run(\"1\", null, \"100\", \"setup_string\", parameter_settings);\n    Map outputs = new HashMap<String,File>();\n    outputs.add(\"predictions\",new File(\"predictions.arff\"));\n    UploadRun response = client.runUpload( run, outputs);\n    int run_id = response.getRun_id();",
            "title": "Tutorial"
        },
        {
            "location": "/Java-guide/#java-docs",
            "text": "Read the full Java Docs",
            "title": "Java Docs"
        },
        {
            "location": "/Java-guide/#download",
            "text": "Stable releases of the Java API are available from  Maven Central \nOr, you can check out the developer version from  GitHub  Include the jar file in your projects as usual, or  install via Maven .",
            "title": "Download"
        },
        {
            "location": "/Java-guide/#quick-start",
            "text": "Create an  OpenmlConnector  instance with your authentication details. This will create a client with all OpenML functionalities.  OpenmlConnector client = new OpenmlConnector(\"api_key\")     All functions are described in the  Java Docs .",
            "title": "Quick Start"
        },
        {
            "location": "/Java-guide/#downloading",
            "text": "To download data, flows, tasks, runs, etc. you need the unique id of that resource. The id is shown on each item's webpage and in the corresponding url. For instance, let's download  Data set 1 . The following returns a DataSetDescription object that contains all information about that data set.  DataSetDescription data = client.dataGet(1);  You can also  search  for the items you need online, and click the icon to get all id's that match a search.",
            "title": "Downloading"
        },
        {
            "location": "/Java-guide/#uploading",
            "text": "To upload data, flows, runs, etc. you need to provide a description of the object. We provide wrapper classes to provide this information, e.g.  DataSetDescription , as well as to capture the server response, e.g.  UploadDataSet , which always includes the generated id for reference:  DataSetDescription description = new DataSetDescription( \"iris\", \"The famous iris dataset\", \"arff\", \"class\");\nUploadDataSet result = client.dataUpload( description, datasetFile );\nint data_id = result.getId();  More details are given in the corresponding functions below. Also see the  Java Docs  for all possible inputs and return values.",
            "title": "Uploading"
        },
        {
            "location": "/Java-guide/#data-download",
            "text": "",
            "title": "Data download"
        },
        {
            "location": "/Java-guide/#datagetint-data_id",
            "text": "Retrieves the description of a specified data set.  DataSetDescription data = client.dataGet(1);\nString name = data.getName();\nString version = data.getVersion();\nString description = data.getDescription();\nString url = data.getUrl();",
            "title": "dataGet(int data_id)"
        },
        {
            "location": "/Java-guide/#datafeaturesint-data_id",
            "text": "Retrieves the description of the features of a specified data set.  DataFeature reponse = client.dataFeatures(1);\nDataFeature.Feature[] features = reponse.getFeatures();\nString name = features[0].getName();\nString type = features[0].getDataType();\nboolean isTarget = features[0].getIs_target();",
            "title": "dataFeatures(int data_id)"
        },
        {
            "location": "/Java-guide/#dataqualityint-data_id",
            "text": "Retrieves the description of the qualities (meta-features) of a specified data set.      DataQuality response = client.dataQuality(1);\n    DataQuality.Quality[] qualities = reponse.getQualities();\n    String name = qualities[0].getName();\n    String value = qualities[0].getValue();",
            "title": "dataQuality(int data_id)"
        },
        {
            "location": "/Java-guide/#dataqualityint-data_id-int-start-int-end-int-interval_size",
            "text": "For data streams. Retrieves the description of the qualities (meta-features) of a specified portion of a data stream.      DataQuality qualities = client.dataQuality(1,0,10000,null);",
            "title": "dataQuality(int data_id, int start, int end, int interval_size)"
        },
        {
            "location": "/Java-guide/#dataqualitylist",
            "text": "Retrieves a list of all data qualities known to OpenML.      DataQualityList response = client.dataQualityList();\n    String[] qualities = response.getQualities();",
            "title": "dataQualityList()"
        },
        {
            "location": "/Java-guide/#data-upload",
            "text": "",
            "title": "Data upload"
        },
        {
            "location": "/Java-guide/#datauploaddatasetdescription-description-file-dataset",
            "text": "Uploads a data set file to OpenML given a description. Throws an exception if the upload failed, see  openml.data.upload  for error codes.      DataSetDescription dataset = new DataSetDescription( \"iris\", \"The iris dataset\", \"arff\", \"class\");\n    UploadDataSet data = client.dataUpload( dataset, new File(\"data/path\"));\n    int data_id = result.getId();",
            "title": "dataUpload(DataSetDescription description, File dataset)"
        },
        {
            "location": "/Java-guide/#datauploaddatasetdescription-description",
            "text": "Registers an existing dataset (hosted elsewhere). The description needs to include the url of the data set. Throws an exception if the upload failed, see  openml.data.upload  for error codes.      DataSetDescription description = new DataSetDescription( \"iris\", \"The iris dataset\", \"arff\", \"class\");\n    description.setUrl(\"http://datarepository.org/mydataset\");\n    UploadDataSet data = client.dataUpload( description );\n    int data_id = result.getId();",
            "title": "dataUpload(DataSetDescription description)"
        },
        {
            "location": "/Java-guide/#flow-download",
            "text": "",
            "title": "Flow download"
        },
        {
            "location": "/Java-guide/#flowgetint-flow_id",
            "text": "Retrieves the description of the flow/implementation with the given id.      Implementation flow = client.flowGet(100);\n    String name = flow.getName();\n    String version = flow.getVersion();\n    String description = flow.getDescription();\n    String binary_url = flow.getBinary_url();\n    String source_url = flow.getSource_url();\n    Parameter[] parameters = flow.getParameter();",
            "title": "flowGet(int flow_id)"
        },
        {
            "location": "/Java-guide/#flow-management",
            "text": "",
            "title": "Flow management"
        },
        {
            "location": "/Java-guide/#flowowned",
            "text": "Retrieves an array of id's of all flows/implementations owned by you.      ImplementationOwned response = client.flowOwned();\n    Integer[] ids = response.getIds();",
            "title": "flowOwned()"
        },
        {
            "location": "/Java-guide/#flowexistsstring-name-string-version",
            "text": "Checks whether an implementation with the given name and version is already registered on OpenML.      ImplementationExists check = client.flowExists(\"weka.j48\", \"3.7.12\");\n    boolean exists = check.exists();\n    int flow_id = check.getId();",
            "title": "flowExists(String name, String version)"
        },
        {
            "location": "/Java-guide/#flowdeleteint-id",
            "text": "Removes the flow with the given id (if you are its owner).      ImplementationDelete response = client.openmlImplementationDelete(100);",
            "title": "flowDelete(int id)"
        },
        {
            "location": "/Java-guide/#flow-upload",
            "text": "",
            "title": "Flow upload"
        },
        {
            "location": "/Java-guide/#flowuploadimplementation-description-file-binary-file-source",
            "text": "Uploads implementation files (binary and/or source) to OpenML given a description.      Implementation flow = new Implementation(\"weka.J48\", \"3.7.12\", \"description\", \"Java\", \"WEKA 3.7.12\")\n    UploadImplementation response = client.flowUpload( flow, new File(\"code.jar\"), new File(\"source.zip\"));\n    int flow_id = response.getId();",
            "title": "flowUpload(Implementation description, File binary, File source)"
        },
        {
            "location": "/Java-guide/#task-download",
            "text": "",
            "title": "Task download"
        },
        {
            "location": "/Java-guide/#taskgetint-task_id",
            "text": "Retrieves the description of the task with the given id.      Task task = client.taskGet(1);\n    String task_type = task.getTask_type();\n    Input[] inputs = task.getInputs();\n    Output[] outputs = task.getOutputs();",
            "title": "taskGet(int task_id)"
        },
        {
            "location": "/Java-guide/#taskevaluationsint-task_id",
            "text": "Retrieves all evaluations for the task with the given id.      TaskEvaluations response = client.taskEvaluations(1);\n    Evaluation[] evaluations = response.getEvaluation();",
            "title": "taskEvaluations(int task_id)"
        },
        {
            "location": "/Java-guide/#taskevaluationsint-task_id-int-start-int-end-int-interval_size",
            "text": "For data streams. Retrieves all evaluations for the task over the specified window of the stream.      TaskEvaluations response = client.taskEvaluations(1);\n    Evaluation[] evaluations = response.getEvaluation();",
            "title": "taskEvaluations(int task_id, int start, int end, int interval_size)"
        },
        {
            "location": "/Java-guide/#run-download",
            "text": "",
            "title": "Run download"
        },
        {
            "location": "/Java-guide/#rungetint-run_id",
            "text": "Retrieves the description of the run with the given id.      Run run = client.runGet(1);\n    int task_id = run.getTask_id();\n    int flow_id = run.getImplementation_id();\n    Parameter_setting[] settings = run.getParameter_settings()\n    EvaluationScore[] scores = run.getOutputEvaluation();",
            "title": "runGet(int run_id)"
        },
        {
            "location": "/Java-guide/#run-management",
            "text": "",
            "title": "Run management"
        },
        {
            "location": "/Java-guide/#rundeleteint-run_id",
            "text": "Deletes the run with the given id (if you are its owner).      RunDelete response = client.runDelete(1);",
            "title": "runDelete(int run_id)"
        },
        {
            "location": "/Java-guide/#run-upload",
            "text": "",
            "title": "Run upload"
        },
        {
            "location": "/Java-guide/#runuploadrun-description-mapstringfile-output_files",
            "text": "Uploads a run to OpenML, including a description and a set of output files depending on the task type.      Run.Parameter_setting[] parameter_settings = new Run.Parameter_setting[1];\n    parameter_settings[0] = Run.Parameter_setting(null, \"M\", \"2\");\n    Run run = new Run(\"1\", null, \"100\", \"setup_string\", parameter_settings);\n    Map outputs = new HashMap<String,File>();\n    outputs.add(\"predictions\",new File(\"predictions.arff\"));\n    UploadRun response = client.runUpload( run, outputs);\n    int run_id = response.getRun_id();",
            "title": "runUpload(Run description, Map&lt;String,File&gt; output_files)"
        },
        {
            "location": "/Python-start/",
            "text": "Fallback link for browsers that don't support iframes",
            "title": "Start"
        },
        {
            "location": "/Python-guide/",
            "text": "Fallback link for browsers that don't support iframes",
            "title": "Tutorial"
        },
        {
            "location": "/Python-API/",
            "text": "Fallback link for browsers that don't support iframes",
            "title": "API Reference"
        },
        {
            "location": "/R-guide/",
            "text": "Fallback link for browsers that don't support iframes",
            "title": "Tutorial"
        },
        {
            "location": "/R-API/",
            "text": "Fallback link for browsers that don't support iframes",
            "title": "API Reference"
        },
        {
            "location": "/NET-API/",
            "text": "The .Net API allows you connect to OpenML from .Net applications.\n\n\nDownload\n\u00b6\n\n\nStable releases of the .Net API are available via \nNuGet\n. Use the NuGet package explorer in the Visual Studia, write \u201cInstall-Package openMl\u201d to the NuGet package manager console or download the whole package from the NuGet website and add it into your project. Or, you can check out the developer version from \nGitHub\n.\n\n\nQuick Start\n\u00b6\n\n\nCreate an \nOpenmlConnector\n instance with your api key. You can find this key in your account settings. This will create a client with OpenML functionalities, The functionalities mirror the OpenMlApi and not all of them are (yet) implemented. If you need some feature, don\u2019t hesitate contact us via our Git page.\n\n\n\n\n\n    `var connector = new OpenMlConnector(\"YOURAPIKEY\");`\n\n\n\n\n\n\nAll OpenMlConnector methods are documented via the usual .Net comments.\n\n\nGet dataset description\n\u00b6\n\n\n\n\n\n    `var datasetDescription = connector.GetDatasetDescription(1);`\n\n\n\n\n\n\nList datasets\n\u00b6\n\n\n\n\n\n    `var data = connector.ListDatasets();`\n\n\n\n\n\n\nGet run\n\u00b6\n\n\n\n\n\n    `var run = connector.GetRun(1);`\n\n\n\n\n\n\nList task types\n\u00b6\n\n\n\n\n\n    `var taskTypes = connector.ListTaskTypes();`\n\n\n\n\n\n\nGet task type\n\u00b6\n\n\n\n\n\n    `var taskType = connector.GetTaskType(1);`\n\n\n\n\n\n\nList evaluation measures\n\u00b6\n\n\n\n\n\n    `var measures = connector.ListEvaluationMeasures();`\n\n\n\n\n\n\nList estimation procedures\n\u00b6\n\n\n\n\n\n    `var estimationProcs = connector.ListEstimationProcedures();`\n\n\n\n\n\n\nGet estimation procedure\n\u00b6\n\n\n\n\n\n    `var estimationProc = connector.GetEstimationProcedure(1);`\n\n\n\n\n\n\nList data qualities\n\u00b6\n\n\n\n\n\n    `var dataQualities = connector.ListDataQualities();`",
            "title": "Tutorial"
        },
        {
            "location": "/NET-API/#download",
            "text": "Stable releases of the .Net API are available via  NuGet . Use the NuGet package explorer in the Visual Studia, write \u201cInstall-Package openMl\u201d to the NuGet package manager console or download the whole package from the NuGet website and add it into your project. Or, you can check out the developer version from  GitHub .",
            "title": "Download"
        },
        {
            "location": "/NET-API/#quick-start",
            "text": "Create an  OpenmlConnector  instance with your api key. You can find this key in your account settings. This will create a client with OpenML functionalities, The functionalities mirror the OpenMlApi and not all of them are (yet) implemented. If you need some feature, don\u2019t hesitate contact us via our Git page.       `var connector = new OpenMlConnector(\"YOURAPIKEY\");`   All OpenMlConnector methods are documented via the usual .Net comments.",
            "title": "Quick Start"
        },
        {
            "location": "/NET-API/#get-dataset-description",
            "text": "`var datasetDescription = connector.GetDatasetDescription(1);`",
            "title": "Get dataset description"
        },
        {
            "location": "/NET-API/#list-datasets",
            "text": "`var data = connector.ListDatasets();`",
            "title": "List datasets"
        },
        {
            "location": "/NET-API/#get-run",
            "text": "`var run = connector.GetRun(1);`",
            "title": "Get run"
        },
        {
            "location": "/NET-API/#list-task-types",
            "text": "`var taskTypes = connector.ListTaskTypes();`",
            "title": "List task types"
        },
        {
            "location": "/NET-API/#get-task-type",
            "text": "`var taskType = connector.GetTaskType(1);`",
            "title": "Get task type"
        },
        {
            "location": "/NET-API/#list-evaluation-measures",
            "text": "`var measures = connector.ListEvaluationMeasures();`",
            "title": "List evaluation measures"
        },
        {
            "location": "/NET-API/#list-estimation-procedures",
            "text": "`var estimationProcs = connector.ListEstimationProcedures();`",
            "title": "List estimation procedures"
        },
        {
            "location": "/NET-API/#get-estimation-procedure",
            "text": "`var estimationProc = connector.GetEstimationProcedure(1);`",
            "title": "Get estimation procedure"
        },
        {
            "location": "/NET-API/#list-data-qualities",
            "text": "`var dataQualities = connector.ListDataQualities();`",
            "title": "List data qualities"
        },
        {
            "location": "/developers/",
            "text": "OpenML \n Open Source\n\u00b6\n\n\nOpenML is an open source project, \nhosted on GitHub\n. We welcome everybody to help improve OpenML, and make it more useful for everyone.\n\n\nTo integrate your own machine learning tools with OpenML, \ncheck out the available APIs\n.\n\n\n We always \nlove to welcome new contributers\n, and will gladly help you in any way possible.\n\n\nGitHub repo's\n\u00b6\n\n\nYou can find relevant code in the corresponding GitHub repositories. Please also post issues in the relevant issue tracker.\n\n\n\n\n OpenML Core\n - The website, web services, and API.\n\n\n Evaluation Engine\n - Evaluate models, analyse datasets, and much more.\n\n\n Java API\n - The Java API and Java-based plugins\n\n\n R API\n - The OpenML R package\n\n\n Python API\n - The Python API\n\n\n\n\nDatabase snapshots\n\u00b6\n\n\nEverything uploaded to OpenML is available to the community. The nightly snapshot of the public database contains all experiment runs, evaluations and links to datasets, implementations and result files. In SQL format (gzipped). You can also download the \nDatabase schema\n.\n\n\n Nightly database SNAPSHOT\n\n\nIf you want to work on the website locally, you'll also need the schema for the 'private' database with non-public information.\n\n\n Private database schema\n\n\nLegacy Resources\n\u00b6\n\n\nOpenML is always evolving, but we keep hosting the resources that were used in prior publications so that others may still build on them.\n\n\n\n\n\n\n The \nexperiment database\n used in \nVanschoren et al. (2012) Experiment databases. Machine Learning 87(2), pp 127-158\n. You'll need to import this database (we used MySQL) to run queries. The database structure is described in the paper. Note that most of the experiments in this database have been rerun using OpenML, using newer algorithm implementations and stored in much more detail.\n\n\n\n\n\n\n The \nExpos\u00e9 ontology\n used in the same paper, and described in more detail \nhere\n and \nhere\n. Expos\u00e9 is used in designing our databases, and we aim to use it to export all OpenML data as Linked Open Data.",
            "title": "Welcome"
        },
        {
            "location": "/developers/#openml-open-source",
            "text": "OpenML is an open source project,  hosted on GitHub . We welcome everybody to help improve OpenML, and make it more useful for everyone.  To integrate your own machine learning tools with OpenML,  check out the available APIs .   We always  love to welcome new contributers , and will gladly help you in any way possible.",
            "title": "OpenML  Open Source"
        },
        {
            "location": "/developers/#github-repos",
            "text": "You can find relevant code in the corresponding GitHub repositories. Please also post issues in the relevant issue tracker.    OpenML Core  - The website, web services, and API.   Evaluation Engine  - Evaluate models, analyse datasets, and much more.   Java API  - The Java API and Java-based plugins   R API  - The OpenML R package   Python API  - The Python API",
            "title": "GitHub repo's"
        },
        {
            "location": "/developers/#database-snapshots",
            "text": "Everything uploaded to OpenML is available to the community. The nightly snapshot of the public database contains all experiment runs, evaluations and links to datasets, implementations and result files. In SQL format (gzipped). You can also download the  Database schema .   Nightly database SNAPSHOT  If you want to work on the website locally, you'll also need the schema for the 'private' database with non-public information.   Private database schema",
            "title": "Database snapshots"
        },
        {
            "location": "/developers/#legacy-resources",
            "text": "OpenML is always evolving, but we keep hosting the resources that were used in prior publications so that others may still build on them.     The  experiment database  used in  Vanschoren et al. (2012) Experiment databases. Machine Learning 87(2), pp 127-158 . You'll need to import this database (we used MySQL) to run queries. The database structure is described in the paper. Note that most of the experiments in this database have been rerun using OpenML, using newer algorithm implementations and stored in much more detail.     The  Expos\u00e9 ontology  used in the same paper, and described in more detail  here  and  here . Expos\u00e9 is used in designing our databases, and we aim to use it to export all OpenML data as Linked Open Data.",
            "title": "Legacy Resources"
        },
        {
            "location": "/OpenML-Docs/",
            "text": "General Documentation\n\u00b6\n\n\nThe general documentation (the one you are reading now) in written in MarkDown, can be easily edited by clicking the edit button\n(the pencil icon) on the top of every page. It will open up an editing page on \nGitHub\n (you do need to be logged in on GitHub). When you are done, add a small message explaining the change and click 'commit changes'. On the next page, just launch the pull request. We will then review it and approve the changes, or discuss them if necessary. \n\n\nThe sources are generated by \nMkDocs\n, using the \nMaterial theme\n.\nCheck these docs to see what is possible in terms of styling.\n\n\n\n\nNote\n\n\nDevelopers: To deploy the documentation, you need to have MkDocs and MkDocs-Material installed, and then run \nmkdocs gh-deploy\n in the \ndocs\n directory (with the \nmkdocs.yml\n file). This will build the HTML files and push them to the gh-pages branch of openml/OpenML. \nhttps://docs.openml.org\n is just a reverse proxy for \nhttps://openml.github.io/OpenML/\n.\n\n\n\n\nREST API\n\u00b6\n\n\nThe REST API is documented using Swagger.io, in YAML. This generates a nice web interface that also allows trying out the API calls using your own API key (when you are logged in).\n\n\nYou can edit the sources on \nSwaggerHub\n. When you are done, export to json and replace the \ndownloads/swagger.json\n file in the OpenML main GitHub repository. You need to do a pull request that is then reviewed by us. When we merge the new file the changes are immediately available.\n\n\nThe \ndata API\n can be edited in the same way.\n\n\nPython API\n\u00b6\n\n\nTo edit the tutorial, you have to edit the \nreStructuredText\n files on \nopenml-python/doc\n. When done, you can do a pull request.\n\n\nTo edit the documentation of the python functions, edit the docstrings in the \nPython code\n. When done, you can do a pull request.\n\n\n\n\nNote\n\n\nDevelopers: A CircleCI job will automatically render the documentation on every GitHub commit, using \nSphinx\n. \n\n\n\n\nR API\n\u00b6\n\n\nTo edit the tutorial, you have to edit the \nRmarkdown\n files on \nopenml-r/vignettes\n. \n\n\nTo edit the documentation of the R functions, edit the Roxygen documention next to the functions in the \nR code\n.\n\n\n\n\nNote\n\n\nDevelopers: A Travis job will automatically render the documentation on every GitHub commit, using \nknitr\n. The Roxygen documentation is updated every time a new version is released on CRAN.\n\n\n\n\nJava API\n\u00b6\n\n\nThe Java Tutorial is written in markdown and can be edited the usual way (see above).\n\n\nTo edit the documentation of the Java functions, edit the documentation next to the functions in the \nJava code\n.\n\n\n\n\nJavadocs: \nhttps://www.openml.org/docs/\n\n\n\n\n\n\nNote\n\n\nDevelopers: A Travis job will automatically render the documentation on every GitHub commit, using \nJavadoc\n.",
            "title": "Editing Documentation"
        },
        {
            "location": "/OpenML-Docs/#general-documentation",
            "text": "The general documentation (the one you are reading now) in written in MarkDown, can be easily edited by clicking the edit button\n(the pencil icon) on the top of every page. It will open up an editing page on  GitHub  (you do need to be logged in on GitHub). When you are done, add a small message explaining the change and click 'commit changes'. On the next page, just launch the pull request. We will then review it and approve the changes, or discuss them if necessary.   The sources are generated by  MkDocs , using the  Material theme .\nCheck these docs to see what is possible in terms of styling.   Note  Developers: To deploy the documentation, you need to have MkDocs and MkDocs-Material installed, and then run  mkdocs gh-deploy  in the  docs  directory (with the  mkdocs.yml  file). This will build the HTML files and push them to the gh-pages branch of openml/OpenML.  https://docs.openml.org  is just a reverse proxy for  https://openml.github.io/OpenML/ .",
            "title": "General Documentation"
        },
        {
            "location": "/OpenML-Docs/#rest-api",
            "text": "The REST API is documented using Swagger.io, in YAML. This generates a nice web interface that also allows trying out the API calls using your own API key (when you are logged in).  You can edit the sources on  SwaggerHub . When you are done, export to json and replace the  downloads/swagger.json  file in the OpenML main GitHub repository. You need to do a pull request that is then reviewed by us. When we merge the new file the changes are immediately available.  The  data API  can be edited in the same way.",
            "title": "REST API"
        },
        {
            "location": "/OpenML-Docs/#python-api",
            "text": "To edit the tutorial, you have to edit the  reStructuredText  files on  openml-python/doc . When done, you can do a pull request.  To edit the documentation of the python functions, edit the docstrings in the  Python code . When done, you can do a pull request.   Note  Developers: A CircleCI job will automatically render the documentation on every GitHub commit, using  Sphinx .",
            "title": "Python API"
        },
        {
            "location": "/OpenML-Docs/#r-api",
            "text": "To edit the tutorial, you have to edit the  Rmarkdown  files on  openml-r/vignettes .   To edit the documentation of the R functions, edit the Roxygen documention next to the functions in the  R code .   Note  Developers: A Travis job will automatically render the documentation on every GitHub commit, using  knitr . The Roxygen documentation is updated every time a new version is released on CRAN.",
            "title": "R API"
        },
        {
            "location": "/OpenML-Docs/#java-api",
            "text": "The Java Tutorial is written in markdown and can be edited the usual way (see above).  To edit the documentation of the Java functions, edit the documentation next to the functions in the  Java code .   Javadocs:  https://www.openml.org/docs/    Note  Developers: A Travis job will automatically render the documentation on every GitHub commit, using  Javadoc .",
            "title": "Java API"
        },
        {
            "location": "/Local-Installation/",
            "text": "Docker installation\n\u00b6\n\n\nThe easiest way to set up a local version of OpenML is to use Docker Compose following the instructions here (thanks to Rui Quintino!):\n\nhttps://github.com/openml/openml-docker-dev\n\n\nIf you run into problems, please post an issue in the same github repo.\n\n\nRequirements\n\u00b6\n\n\nYou'll need to have the following software running:\n* Apache Webserver, (with the rewrite module enabled. Is installed by default,\nnot enabled.)\n* MySQL Server.\n* PHP 5.5 or higher (comes also with Apache)\nOr just a XAMP (Mac), LAMP (Linux) or WAMP (Windows) package, which conveniently contains all these applications.\n\n\nDatabases\n\u00b6\n\n\nNext, OpenML runs on two databases, a public database with all experiment information, and a private database, with information like user accounts etc. The latest version of both databases can be downloaded here: \nhttps://www.openml.org/guide/developers\n\n\nObviously, the private database does not include any actual user account info.\n\n\nBackend\n\u00b6\n\n\nThe source code is available in the 'OpenML' repository: \nhttps://github.com/openml/OpenML\n\n\nOpenML is written in PHP, and can be 'installed' by copying all files in the 'www' or 'public_html' directory of Apache.\n\n\nAfter that, you need to provide your local paths and database accounts and passwords using the config file in:\n'APACHE_WWW_DIR'/openml_OS/config/BASE_CONFIG.php.\n\n\nIf everything is configured correctly, OpenML should now be running.\n\n\nSearch Indices\n\u00b6\n\n\nIf you want to run your own (separate) OpenML instance, and store your own data, you'll also want to build your own search indices to show all data on the website. The OpenML website is based on the ElasticSearch stack. To install it, follow the instructions here: \nhttp://knowm.org/how-to-set-up-the-elk-stack-elasticsearch-logstash-and-kibana/\n\n\nInitialization\n\u00b6\n\n\nThis script wipes all OpenML server data and rebuilds the database and search index. Replace 'openmldir' with the directory where you want OpenML to store files.\n\n\n# delete data from server\nsudo rm -rf /openmldir/*\nmkdir /openmldir/log\n\n# delete database\nmysqladmin -u \"root\" -p\"yourpassword\" DROP openml_expdb\nmysql -h localhost -u root -p\"yourpassword\" -e \"TRUNCATE openml.file;\"\n\n# reset ES search index\necho \"Deleting and recreating the ES index: \"\ncurl -XDELETE http://localhost:9200/openml\ncurl -XPUT 'localhost:9200/openml?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"settings\" : {\n        \"index\" : {\n            \"number_of_shards\" : 3,\n            \"number_of_replicas\" : 2\n        }\n    }\n}\n'\n\n# go to directory with the website source code\ncd /var/www/openml.org/public_html/\n\n# reinitiate the database\nmysql -u root -p\"yourpassword!\" < downloads/openml_expdb.sql\n\n# fill important columns\nsudo php index.php cron install_database\n\n# rebuild search index\nsudo php index.php cron initialize_es_indices\nsudo php index.php cron build_es_indices\n\nsudo chown apache:apache /openmldir/log\nsudo chown apache:apache /openmldir/log/*",
            "title": "Local Installation"
        },
        {
            "location": "/Local-Installation/#docker-installation",
            "text": "The easiest way to set up a local version of OpenML is to use Docker Compose following the instructions here (thanks to Rui Quintino!): https://github.com/openml/openml-docker-dev  If you run into problems, please post an issue in the same github repo.",
            "title": "Docker installation"
        },
        {
            "location": "/Local-Installation/#requirements",
            "text": "You'll need to have the following software running:\n* Apache Webserver, (with the rewrite module enabled. Is installed by default,\nnot enabled.)\n* MySQL Server.\n* PHP 5.5 or higher (comes also with Apache)\nOr just a XAMP (Mac), LAMP (Linux) or WAMP (Windows) package, which conveniently contains all these applications.",
            "title": "Requirements"
        },
        {
            "location": "/Local-Installation/#databases",
            "text": "Next, OpenML runs on two databases, a public database with all experiment information, and a private database, with information like user accounts etc. The latest version of both databases can be downloaded here:  https://www.openml.org/guide/developers  Obviously, the private database does not include any actual user account info.",
            "title": "Databases"
        },
        {
            "location": "/Local-Installation/#backend",
            "text": "The source code is available in the 'OpenML' repository:  https://github.com/openml/OpenML  OpenML is written in PHP, and can be 'installed' by copying all files in the 'www' or 'public_html' directory of Apache.  After that, you need to provide your local paths and database accounts and passwords using the config file in:\n'APACHE_WWW_DIR'/openml_OS/config/BASE_CONFIG.php.  If everything is configured correctly, OpenML should now be running.",
            "title": "Backend"
        },
        {
            "location": "/Local-Installation/#search-indices",
            "text": "If you want to run your own (separate) OpenML instance, and store your own data, you'll also want to build your own search indices to show all data on the website. The OpenML website is based on the ElasticSearch stack. To install it, follow the instructions here:  http://knowm.org/how-to-set-up-the-elk-stack-elasticsearch-logstash-and-kibana/",
            "title": "Search Indices"
        },
        {
            "location": "/Local-Installation/#initialization",
            "text": "This script wipes all OpenML server data and rebuilds the database and search index. Replace 'openmldir' with the directory where you want OpenML to store files.  # delete data from server\nsudo rm -rf /openmldir/*\nmkdir /openmldir/log\n\n# delete database\nmysqladmin -u \"root\" -p\"yourpassword\" DROP openml_expdb\nmysql -h localhost -u root -p\"yourpassword\" -e \"TRUNCATE openml.file;\"\n\n# reset ES search index\necho \"Deleting and recreating the ES index: \"\ncurl -XDELETE http://localhost:9200/openml\ncurl -XPUT 'localhost:9200/openml?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"settings\" : {\n        \"index\" : {\n            \"number_of_shards\" : 3,\n            \"number_of_replicas\" : 2\n        }\n    }\n}\n'\n\n# go to directory with the website source code\ncd /var/www/openml.org/public_html/\n\n# reinitiate the database\nmysql -u root -p\"yourpassword!\" < downloads/openml_expdb.sql\n\n# fill important columns\nsudo php index.php cron install_database\n\n# rebuild search index\nsudo php index.php cron initialize_es_indices\nsudo php index.php cron build_es_indices\n\nsudo chown apache:apache /openmldir/log\nsudo chown apache:apache /openmldir/log/*",
            "title": "Initialization"
        },
        {
            "location": "/Client-API-Standards/",
            "text": "This page defines a minimal standard to adhere in programming APIs.\n\n\nConfiguration file\n\u00b6\n\n\nThe configuration file resides in a directory \n.openml\n in the home directory of the user and is called config. It consists of \nkey = value\n pairs which are seperated by newlines. The following keys are defined:\n\n\n\n\napikey:\n\n\nrequired to access the server\n\n\n\n\n\n\nserver:\n\n\ndefault: \nhttp://www.openml.org\n\n\n\n\n\n\nverbosity:\n\n\n0: normal output\n\n\n1: info output\n\n\n2: debug output\n\n\n\n\n\n\ncachedir:\n\n\nif not given, will default to \nfile.path(tempdir(), \"cache\")\n.\n\n\n\n\n\n\narff.reader:\n\n\nRWeka\n: This is the standard Java parser used in Weka.\n\n\nfarff\n: The \nfarff package\n lives below the mlr-org and is a newer, faster parser without Java.\n\n\n\n\n\n\n\n\nCaching\n\u00b6\n\n\nCache invalidation\n\u00b6\n\n\nAll parts of the entities which affect experiments are immutable. The entities dataset and task have a flag \nstatus\n which tells the user whether they can be used safely.\n\n\nFile structure\n\u00b6\n\n\nCaching should be implemented for\n\n\n\n\ndatasets\n\n\ntasks\n\n\nsplits\n\n\npredictions\n\n\n\n\nand further entities might follow in the future. The cache directory \n$cache\n should be specified by the user when invoking the API. The structure in the cache directory should be as following:\n\n\n\n\nOne directory for the following entities:\n\n\n$cache/datasets\n\n\n$cache/tasks\n\n\n$cache/runs\n\n\n\n\n\n\nFor every dataset there is an extra directory for which the name is the dataset ID, e.g. \n$cache/datasets/2\n for the dataset anneal.ORIG\n\n\nThe dataset should be called \ndataset.arff\n\n\nEvery other file should be named by the API call which was used to obtain it. The XML returned by invoking \nopenml.data.qualities\n should therefore be called qualities.xml.\n\n\n\n\n\n\nFor every task there is an extra directory for which the name is the task ID, e.g. \n$cache/tasks/1\n\n\nThe task file should be called \ntask.xml\n.\n\n\nThe splits accompanying a task are stored in a file \ndatasplits.arff\n.\n\n\n\n\n\n\nFor every run there is an extra directory for which the name is the run ID, e.g. \n$cache/run/1\n\n\nThe predictions should be called \npredictions.arff\n.",
            "title": "Client API Standards"
        },
        {
            "location": "/Client-API-Standards/#configuration-file",
            "text": "The configuration file resides in a directory  .openml  in the home directory of the user and is called config. It consists of  key = value  pairs which are seperated by newlines. The following keys are defined:   apikey:  required to access the server    server:  default:  http://www.openml.org    verbosity:  0: normal output  1: info output  2: debug output    cachedir:  if not given, will default to  file.path(tempdir(), \"cache\") .    arff.reader:  RWeka : This is the standard Java parser used in Weka.  farff : The  farff package  lives below the mlr-org and is a newer, faster parser without Java.",
            "title": "Configuration file"
        },
        {
            "location": "/Client-API-Standards/#caching",
            "text": "",
            "title": "Caching"
        },
        {
            "location": "/Client-API-Standards/#cache-invalidation",
            "text": "All parts of the entities which affect experiments are immutable. The entities dataset and task have a flag  status  which tells the user whether they can be used safely.",
            "title": "Cache invalidation"
        },
        {
            "location": "/Client-API-Standards/#file-structure",
            "text": "Caching should be implemented for   datasets  tasks  splits  predictions   and further entities might follow in the future. The cache directory  $cache  should be specified by the user when invoking the API. The structure in the cache directory should be as following:   One directory for the following entities:  $cache/datasets  $cache/tasks  $cache/runs    For every dataset there is an extra directory for which the name is the dataset ID, e.g.  $cache/datasets/2  for the dataset anneal.ORIG  The dataset should be called  dataset.arff  Every other file should be named by the API call which was used to obtain it. The XML returned by invoking  openml.data.qualities  should therefore be called qualities.xml.    For every task there is an extra directory for which the name is the task ID, e.g.  $cache/tasks/1  The task file should be called  task.xml .  The splits accompanying a task are stored in a file  datasplits.arff .    For every run there is an extra directory for which the name is the run ID, e.g.  $cache/run/1  The predictions should be called  predictions.arff .",
            "title": "File structure"
        },
        {
            "location": "/API-rules/",
            "text": "Important resources API\n\u00b6\n\n\nAPI docs: \nwww.openml.org/api_docs\n\n\nController: \nhttps://github.com/openml/website/blob/master/openml_OS/controllers/Api_new.php\n\n\nModels: \nhttps://github.com/openml/website/tree/master/openml_OS/models/api/v1\n\n\nTemplates: \nhttps://github.com/openml/website/tree/master/openml_OS/views/pages/api_new/v1\n\n\nGolden Rules for Development\n\u00b6\n\n\n\n\nCode Maintainability before anything else\n. The code has to be understandable, and if not conflicting with that, short. Avoid code duplications as much as possible.\n\n\nThe API controller is the only entity giving access to the API models. Therefore, the responsibility for API access can be handled by the controller\n\n\nRead-Only operations are of the type GET. Operations that make changes in the database are of type POST or DELETE. Important, because this is the way the controller determines to allow users with a given set of privileges to access functions. \n\n\nTry to avoid direct queries to the database. Instead, use the respective models functions: 'get()', 'getWhere()', 'getById()', insert(), etc (Please make yourself familiar with the basic model: \nread-only\n and \nwrite\n)\n\n\nNo external program/script execution during API calls (with one exception: data split generation). This makes the API unnecessarily slow, hard to debug and vulnerable to crashes. If necessary, make a cronjob that executes the program / script",
            "title": "Server API Development"
        },
        {
            "location": "/API-rules/#important-resources-api",
            "text": "API docs:  www.openml.org/api_docs  Controller:  https://github.com/openml/website/blob/master/openml_OS/controllers/Api_new.php  Models:  https://github.com/openml/website/tree/master/openml_OS/models/api/v1  Templates:  https://github.com/openml/website/tree/master/openml_OS/views/pages/api_new/v1",
            "title": "Important resources API"
        },
        {
            "location": "/API-rules/#golden-rules-for-development",
            "text": "Code Maintainability before anything else . The code has to be understandable, and if not conflicting with that, short. Avoid code duplications as much as possible.  The API controller is the only entity giving access to the API models. Therefore, the responsibility for API access can be handled by the controller  Read-Only operations are of the type GET. Operations that make changes in the database are of type POST or DELETE. Important, because this is the way the controller determines to allow users with a given set of privileges to access functions.   Try to avoid direct queries to the database. Instead, use the respective models functions: 'get()', 'getWhere()', 'getById()', insert(), etc (Please make yourself familiar with the basic model:  read-only  and  write )  No external program/script execution during API calls (with one exception: data split generation). This makes the API unnecessarily slow, hard to debug and vulnerable to crashes. If necessary, make a cronjob that executes the program / script",
            "title": "Golden Rules for Development"
        },
        {
            "location": "/API-Authentication-and-User-types/",
            "text": "Authentication towards the server goes by means of a so-called api_key (a hexa-decimal string which uniquely identifies a user). Upon interaction with the server, the client passes this api_key to the server, and the server checks the rights of the user. Currently this goes by means of a get or post variable, but in the future we might want to use a header field (because of security). It is recommended to refresh your api_key every month. \n\n\nIMPORTANT: Most authentication operations are handled by the ION_Auth library (\nhttp://benedmunds.com/ion_auth/\n). DO NOT alter information directly in the user table, always use the ION_Auth API. \n\n\nA user can be part of one or many groups. The following user groups exists:\n1. Admin Group: With great power comes great responsibility. Admin users can overrule all security checks on the server, i.e., delete a dataset or run that is not theirs, or even delete a flow that contains runs.\n2. Normal Group: Level that is required for read/write interaction with the server. Almost all users are part of this group. \n3. Read-only Group: Level that can be used for read interaction with the server. If a user is part of this group, but not part of 'Normal Group', he is allowed to download content, but can not upload or delete content. \n4. Backend Group: (Work in Progress) Level that has more privileges than 'Normal Group'. Can submit Data Qualities and Evaluations. \n\n\nThe ION_Auth functions in_group(), add_to_group(), remove_from_group() and get_users_groups() are key towards interaction with these tables. \n\n\nFrontend todo: show on user page which groups he belongs to.",
            "title": "User Authentication"
        },
        {
            "location": "/Data-Formats/",
            "text": "To guarantee interoperability, we focus on a limited set of data formats. We aim to support all sorts of data, but for the moment we only fully support tabular data in the ARFF format. Please get in touch if you like to help us add support for new data formats.\n\n\nFormat definitions\n\u00b6\n\n\nARFF definition\n. Also check that attribute definitions do not mix spaces and tabs, and do not include end-of-line comments.",
            "title": "Data Formats"
        },
        {
            "location": "/Data-Formats/#format-definitions",
            "text": "ARFF definition . Also check that attribute definitions do not mix spaces and tabs, and do not include end-of-line comments.",
            "title": "Format definitions"
        },
        {
            "location": "/Data-Repositories/",
            "text": "This is a list of public dataset repositories we aim to connect to for getting more varied datasets in OpenML.\nThese have widely varying data formats, so we need both manual selection plus automatic conversion or meta-data extraction to make them easily usable.\n\n\nA collection of sources made by different users\n\n\n\n\nhttps://github.com/caesar0301/awesome-public-datasets\n\n\nhttps://dreamtolearn.com/ryan/1001_datasets\n\n\n\n\nMachine learning dataset repositories (mostly already in OpenML)\n\n\n\n\nUCI: \nhttps://archive.ics.uci.edu/ml/index.html\n\n\nKEEL: \nhttp://sci2s.ugr.es/keel/datasets.php\n\n\nLIBSVM: \nhttp://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/\n\n\nAutoWEKA datasets: \nhttp://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/\n\n\nskData package: \nhttps://github.com/jaberg/skdata/tree/master/skdata\n\n\nRdatasets: \nhttp://vincentarelbundock.github.io/Rdatasets/datasets.html\n\n\nDataBrewer: \nhttps://github.com/rmax/databrewer\n\n\n\n\nTime series data:\n\n\n\n\nUCR: \nhttp://timeseriesclassification.com/\n\n\nOlder version: \nhttp://www.cs.ucr.edu/~eamonn/time_series_data/\n\n\n\n\nDeep learning datasets (mostly image data)\n\n\n\n\nhttp://deeplearning.net/datasets/\n\n\nhttps://deeplearning4j.org/opendata\n\n\nhttp://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n\n\n\n\nExtreme classification:\n\n\n\n\nhttp://manikvarma.org/downloads/XC/XMLRepository.html\n\n\n\n\nMLData (will merge with OpenML in 2017)\n\n\n\n\nhttp://mldata.org/\n\n\n\n\nAutoWEKA datasets:\n\n\n\n\nhttp://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/\n\n\n\n\nKaggle public datasets\n\n\n\n\nhttps://www.kaggle.com/datasets\n\n\n\n\nRAMP Challenge datasets\n\n\n\n\nhttp://www.ramp.studio/data_domains\n\n\n\n\nWolfram data repository\n\n\n\n\nhttp://datarepository.wolframcloud.com/\n\n\n\n\nData.world\n\n\n\n\nhttps://data.world/\n\n\n\n\nFigshare (needs digging, lots of Excel files)\n\n\n\n\nhttps://figshare.com/search?q=dataset&quick=1\n\n\n\n\nKDNuggets list of data sets (meta-list, lots of stuff here):\n\n\n\n\nhttp://www.kdnuggets.com/datasets/index.html\n\n\n\n\nBenchmark Data Sets for Highly Imbalanced Binary Classification\n\n\n\n\nhttp://www.cs.gsu.edu/~zding/research/imbalance-data/x19data.txt\n\n\n\n\nFeature Selection Challenge Datasets\n\n\n\n\nhttp://www.nipsfsc.ecs.soton.ac.uk/datasets/\n\n\nhttp://featureselection.asu.edu/datasets.php\n\n\n\n\nBigML's list of 1000+ data sources\n\n\n\n\nhttp://blog.bigml.com/2013/02/28/data-data-data-thousands-of-public-data-sources/\n\n\n\n\nMassive list from Data Science Central.\n\n\n\n\nhttp://www.datasciencecentral.com/profiles/blogs/data-sources-for-cool-data-science-projects\n\n\n\n\nR packages (also see \nhttps://github.com/openml/openml-r/issues/185\n)\n\n\n\n\nhttp://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.html\n\n\nmlbench\n\n\nStata datasets: \nhttp://www.stata-press.com/data/r13/r.html\n\n\n\n\nUTwente Activity recognition datasets:\n\n\n\n\nhttp://ps.ewi.utwente.nl/Datasets.php\n\n\n\n\nVanderbilt:\n\n\n\n\nhttp://biostat.mc.vanderbilt.edu/wiki/Main/DataSets\n\n\n\n\nQuandl\n\n\n\n\nhttps://www.quandl.com\n\n\n\n\nMicroarray data:  \n\n\n\n\nhttp://genomics-pubs.princeton.edu/oncology/\n\n\nhttp://svitsrv25.epfl.ch/R-doc/library/multtest/html/golub.html\n\n\n\n\nMedical data:\n\n\n\n\nhttp://www.healthdata.gov/\n  \n\n\nhttp://homepages.inf.ed.ac.uk/rbf/IAPR/researchers/PPRPAGES/pprdat.htm\n  \n\n\nhttp://hcup-us.ahrq.gov/\n  \n\n\nhttps://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-Supplier.html\n  \n\n\nhttps://nsduhweb.rti.org/respweb/homepage.cfm\n  \n\n\nhttp://orwh.od.nih.gov/resources/policyreports/womenofcolor.asp\n  \n\n\n\n\nNature.com Scientific data repositories list\n\n\n\n\nhttps://www.nature.com/sdata/policies/repositories",
            "title": "Data Repositories"
        },
        {
            "location": "/Data-collections/",
            "text": "This website is supposed to gather and explain curated lists of OpenML datasets.\n\n\nEfficient and Robust Automated Machine Learning - Feurer et al. - NIPS 2015\n\u00b6\n\n\nContact: \n@mfeurer\n\n\nUsed in: \nEfficient and Robust Automated Machine Learning\n\n\nDatasets:\n\n\n1000,1002,1018,1019,1020,1021,1036,1040,1041,1049,1050,1053,1056,1067,1068,1069,1111,1112,1114,1116,1119,1120,1128,1130,1134,1138,1139,1142,1146,1161,1166,12,14,16,179,180,181,182,184,185,18,21,22,23,24,26,273,28,293,300,30,31,32,351,354,357,36,389,38,390,391,392,393,395,396,398,399,3,401,44,46,554,57,60,679,6,715,718,720,722,723,727,728,734,735,737,740,741,743,751,752,761,772,797,799,803,806,807,813,816,819,821,822,823,833,837,843,845,846,847,849,866,871,881,897,901,903,904,910,912,913,914,917,923,930,934,953,958,959,962,966,971,976,977,978,979,980,991,993,995\n\n\n\n\n\n\n\n\n\n\n\ndid\n\n\nname\n\n\nn\n\n\np\n\n\np.num\n\n\np.syms\n\n\np.bin\n\n\nn.class\n\n\nminclass\n\n\nmaxclass\n\n\nn.miss\n\n\n\n\n\n\n\n\n\n\n29\n\n\n31\n\n\ncredit-g\n\n\n1000\n\n\n21\n\n\n7\n\n\n13\n\n\n2\n\n\n2\n\n\n300\n\n\n700\n\n\n0\n\n\n\n\n\n\n570\n\n\n715\n\n\nfri_c3_1000_25\n\n\n1000\n\n\n26\n\n\n25\n\n\n0\n\n\n0\n\n\n2\n\n\n443\n\n\n557\n\n\n0\n\n\n\n\n\n\n573\n\n\n718\n\n\nfri_c4_1000_100\n\n\n1000\n\n\n101\n\n\n100\n\n\n0\n\n\n0\n\n\n2\n\n\n436\n\n\n564\n\n\n0\n\n\n\n\n\n\n578\n\n\n723\n\n\nfri_c4_1000_25\n\n\n1000\n\n\n26\n\n\n25\n\n\n0\n\n\n0\n\n\n2\n\n\n453\n\n\n547\n\n\n0\n\n\n\n\n\n\n595\n\n\n740\n\n\nfri_c3_1000_10\n\n\n1000\n\n\n11\n\n\n10\n\n\n0\n\n\n0\n\n\n2\n\n\n440\n\n\n560\n\n\n0\n\n\n\n\n\n\n598\n\n\n743\n\n\nfri_c1_1000_5\n\n\n1000\n\n\n6\n\n\n5\n\n\n0\n\n\n0\n\n\n2\n\n\n457\n\n\n543\n\n\n0\n\n\n\n\n\n\n606\n\n\n751\n\n\nfri_c4_1000_10\n\n\n1000\n\n\n11\n\n\n10\n\n\n0\n\n\n0\n\n\n2\n\n\n440\n\n\n560\n\n\n0\n\n\n\n\n\n\n651\n\n\n797\n\n\nfri_c4_1000_50\n\n\n1000\n\n\n51\n\n\n50\n\n\n0\n\n\n0\n\n\n2\n\n\n440\n\n\n560\n\n\n0\n\n\n\n\n\n\n653\n\n\n799\n\n\nfri_c0_1000_5\n\n\n1000\n\n\n6\n\n\n5\n\n\n0\n\n\n0\n\n\n2\n\n\n497\n\n\n503\n\n\n0\n\n\n\n\n\n\n660\n\n\n806\n\n\nfri_c3_1000_50\n\n\n1000\n\n\n51\n\n\n50\n\n\n0\n\n\n0\n\n\n2\n\n\n445\n\n\n555\n\n\n0\n\n\n\n\n\n\n667\n\n\n813\n\n\nfri_c3_1000_5\n\n\n1000\n\n\n6\n\n\n5\n\n\n0\n\n\n0\n\n\n2\n\n\n437\n\n\n563\n\n\n0\n\n\n\n\n\n\n691\n\n\n837\n\n\nfri_c1_1000_50\n\n\n1000\n\n\n51\n\n\n50\n\n\n0\n\n\n0\n\n\n2\n\n\n453\n\n\n547\n\n\n0\n\n\n\n\n\n\n699\n\n\n845\n\n\nfri_c0_1000_10\n\n\n1000\n\n\n11\n\n\n10\n\n\n0\n\n\n0\n\n\n2\n\n\n491\n\n\n509\n\n\n0\n\n\n\n\n\n\n703\n\n\n849\n\n\nfri_c0_1000_25\n\n\n1000\n\n\n26\n\n\n25\n\n\n0\n\n\n0\n\n\n2\n\n\n497\n\n\n503\n\n\n0\n\n\n\n\n\n\n719\n\n\n866\n\n\nfri_c2_1000_50\n\n\n1000\n\n\n51\n\n\n50\n\n\n0\n\n\n0\n\n\n2\n\n\n418\n\n\n582\n\n\n0\n\n\n\n\n\n\n755\n\n\n903\n\n\nfri_c2_1000_25\n\n\n1000\n\n\n26\n\n\n25\n\n\n0\n\n\n0\n\n\n2\n\n\n437\n\n\n563\n\n\n0\n\n\n\n\n\n\n756\n\n\n904\n\n\nfri_c0_1000_50\n\n\n1000\n\n\n51\n\n\n50\n\n\n0\n\n\n0\n\n\n2\n\n\n490\n\n\n510\n\n\n0\n\n\n\n\n\n\n762\n\n\n910\n\n\nfri_c1_1000_10\n\n\n1000\n\n\n11\n\n\n10\n\n\n0\n\n\n0\n\n\n2\n\n\n436\n\n\n564\n\n\n0\n\n\n\n\n\n\n764\n\n\n912\n\n\nfri_c2_1000_5\n\n\n1000\n\n\n6\n\n\n5\n\n\n0\n\n\n0\n\n\n2\n\n\n416\n\n\n584\n\n\n0\n\n\n\n\n\n\n765\n\n\n913\n\n\nfri_c2_1000_10\n\n\n1000\n\n\n11\n\n\n10\n\n\n0\n\n\n0\n\n\n2\n\n\n420\n\n\n580\n\n\n0\n\n\n\n\n\n\n769\n\n\n917\n\n\nfri_c1_1000_25\n\n\n1000\n\n\n26\n\n\n25\n\n\n0\n\n\n0\n\n\n2\n\n\n454\n\n\n546\n\n\n0\n\n\n\n\n\n\n262\n\n\n392\n\n\noh0.wc\n\n\n1003\n\n\n3183\n\n\n3182\n\n\n0\n\n\n0\n\n\n10\n\n\n51\n\n\n194\n\n\n0\n\n\n\n\n\n\n535\n\n\n679\n\n\nrmftsa_sleepdata\n\n\n1024\n\n\n3\n\n\n2\n\n\n1\n\n\n0\n\n\n4\n\n\n94\n\n\n404\n\n\n0\n\n\n\n\n\n\n596\n\n\n741\n\n\nrmftsa_sleepdata\n\n\n1024\n\n\n3\n\n\n1\n\n\n1\n\n\n0\n\n\n2\n\n\n509\n\n\n515\n\n\n0\n\n\n\n\n\n\n271\n\n\n401\n\n\noh10.wc\n\n\n1050\n\n\n3239\n\n\n3238\n\n\n0\n\n\n0\n\n\n10\n\n\n52\n\n\n165\n\n\n0\n\n\n\n\n\n\n914\n\n\n1068\n\n\npc1\n\n\n1109\n\n\n22\n\n\n21\n\n\n0\n\n\n0\n\n\n2\n\n\n77\n\n\n1032\n\n\n0\n\n\n\n\n\n\n786\n\n\n934\n\n\nsocmob\n\n\n1156\n\n\n6\n\n\n1\n\n\n4\n\n\n2\n\n\n2\n\n\n256\n\n\n900\n\n\n0\n\n\n\n\n\n\n749\n\n\n897\n\n\ncolleges_aaup\n\n\n1161\n\n\n17\n\n\n13\n\n\n3\n\n\n0\n\n\n2\n\n\n348\n\n\n813\n\n\n87\n\n\n\n\n\n\n782\n\n\n930\n\n\ncolleges_usnews\n\n\n1302\n\n\n35\n\n\n32\n\n\n2\n\n\n0\n\n\n2\n\n\n614\n\n\n688\n\n\n1144\n\n\n\n\n\n\n123\n\n\n185\n\n\nbaseball\n\n\n1340\n\n\n18\n\n\n15\n\n\n2\n\n\n0\n\n\n3\n\n\n57\n\n\n1215\n\n\n20\n\n\n\n\n\n\n818\n\n\n966\n\n\nanalcatdata_halloffame\n\n\n1340\n\n\n18\n\n\n15\n\n\n2\n\n\n0\n\n\n2\n\n\n125\n\n\n1215\n\n\n20\n\n\n\n\n\n\n896\n\n\n1049\n\n\npc4\n\n\n1458\n\n\n38\n\n\n37\n\n\n0\n\n\n0\n\n\n2\n\n\n178\n\n\n1280\n\n\n0\n\n\n\n\n\n\n21\n\n\n23\n\n\ncmc\n\n\n1473\n\n\n10\n\n\n2\n\n\n7\n\n\n3\n\n\n3\n\n\n333\n\n\n629\n\n\n0\n\n\n\n\n\n\n119\n\n\n181\n\n\nyeast\n\n\n1484\n\n\n9\n\n\n8\n\n\n0\n\n\n0\n\n\n10\n\n\n5\n\n\n463\n\n\n0\n\n\n\n\n\n\n261\n\n\n391\n\n\nre0.wc\n\n\n1504\n\n\n2887\n\n\n2886\n\n\n0\n\n\n0\n\n\n13\n\n\n11\n\n\n608\n\n\n0\n\n\n\n\n\n\n972\n\n\n1128\n\n\nOVA_Breast\n\n\n1545\n\n\n10937\n\n\n10936\n\n\n1\n\n\n0\n\n\n2\n\n\n344\n\n\n1201\n\n\n0\n\n\n\n\n\n\n974\n\n\n1130\n\n\nOVA_Lung\n\n\n1545\n\n\n10937\n\n\n10936\n\n\n0\n\n\n0\n\n\n2\n\n\n126\n\n\n1419\n\n\n0\n\n\n\n\n\n\n978\n\n\n1134\n\n\nOVA_Kidney\n\n\n1545\n\n\n10937\n\n\n10936\n\n\n0\n\n\n0\n\n\n2\n\n\n260\n\n\n1285\n\n\n0\n\n\n\n\n\n\n982\n\n\n1138\n\n\nOVA_Uterus\n\n\n1545\n\n\n10937\n\n\n10936\n\n\n1\n\n\n0\n\n\n2\n\n\n124\n\n\n1421\n\n\n0\n\n\n\n\n\n\n983\n\n\n1139\n\n\nOVA_Omentum\n\n\n1545\n\n\n10937\n\n\n10936\n\n\n0\n\n\n0\n\n\n2\n\n\n77\n\n\n1468\n\n\n0\n\n\n\n\n\n\n986\n\n\n1142\n\n\nOVA_Endometrium\n\n\n1545\n\n\n10937\n\n\n10936\n\n\n0\n\n\n0\n\n\n2\n\n\n61\n\n\n1484\n\n\n0\n\n\n\n\n\n\n990\n\n\n1146\n\n\nOVA_Prostate\n\n\n1545\n\n\n10937\n\n\n10936\n\n\n0\n\n\n0\n\n\n2\n\n\n69\n\n\n1476\n\n\n0\n\n\n\n\n\n\n1005\n\n\n1161\n\n\nOVA_Colon\n\n\n1545\n\n\n10937\n\n\n10936\n\n\n0\n\n\n0\n\n\n2\n\n\n286\n\n\n1259\n\n\n0\n\n\n\n\n\n\n1010\n\n\n1166\n\n\nOVA_Ovary\n\n\n1545\n\n\n10937\n\n\n10936\n\n\n0\n\n\n0\n\n\n2\n\n\n198\n\n\n1347\n\n\n0\n\n\n\n\n\n\n268\n\n\n398\n\n\nwap.wc\n\n\n1560\n\n\n8461\n\n\n8460\n\n\n0\n\n\n0\n\n\n20\n\n\n5\n\n\n341\n\n\n0\n\n\n\n\n\n\n897\n\n\n1050\n\n\npc3\n\n\n1563\n\n\n38\n\n\n37\n\n\n0\n\n\n0\n\n\n2\n\n\n160\n\n\n1403\n\n\n0\n\n\n\n\n\n\n265\n\n\n395\n\n\nre1.wc\n\n\n1657\n\n\n3759\n\n\n3758\n\n\n1\n\n\n0\n\n\n25\n\n\n10\n\n\n371\n\n\n0\n\n\n\n\n\n\n19\n\n\n21\n\n\ncar\n\n\n1728\n\n\n7\n\n\n0\n\n\n6\n\n\n0\n\n\n4\n\n\n65\n\n\n1210\n\n\n0\n\n\n\n\n\n\n843\n\n\n991\n\n\ncar\n\n\n1728\n\n\n7\n\n\n0\n\n\n6\n\n\n0\n\n\n2\n\n\n518\n\n\n1210\n\n\n0\n\n\n\n\n\n\n12\n\n\n12\n\n\nmfeat-factors\n\n\n2000\n\n\n217\n\n\n216\n\n\n0\n\n\n0\n\n\n10\n\n\n200\n\n\n200\n\n\n0\n\n\n\n\n\n\n14\n\n\n14\n\n\nmfeat-fourier\n\n\n2000\n\n\n77\n\n\n76\n\n\n0\n\n\n0\n\n\n10\n\n\n200\n\n\n200\n\n\n0\n\n\n\n\n\n\n16\n\n\n16\n\n\nmfeat-karhunen\n\n\n2000\n\n\n65\n\n\n64\n\n\n0\n\n\n0\n\n\n10\n\n\n200\n\n\n200\n\n\n0\n\n\n\n\n\n\n17\n\n\n18\n\n\nmfeat-morphological\n\n\n2000\n\n\n7\n\n\n6\n\n\n0\n\n\n0\n\n\n10\n\n\n200\n\n\n200\n\n\n0\n\n\n\n\n\n\n20\n\n\n22\n\n\nmfeat-zernike\n\n\n2000\n\n\n48\n\n\n47\n\n\n0\n\n\n0\n\n\n10\n\n\n200\n\n\n200\n\n\n0\n\n\n\n\n\n\n814\n\n\n962\n\n\nmfeat-morphological\n\n\n2000\n\n\n7\n\n\n6\n\n\n0\n\n\n0\n\n\n2\n\n\n200\n\n\n1800\n\n\n0\n\n\n\n\n\n\n823\n\n\n971\n\n\nmfeat-fourier\n\n\n2000\n\n\n77\n\n\n76\n\n\n0\n\n\n0\n\n\n2\n\n\n200\n\n\n1800\n\n\n0\n\n\n\n\n\n\n830\n\n\n978\n\n\nmfeat-factors\n\n\n2000\n\n\n217\n\n\n216\n\n\n0\n\n\n0\n\n\n2\n\n\n200\n\n\n1800\n\n\n0\n\n\n\n\n\n\n847\n\n\n995\n\n\nmfeat-zernike\n\n\n2000\n\n\n48\n\n\n47\n\n\n0\n\n\n0\n\n\n2\n\n\n200\n\n\n1800\n\n\n0\n\n\n\n\n\n\n872\n\n\n1020\n\n\nmfeat-karhunen\n\n\n2000\n\n\n65\n\n\n64\n\n\n0\n\n\n0\n\n\n2\n\n\n200\n\n\n1800\n\n\n0\n\n\n\n\n\n\n766\n\n\n914\n\n\nballoon\n\n\n2001\n\n\n3\n\n\n2\n\n\n0\n\n\n0\n\n\n2\n\n\n482\n\n\n1519\n\n\n0\n\n\n\n\n\n\n913\n\n\n1067\n\n\nkc1\n\n\n2109\n\n\n22\n\n\n21\n\n\n0\n\n\n0\n\n\n2\n\n\n326\n\n\n1783\n\n\n0\n\n\n\n\n\n\n627\n\n\n772\n\n\nquake\n\n\n2178\n\n\n4\n\n\n3\n\n\n0\n\n\n0\n\n\n2\n\n\n969\n\n\n1209\n\n\n0\n\n\n\n\n\n\n33\n\n\n36\n\n\nsegment\n\n\n2310\n\n\n20\n\n\n19\n\n\n0\n\n\n0\n\n\n7\n\n\n330\n\n\n330\n\n\n0\n\n\n\n\n\n\n810\n\n\n958\n\n\nsegment\n\n\n2310\n\n\n20\n\n\n19\n\n\n0\n\n\n0\n\n\n2\n\n\n330\n\n\n1980\n\n\n0\n\n\n\n\n\n\n259\n\n\n389\n\n\nfbis.wc\n\n\n2463\n\n\n2001\n\n\n2000\n\n\n0\n\n\n0\n\n\n17\n\n\n38\n\n\n506\n\n\n0\n\n\n\n\n\n\n263\n\n\n393\n\n\nla2s.wc\n\n\n3075\n\n\n12433\n\n\n12432\n\n\n1\n\n\n0\n\n\n6\n\n\n248\n\n\n905\n\n\n0\n\n\n\n\n\n\n592\n\n\n737\n\n\nspace_ga\n\n\n3107\n\n\n7\n\n\n6\n\n\n0\n\n\n0\n\n\n2\n\n\n1541\n\n\n1566\n\n\n0\n\n\n\n\n\n\n42\n\n\n46\n\n\nsplice\n\n\n3190\n\n\n62\n\n\n0\n\n\n61\n\n\n0\n\n\n3\n\n\n767\n\n\n1655\n\n\n0\n\n\n\n\n\n\n805\n\n\n953\n\n\nsplice\n\n\n3190\n\n\n62\n\n\n0\n\n\n61\n\n\n0\n\n\n2\n\n\n1535\n\n\n1655\n\n\n0\n\n\n\n\n\n\n3\n\n\n3\n\n\nkr-vs-kp\n\n\n3196\n\n\n37\n\n\n0\n\n\n36\n\n\n34\n\n\n2\n\n\n1527\n\n\n1669\n\n\n0\n\n\n\n\n\n\n266\n\n\n396\n\n\nla1s.wc\n\n\n3204\n\n\n13196\n\n\n13195\n\n\n1\n\n\nNA\n\n\n6\n\n\n273\n\n\n943\n\n\n0\n\n\n\n\n\n\n888\n\n\n1041\n\n\ngina_prior2\n\n\n3468\n\n\n785\n\n\n784\n\n\n0\n\n\n0\n\n\n10\n\n\n315\n\n\n383\n\n\n0\n\n\n\n\n\n\n35\n\n\n38\n\n\nsick\n\n\n3772\n\n\n30\n\n\n7\n\n\n22\n\n\n20\n\n\n2\n\n\n231\n\n\n3541\n\n\n3772\n\n\n\n\n\n\n52\n\n\n57\n\n\nhypothyroid\n\n\n3772\n\n\n30\n\n\n7\n\n\n22\n\n\n20\n\n\n4\n\n\n2\n\n\n3481\n\n\n3772\n\n\n\n\n\n\n852\n\n\n1000\n\n\nhypothyroid\n\n\n3772\n\n\n30\n\n\n7\n\n\n22\n\n\n20\n\n\n2\n\n\n291\n\n\n3481\n\n\n3772\n\n\n\n\n\n\n724\n\n\n871\n\n\npollen\n\n\n3848\n\n\n6\n\n\n5\n\n\n0\n\n\n0\n\n\n2\n\n\n1924\n\n\n1924\n\n\n0\n\n\n\n\n\n\n583\n\n\n728\n\n\nanalcatdata_supreme\n\n\n4052\n\n\n8\n\n\n7\n\n\n0\n\n\n0\n\n\n2\n\n\n971\n\n\n3081\n\n\n0\n\n\n\n\n\n\n575\n\n\n720\n\n\nabalone\n\n\n4177\n\n\n9\n\n\n7\n\n\n1\n\n\n0\n\n\n2\n\n\n2081\n\n\n2096\n\n\n0\n\n\n\n\n\n\n41\n\n\n44\n\n\nspambase\n\n\n4601\n\n\n58\n\n\n57\n\n\n0\n\n\n0\n\n\n2\n\n\n1813\n\n\n2788\n\n\n0\n\n\n\n\n\n\n54\n\n\n60\n\n\nwaveform-5000\n\n\n5000\n\n\n41\n\n\n40\n\n\n0\n\n\n0\n\n\n3\n\n\n1653\n\n\n1692\n\n\n0\n\n\n\n\n\n\n831\n\n\n979\n\n\nwaveform-5000\n\n\n5000\n\n\n41\n\n\n40\n\n\n0\n\n\n0\n\n\n2\n\n\n1692\n\n\n3308\n\n\n0\n\n\n\n\n\n\n28\n\n\n30\n\n\npage-blocks\n\n\n5473\n\n\n11\n\n\n10\n\n\n0\n\n\n0\n\n\n5\n\n\n28\n\n\n4913\n\n\n0\n\n\n\n\n\n\n873\n\n\n1021\n\n\npage-blocks\n\n\n5473\n\n\n11\n\n\n10\n\n\n0\n\n\n0\n\n\n2\n\n\n560\n\n\n4913\n\n\n0\n\n\n\n\n\n\n915\n\n\n1069\n\n\npc2\n\n\n5589\n\n\n37\n\n\n36\n\n\n0\n\n\n0\n\n\n2\n\n\n23\n\n\n5566\n\n\n0\n\n\n\n\n\n\n26\n\n\n28\n\n\noptdigits\n\n\n5620\n\n\n65\n\n\n64\n\n\n0\n\n\n0\n\n\n10\n\n\n554\n\n\n572\n\n\n0\n\n\n\n\n\n\n832\n\n\n980\n\n\noptdigits\n\n\n5620\n\n\n65\n\n\n64\n\n\n0\n\n\n0\n\n\n2\n\n\n572\n\n\n5048\n\n\n0\n\n\n\n\n\n\n120\n\n\n182\n\n\nsatimage\n\n\n6430\n\n\n37\n\n\n36\n\n\n0\n\n\n0\n\n\n6\n\n\n625\n\n\n1531\n\n\n0\n\n\n\n\n\n\n701\n\n\n847\n\n\nwind\n\n\n6574\n\n\n15\n\n\n14\n\n\n0\n\n\n0\n\n\n2\n\n\n3073\n\n\n3501\n\n\n0\n\n\n\n\n\n\n960\n\n\n1116\n\n\nmusk\n\n\n6598\n\n\n170\n\n\n167\n\n\n2\n\n\n0\n\n\n2\n\n\n1017\n\n\n5581\n\n\n0\n\n\n\n\n\n\n845\n\n\n993\n\n\nkdd_ipums_la_97-small\n\n\n7019\n\n\n61\n\n\n33\n\n\n27\n\n\n8\n\n\n2\n\n\n2594\n\n\n4425\n\n\n7019\n\n\n\n\n\n\n657\n\n\n803\n\n\ndelta_ailerons\n\n\n7129\n\n\n6\n\n\n5\n\n\n0\n\n\n0\n\n\n2\n\n\n3346\n\n\n3783\n\n\n0\n\n\n\n\n\n\n854\n\n\n1002\n\n\nkdd_ipums_la_98-small\n\n\n7485\n\n\n56\n\n\n16\n\n\n39\n\n\n8\n\n\n2\n\n\n791\n\n\n6694\n\n\n7369\n\n\n\n\n\n\n211\n\n\n300\n\n\nisolet\n\n\n7797\n\n\n618\n\n\n617\n\n\n0\n\n\n0\n\n\n26\n\n\n298\n\n\n300\n\n\n0\n\n\n\n\n\n\n22\n\n\n24\n\n\nmushroom\n\n\n8124\n\n\n23\n\n\n0\n\n\n22\n\n\n4\n\n\n2\n\n\n3916\n\n\n4208\n\n\n2480\n\n\n\n\n\n\n590\n\n\n735\n\n\ncpu_small\n\n\n8192\n\n\n13\n\n\n12\n\n\n0\n\n\n0\n\n\n2\n\n\n2477\n\n\n5715\n\n\n0\n\n\n\n\n\n\n607\n\n\n752\n\n\npuma32H\n\n\n8192\n\n\n33\n\n\n32\n\n\n0\n\n\n0\n\n\n2\n\n\n4064\n\n\n4128\n\n\n0\n\n\n\n\n\n\n616\n\n\n761\n\n\ncpu_act\n\n\n8192\n\n\n22\n\n\n21\n\n\n0\n\n\n0\n\n\n2\n\n\n2477\n\n\n5715\n\n\n0\n\n\n\n\n\n\n661\n\n\n807\n\n\nkin8nm\n\n\n8192\n\n\n9\n\n\n8\n\n\n0\n\n\n0\n\n\n2\n\n\n4024\n\n\n4168\n\n\n0\n\n\n\n\n\n\n670\n\n\n816\n\n\npuma8NH\n\n\n8192\n\n\n9\n\n\n8\n\n\n0\n\n\n0\n\n\n2\n\n\n4078\n\n\n4114\n\n\n0\n\n\n\n\n\n\n687\n\n\n833\n\n\nbank32nh\n\n\n8192\n\n\n33\n\n\n32\n\n\n0\n\n\n0\n\n\n2\n\n\n2543\n\n\n5649\n\n\n0\n\n\n\n\n\n\n775\n\n\n923\n\n\nvisualizing_soil\n\n\n8641\n\n\n5\n\n\n3\n\n\n1\n\n\n1\n\n\n2\n\n\n3888\n\n\n4753\n\n\n0\n\n\n\n\n\n\n870\n\n\n1018\n\n\nkdd_ipums_la_99-small\n\n\n8844\n\n\n57\n\n\n15\n\n\n41\n\n\n9\n\n\n2\n\n\n568\n\n\n8276\n\n\n8844\n\n\n\n\n\n\n902\n\n\n1056\n\n\nmc1\n\n\n9466\n\n\n39\n\n\n38\n\n\n0\n\n\n0\n\n\n2\n\n\n68\n\n\n9398\n\n\n0\n\n\n\n\n\n\n673\n\n\n819\n\n\ndelta_elevators\n\n\n9517\n\n\n7\n\n\n6\n\n\n0\n\n\n0\n\n\n2\n\n\n4732\n\n\n4785\n\n\n0\n\n\n\n\n\n\n260\n\n\n390\n\n\nnew3s.wc\n\n\n9558\n\n\n26833\n\n\n26832\n\n\n1\n\n\n0\n\n\n44\n\n\n104\n\n\n696\n\n\n0\n\n\n\n\n\n\n828\n\n\n976\n\n\nkdd_JapaneseVowels\n\n\n9961\n\n\n15\n\n\n14\n\n\n0\n\n\n0\n\n\n2\n\n\n1614\n\n\n8347\n\n\n0\n\n\n\n\n\n\n899\n\n\n1053\n\n\njm1\n\n\n10885\n\n\n22\n\n\n21\n\n\n0\n\n\n0\n\n\n2\n\n\n2106\n\n\n8779\n\n\n5\n\n\n\n\n\n\n30\n\n\n32\n\n\npendigits\n\n\n10992\n\n\n17\n\n\n16\n\n\n0\n\n\n0\n\n\n10\n\n\n1055\n\n\n1144\n\n\n0\n\n\n\n\n\n\n871\n\n\n1019\n\n\npendigits\n\n\n10992\n\n\n17\n\n\n16\n\n\n0\n\n\n0\n\n\n2\n\n\n1144\n\n\n9848\n\n\n0\n\n\n\n\n\n\n269\n\n\n399\n\n\nohscal.wc\n\n\n11162\n\n\n11466\n\n\n11465\n\n\n0\n\n\n0\n\n\n10\n\n\n709\n\n\n1621\n\n\n0\n\n\n\n\n\n\n24\n\n\n26\n\n\nnursery\n\n\n12960\n\n\n9\n\n\n0\n\n\n8\n\n\n1\n\n\n5\n\n\n2\n\n\n4320\n\n\n0\n\n\n\n\n\n\n811\n\n\n959\n\n\nnursery\n\n\n12960\n\n\n9\n\n\n0\n\n\n8\n\n\n1\n\n\n2\n\n\n4320\n\n\n8640\n\n\n0\n\n\n\n\n\n\n589\n\n\n734\n\n\nailerons\n\n\n13750\n\n\n41\n\n\n40\n\n\n0\n\n\n0\n\n\n2\n\n\n5828\n\n\n7922\n\n\n0\n\n\n\n\n\n\n883\n\n\n1036\n\n\nsylva_agnostic\n\n\n14395\n\n\n217\n\n\n216\n\n\n0\n\n\n0\n\n\n2\n\n\n886\n\n\n13509\n\n\n0\n\n\n\n\n\n\n887\n\n\n1040\n\n\nsylva_prior\n\n\n14395\n\n\n109\n\n\n108\n\n\n0\n\n\n0\n\n\n2\n\n\n886\n\n\n13509\n\n\n0\n\n\n\n\n\n\n577\n\n\n722\n\n\npol\n\n\n15000\n\n\n49\n\n\n48\n\n\n0\n\n\n0\n\n\n2\n\n\n5041\n\n\n9959\n\n\n0\n\n\n\n\n\n\n700\n\n\n846\n\n\nelevators\n\n\n16599\n\n\n19\n\n\n18\n\n\n0\n\n\n0\n\n\n2\n\n\n5130\n\n\n11469\n\n\n0\n\n\n\n\n\n\n964\n\n\n1120\n\n\nMagicTelescope\n\n\n19020\n\n\n12\n\n\n11\n\n\n0\n\n\n0\n\n\n2\n\n\n6688\n\n\n12332\n\n\n0\n\n\n\n\n\n\n6\n\n\n6\n\n\nletter\n\n\n20000\n\n\n17\n\n\n16\n\n\n0\n\n\n0\n\n\n26\n\n\n734\n\n\n813\n\n\n0\n\n\n\n\n\n\n829\n\n\n977\n\n\nletter\n\n\n20000\n\n\n17\n\n\n16\n\n\n0\n\n\n0\n\n\n2\n\n\n813\n\n\n19187\n\n\n0\n\n\n\n\n\n\n676\n\n\n822\n\n\ncal_housing\n\n\n20640\n\n\n9\n\n\n8\n\n\n0\n\n\n0\n\n\n2\n\n\n8385\n\n\n12255\n\n\n0\n\n\n\n\n\n\n677\n\n\n823\n\n\nhouses\n\n\n20640\n\n\n9\n\n\n8\n\n\n0\n\n\n0\n\n\n2\n\n\n8914\n\n\n11726\n\n\n0\n\n\n\n\n\n\n675\n\n\n821\n\n\nhouse_16H\n\n\n22784\n\n\n17\n\n\n16\n\n\n0\n\n\n0\n\n\n2\n\n\n6744\n\n\n16040\n\n\n0\n\n\n\n\n\n\n697\n\n\n843\n\n\nhouse_8L\n\n\n22784\n\n\n9\n\n\n8\n\n\n0\n\n\n0\n\n\n2\n\n\n6744\n\n\n16040\n\n\n0\n\n\n\n\n\n\n122\n\n\n184\n\n\nkropt\n\n\n28056\n\n\n7\n\n\n0\n\n\n6\n\n\n0\n\n\n18\n\n\n27\n\n\n4553\n\n\n0\n\n\n\n\n\n\n963\n\n\n1119\n\n\nadult-census\n\n\n32561\n\n\n16\n\n\n7\n\n\n8\n\n\n1\n\n\n2\n\n\n7841\n\n\n24720\n\n\n2399\n\n\n\n\n\n\n582\n\n\n727\n\n\n2dplanes\n\n\n40768\n\n\n11\n\n\n10\n\n\n0\n\n\n0\n\n\n2\n\n\n20348\n\n\n20420\n\n\n0\n\n\n\n\n\n\n734\n\n\n881\n\n\nmv\n\n\n40768\n\n\n11\n\n\n7\n\n\n3\n\n\n2\n\n\n2\n\n\n16447\n\n\n24321\n\n\n0\n\n\n\n\n\n\n753\n\n\n901\n\n\nfried\n\n\n40768\n\n\n11\n\n\n10\n\n\n0\n\n\n0\n\n\n2\n\n\n20341\n\n\n20427\n\n\n0\n\n\n\n\n\n\n117\n\n\n179\n\n\nadult\n\n\n48842\n\n\n15\n\n\n2\n\n\n12\n\n\n1\n\n\n2\n\n\n11687\n\n\n37155\n\n\n3620\n\n\n\n\n\n\n955\n\n\n1111\n\n\nKDDCup09_appetency\n\n\n50000\n\n\n231\n\n\n192\n\n\n38\n\n\n4\n\n\n2\n\n\n890\n\n\n49110\n\n\n50000\n\n\n\n\n\n\n956\n\n\n1112\n\n\nKDDCup09_churn\n\n\n50000\n\n\n231\n\n\n192\n\n\n39\n\n\nNA\n\n\n2\n\n\n3672\n\n\n46328\n\n\n50000\n\n\n\n\n\n\n958\n\n\n1114\n\n\nKDDCup09_upselling\n\n\n50000\n\n\n231\n\n\n192\n\n\n38\n\n\n4\n\n\n2\n\n\n3682\n\n\n46318\n\n\n50000\n\n\n\n\n\n\n414\n\n\n554\n\n\nmnist_784\n\n\n70000\n\n\n785\n\n\n784\n\n\n0\n\n\n0\n\n\n10\n\n\n6313\n\n\n7877\n\n\n0\n\n\n\n\n\n\n241\n\n\n357\n\n\nvehicle_sensIT\n\n\n98528\n\n\n101\n\n\n100\n\n\n1\n\n\n1\n\n\n2\n\n\n49264\n\n\n49264\n\n\n0\n\n\n\n\n\n\n118\n\n\n180\n\n\ncovertype\n\n\n110393\n\n\n55\n\n\n14\n\n\n40\n\n\n40\n\n\n7\n\n\n1339\n\n\n51682\n\n\n0\n\n\n\n\n\n\n196\n\n\n273\n\n\nIMDB.drama\n\n\n120919\n\n\n1002\n\n\n1001\n\n\n1\n\n\n1\n\n\n2\n\n\n43779\n\n\n77140\n\n\n0\n\n\n\n\n\n\n239\n\n\n351\n\n\ncodrna\n\n\n488565\n\n\n9\n\n\n8\n\n\n1\n\n\n1\n\n\n2\n\n\n162855\n\n\n325710\n\n\n0\n\n\n\n\n\n\n206\n\n\n293\n\n\ncovertype\n\n\n581012\n\n\n55\n\n\n54\n\n\n1\n\n\n1\n\n\n2\n\n\n283301\n\n\n297711\n\n\n0\n\n\n\n\n\n\n240\n\n\n354\n\n\npoker\n\n\n1025010\n\n\n11\n\n\n10\n\n\n1\n\n\n1\n\n\n2\n\n\n511308\n\n\n513702\n\n\n0",
            "title": "Data collections"
        },
        {
            "location": "/Data-collections/#efficient-and-robust-automated-machine-learning-feurer-et-al-nips-2015",
            "text": "Contact:  @mfeurer  Used in:  Efficient and Robust Automated Machine Learning  Datasets:  1000,1002,1018,1019,1020,1021,1036,1040,1041,1049,1050,1053,1056,1067,1068,1069,1111,1112,1114,1116,1119,1120,1128,1130,1134,1138,1139,1142,1146,1161,1166,12,14,16,179,180,181,182,184,185,18,21,22,23,24,26,273,28,293,300,30,31,32,351,354,357,36,389,38,390,391,392,393,395,396,398,399,3,401,44,46,554,57,60,679,6,715,718,720,722,723,727,728,734,735,737,740,741,743,751,752,761,772,797,799,803,806,807,813,816,819,821,822,823,833,837,843,845,846,847,849,866,871,881,897,901,903,904,910,912,913,914,917,923,930,934,953,958,959,962,966,971,976,977,978,979,980,991,993,995      did  name  n  p  p.num  p.syms  p.bin  n.class  minclass  maxclass  n.miss      29  31  credit-g  1000  21  7  13  2  2  300  700  0    570  715  fri_c3_1000_25  1000  26  25  0  0  2  443  557  0    573  718  fri_c4_1000_100  1000  101  100  0  0  2  436  564  0    578  723  fri_c4_1000_25  1000  26  25  0  0  2  453  547  0    595  740  fri_c3_1000_10  1000  11  10  0  0  2  440  560  0    598  743  fri_c1_1000_5  1000  6  5  0  0  2  457  543  0    606  751  fri_c4_1000_10  1000  11  10  0  0  2  440  560  0    651  797  fri_c4_1000_50  1000  51  50  0  0  2  440  560  0    653  799  fri_c0_1000_5  1000  6  5  0  0  2  497  503  0    660  806  fri_c3_1000_50  1000  51  50  0  0  2  445  555  0    667  813  fri_c3_1000_5  1000  6  5  0  0  2  437  563  0    691  837  fri_c1_1000_50  1000  51  50  0  0  2  453  547  0    699  845  fri_c0_1000_10  1000  11  10  0  0  2  491  509  0    703  849  fri_c0_1000_25  1000  26  25  0  0  2  497  503  0    719  866  fri_c2_1000_50  1000  51  50  0  0  2  418  582  0    755  903  fri_c2_1000_25  1000  26  25  0  0  2  437  563  0    756  904  fri_c0_1000_50  1000  51  50  0  0  2  490  510  0    762  910  fri_c1_1000_10  1000  11  10  0  0  2  436  564  0    764  912  fri_c2_1000_5  1000  6  5  0  0  2  416  584  0    765  913  fri_c2_1000_10  1000  11  10  0  0  2  420  580  0    769  917  fri_c1_1000_25  1000  26  25  0  0  2  454  546  0    262  392  oh0.wc  1003  3183  3182  0  0  10  51  194  0    535  679  rmftsa_sleepdata  1024  3  2  1  0  4  94  404  0    596  741  rmftsa_sleepdata  1024  3  1  1  0  2  509  515  0    271  401  oh10.wc  1050  3239  3238  0  0  10  52  165  0    914  1068  pc1  1109  22  21  0  0  2  77  1032  0    786  934  socmob  1156  6  1  4  2  2  256  900  0    749  897  colleges_aaup  1161  17  13  3  0  2  348  813  87    782  930  colleges_usnews  1302  35  32  2  0  2  614  688  1144    123  185  baseball  1340  18  15  2  0  3  57  1215  20    818  966  analcatdata_halloffame  1340  18  15  2  0  2  125  1215  20    896  1049  pc4  1458  38  37  0  0  2  178  1280  0    21  23  cmc  1473  10  2  7  3  3  333  629  0    119  181  yeast  1484  9  8  0  0  10  5  463  0    261  391  re0.wc  1504  2887  2886  0  0  13  11  608  0    972  1128  OVA_Breast  1545  10937  10936  1  0  2  344  1201  0    974  1130  OVA_Lung  1545  10937  10936  0  0  2  126  1419  0    978  1134  OVA_Kidney  1545  10937  10936  0  0  2  260  1285  0    982  1138  OVA_Uterus  1545  10937  10936  1  0  2  124  1421  0    983  1139  OVA_Omentum  1545  10937  10936  0  0  2  77  1468  0    986  1142  OVA_Endometrium  1545  10937  10936  0  0  2  61  1484  0    990  1146  OVA_Prostate  1545  10937  10936  0  0  2  69  1476  0    1005  1161  OVA_Colon  1545  10937  10936  0  0  2  286  1259  0    1010  1166  OVA_Ovary  1545  10937  10936  0  0  2  198  1347  0    268  398  wap.wc  1560  8461  8460  0  0  20  5  341  0    897  1050  pc3  1563  38  37  0  0  2  160  1403  0    265  395  re1.wc  1657  3759  3758  1  0  25  10  371  0    19  21  car  1728  7  0  6  0  4  65  1210  0    843  991  car  1728  7  0  6  0  2  518  1210  0    12  12  mfeat-factors  2000  217  216  0  0  10  200  200  0    14  14  mfeat-fourier  2000  77  76  0  0  10  200  200  0    16  16  mfeat-karhunen  2000  65  64  0  0  10  200  200  0    17  18  mfeat-morphological  2000  7  6  0  0  10  200  200  0    20  22  mfeat-zernike  2000  48  47  0  0  10  200  200  0    814  962  mfeat-morphological  2000  7  6  0  0  2  200  1800  0    823  971  mfeat-fourier  2000  77  76  0  0  2  200  1800  0    830  978  mfeat-factors  2000  217  216  0  0  2  200  1800  0    847  995  mfeat-zernike  2000  48  47  0  0  2  200  1800  0    872  1020  mfeat-karhunen  2000  65  64  0  0  2  200  1800  0    766  914  balloon  2001  3  2  0  0  2  482  1519  0    913  1067  kc1  2109  22  21  0  0  2  326  1783  0    627  772  quake  2178  4  3  0  0  2  969  1209  0    33  36  segment  2310  20  19  0  0  7  330  330  0    810  958  segment  2310  20  19  0  0  2  330  1980  0    259  389  fbis.wc  2463  2001  2000  0  0  17  38  506  0    263  393  la2s.wc  3075  12433  12432  1  0  6  248  905  0    592  737  space_ga  3107  7  6  0  0  2  1541  1566  0    42  46  splice  3190  62  0  61  0  3  767  1655  0    805  953  splice  3190  62  0  61  0  2  1535  1655  0    3  3  kr-vs-kp  3196  37  0  36  34  2  1527  1669  0    266  396  la1s.wc  3204  13196  13195  1  NA  6  273  943  0    888  1041  gina_prior2  3468  785  784  0  0  10  315  383  0    35  38  sick  3772  30  7  22  20  2  231  3541  3772    52  57  hypothyroid  3772  30  7  22  20  4  2  3481  3772    852  1000  hypothyroid  3772  30  7  22  20  2  291  3481  3772    724  871  pollen  3848  6  5  0  0  2  1924  1924  0    583  728  analcatdata_supreme  4052  8  7  0  0  2  971  3081  0    575  720  abalone  4177  9  7  1  0  2  2081  2096  0    41  44  spambase  4601  58  57  0  0  2  1813  2788  0    54  60  waveform-5000  5000  41  40  0  0  3  1653  1692  0    831  979  waveform-5000  5000  41  40  0  0  2  1692  3308  0    28  30  page-blocks  5473  11  10  0  0  5  28  4913  0    873  1021  page-blocks  5473  11  10  0  0  2  560  4913  0    915  1069  pc2  5589  37  36  0  0  2  23  5566  0    26  28  optdigits  5620  65  64  0  0  10  554  572  0    832  980  optdigits  5620  65  64  0  0  2  572  5048  0    120  182  satimage  6430  37  36  0  0  6  625  1531  0    701  847  wind  6574  15  14  0  0  2  3073  3501  0    960  1116  musk  6598  170  167  2  0  2  1017  5581  0    845  993  kdd_ipums_la_97-small  7019  61  33  27  8  2  2594  4425  7019    657  803  delta_ailerons  7129  6  5  0  0  2  3346  3783  0    854  1002  kdd_ipums_la_98-small  7485  56  16  39  8  2  791  6694  7369    211  300  isolet  7797  618  617  0  0  26  298  300  0    22  24  mushroom  8124  23  0  22  4  2  3916  4208  2480    590  735  cpu_small  8192  13  12  0  0  2  2477  5715  0    607  752  puma32H  8192  33  32  0  0  2  4064  4128  0    616  761  cpu_act  8192  22  21  0  0  2  2477  5715  0    661  807  kin8nm  8192  9  8  0  0  2  4024  4168  0    670  816  puma8NH  8192  9  8  0  0  2  4078  4114  0    687  833  bank32nh  8192  33  32  0  0  2  2543  5649  0    775  923  visualizing_soil  8641  5  3  1  1  2  3888  4753  0    870  1018  kdd_ipums_la_99-small  8844  57  15  41  9  2  568  8276  8844    902  1056  mc1  9466  39  38  0  0  2  68  9398  0    673  819  delta_elevators  9517  7  6  0  0  2  4732  4785  0    260  390  new3s.wc  9558  26833  26832  1  0  44  104  696  0    828  976  kdd_JapaneseVowels  9961  15  14  0  0  2  1614  8347  0    899  1053  jm1  10885  22  21  0  0  2  2106  8779  5    30  32  pendigits  10992  17  16  0  0  10  1055  1144  0    871  1019  pendigits  10992  17  16  0  0  2  1144  9848  0    269  399  ohscal.wc  11162  11466  11465  0  0  10  709  1621  0    24  26  nursery  12960  9  0  8  1  5  2  4320  0    811  959  nursery  12960  9  0  8  1  2  4320  8640  0    589  734  ailerons  13750  41  40  0  0  2  5828  7922  0    883  1036  sylva_agnostic  14395  217  216  0  0  2  886  13509  0    887  1040  sylva_prior  14395  109  108  0  0  2  886  13509  0    577  722  pol  15000  49  48  0  0  2  5041  9959  0    700  846  elevators  16599  19  18  0  0  2  5130  11469  0    964  1120  MagicTelescope  19020  12  11  0  0  2  6688  12332  0    6  6  letter  20000  17  16  0  0  26  734  813  0    829  977  letter  20000  17  16  0  0  2  813  19187  0    676  822  cal_housing  20640  9  8  0  0  2  8385  12255  0    677  823  houses  20640  9  8  0  0  2  8914  11726  0    675  821  house_16H  22784  17  16  0  0  2  6744  16040  0    697  843  house_8L  22784  9  8  0  0  2  6744  16040  0    122  184  kropt  28056  7  0  6  0  18  27  4553  0    963  1119  adult-census  32561  16  7  8  1  2  7841  24720  2399    582  727  2dplanes  40768  11  10  0  0  2  20348  20420  0    734  881  mv  40768  11  7  3  2  2  16447  24321  0    753  901  fried  40768  11  10  0  0  2  20341  20427  0    117  179  adult  48842  15  2  12  1  2  11687  37155  3620    955  1111  KDDCup09_appetency  50000  231  192  38  4  2  890  49110  50000    956  1112  KDDCup09_churn  50000  231  192  39  NA  2  3672  46328  50000    958  1114  KDDCup09_upselling  50000  231  192  38  4  2  3682  46318  50000    414  554  mnist_784  70000  785  784  0  0  10  6313  7877  0    241  357  vehicle_sensIT  98528  101  100  1  1  2  49264  49264  0    118  180  covertype  110393  55  14  40  40  7  1339  51682  0    196  273  IMDB.drama  120919  1002  1001  1  1  2  43779  77140  0    239  351  codrna  488565  9  8  1  1  2  162855  325710  0    206  293  covertype  581012  55  54  1  1  2  283301  297711  0    240  354  poker  1025010  11  10  1  1  2  511308  513702  0",
            "title": "Efficient and Robust Automated Machine Learning - Feurer et al. - NIPS 2015"
        },
        {
            "location": "/WebApp-(PHP)/",
            "text": "Backend\n\u00b6\n\n\nThe high-level architecture of the website, including the controllers for different parts of the website (REST API, html, ...) and connections to the database.\n\n\nCode\n\u00b6\n\n\nThe source code is available in the 'website' repository:\n\nhttps://github.com/openml/website\n\n\nImportant files and folders\n\u00b6\n\n\nIn this section we go through all important files and folder of the\nsystem.\n\n\nRoot directory\n\u00b6\n\n\nThe root directory of OpenML contains the following files and folders.\n\n\n\n\n\n\nsystem\n: This folder contains all files provided by\n    CodeIgniter 2.1.3. The contents of this folder is\n    beyond the scope of this document, and not relevant for extending\n    OpenML. All the files in this folder are in the same state as they\n    were provided by Ellislabs, and none of these files should ever be\n    changed.\n\n\n\n\n\n\nsparks\n: Sparks is a package management system for\n    Codeigniter that allows for instant installation of libraries into\n    the application. This folder contains two libraries provided by\n    third party software developers, oauth1 (based on version 1 the\n    oauth protocol) and oauth2 (similarly, based on version 2 of the\n    oauth protocol). The exact contents of this folder is beyond the\n    scope of this document and not relevant for extending OpenML.\n\n\n\n\n\n\nopenml_OS\n: All files in this folder are written specifically\n    for OpenML. When extending the functionality OpenML, usually one of\n    the files in this folder needs to be adjusted. As a thorough\n    understanding of the contents of this folder is vital for extending\n    OpenML, we will discuss the contents of this folder in\n    [[URL Mapping]] in more detail.\n\n\n\n\n\n\nindex.php\n: This is the \u201cbootstrap\u201d file of the system.\n    Basically, every page request on OpenML goes through this file (with\n    the css, images and javascript files as only exception). It then\n    determines which CodeIgniter and OpenML files need to be included.\n    This file should not be edited.\n\n\n\n\n\n\n.htaccess\n: This file (which configures the Apache Rewrite\n    Engine) makes sure that all URL requests will be directed to\n    \nindex.php\n. Without this file, we would need to include \nindex.php\n\n    explicitly in every URL request. This file makes sure that all other\n    URL requests without \nindex.php\n embedded in it automatically will\n    be transformed to \nindex.php\n. Eg.,\n    \nhttp://www.openml.org/frontend/page/home\n will be rewritten to\n    \nhttp://www.openml.org/index.php/frontend/page/home\n. This will be\n    explained in detail in [[URL Mapping]].\n\n\n\n\n\n\ncss\n: A folder containing all stylesheets. These are important\n    for the layout of OpenML.\n\n\n\n\n\n\ndata\n: A folder containing data files, e.g., datasets,\n    implementation files, uploaded content. Please note that this folder\n    does not necessarily needs to be present in the root directory. The\n    OpenML Base Config file determines the\n    exact location of this folder.\n\n\n\n\n\n\ndownloads\n: Another data folder, containing files like the most\n    recent database snapshot.\n\n\n\n\n\n\nimg\n: A folder containing all static images shown on the webpage.\n\n\n\n\n\n\njs\n: A folder containing all used Javascript files and libraries,\n    including third party libraries like jQuery and datatables.\n\n\n\n\n\n\nVarious other files, like .gitignore, favicon.ico, etc.\n\n\n\n\n\n\nopenml_OS\n\u00b6\n\n\nThis folder is (in CodeIgniter jargon) the \u201cApplication folder\u201d, and\ncontains all files relevant to OpenML. Within this folder, the following\nfolders should be present: (And also some other folders, but these are\nnot used by OpenML)\n\n\n\n\n\n\nconfig\n: A folder containing all config files. Most notably, it\n    contains the file \nBASE_CONFIG.php\n, in which all system\n    specific variables are set; the config items within this file\n    differs over various installations (e.g., on localhost,\n    \nopenml.org\n). Most other config files, like\n    \ndatabase.php\n, will receive their values from\n    \nBASE_CONFIG.php\n. Other important config files are\n    \nautoload.php\n, determining which CodeIgniter / OpenML\n    files will be loaded on any request, \nopenML.php\n,\n    containing config items specific to OpenML, and\n    \nroutes.php\n, which will be explained in\n    [[URL Mapping]].\n\n\n\n\n\n\ncontrollers\n: In the Model/View/Controller design pattern, all\n    user interaction goes through controllers. In a webapplication\n    setting this means that every time a URL gets requested, exactly one\n    controller gets invoked. The exact dynamics of this will be\n    explained in [[URL Mapping]].\n\n\n\n\n\n\ncore\n: A folder that contains CodeIgniter specific files. These\n    are not relevant for the understanding of OpenML.\n\n\n\n\n\n\nhelpers\n: This folder contains many convenience functions.\n    Wikipedia states: \u201cA convenience function is a non-essential\n    subroutine in a programming library or framework which is intended\n    to ease commonly performed tasks\u201d. For example the\n    \nfile_upload_helper.php\n contains many functions that\n    assist with uploading of files. Please note that a helper function\n    must be explicitly loaded in either the autoload config or the files\n    that uses its functions.\n\n\n\n\n\n\nlibraries\n: Similar to sparks, this folder contains libraries\n    specifically written for CodeIgniter. For example, the library used\n    for all user management routines is in this folder.\n\n\n\n\n\n\nmodels\n: In the Model/View/Controller design pattern, models\n    represent the state of the system. In a webapplication setting, you\n    could say that a model is the link to the database. In OpenML,\n    almost all tables of the database are represented by a model. Each\n    model has general functionality applicable to all models (e.g.,\n    retrieve all records, retrieve record with constraints, insert\n    record) and functionality specific to that model (e.g., retrieve a\n    dataset that has certain data properties). Most models extend an\n    (abstract) base class, located in the \nabstract\n folder.\n    This way, all general functionality is programmed and maintained in\n    one place.\n\n\n\n\n\n\nthird_party\n: Although the name might suggests differently, this\n    folder contains all OpenML Java libraries.\n\n\n\n\n\n\nviews\n: In the Model/View/Controller design pattern, the views\n    are the way information is presented on the screen. In a\n    webapplication setting, a view usually is a block of (PHP generated)\n    HTML code. The most notable view is \nfrontend_main.php\n,\n    which is the template file determining the main look and feel of\n    OpenML. Every single page also has its own specific view (which is\n    parsed within \nfrontend_main.php\n). These pages can be\n    found (categorized by controller and name) in the \npages\n\n    folder. More about this structure is explained in\n    [[URL Mapping]].\n\n\n\n\n\n\nFrontend\n\u00b6\n\n\nArchitecture and libraries involved in generating the frontend functions.\n\n\nCode\n\u00b6\n\n\nhttps://github.com/openml/website/tree/master/openml_OS/views\n\n\nHigh-level\n\u00b6\n\n\nAll pages are generated by first loading \nfrontend_main.php\n. This creates the 'shell' in which the content is loaded. It loads all css and javascript libraries, and contains the html for displaying headers and footers.\n\n\nCreate new page\n\u00b6\n\n\nThe preferred method is creating a new folder into the folder\n\n<root_directory>/openml_OS/views/pages/frontend\n\nThis page can be requested by\n\nhttp://www.openml.org/frontend/page/<folder_name>\n\nor just\n\nhttp://www.openml.org/<folder_name>\n\nThis method is preferred for human readable webpages, where the internal\nactions are simple, and the output is complex. We will describe the\nfiles that can be in this folder.\n\n\n\n\n\n\npre.php\n: Mandatory file. Will be executed first. Do not make\n    this file produce any output! Can be used to pre-render data, or set\n    some variables that are used in other files.\n\n\n\n\n\n\nbody.php\n: Highly recommended file. Intended for displaying the\n    main content of this file. Will be rendered at the right location\n    within the template file (\nfrontend_main.php\n).\n\n\n\n\n\n\njavascript.php\n: Non-mandatory file. Intended for javascript\n    function on which \nbody.php\n relies. Will be rendered within a\n    javascript block in the header of the page.\n\n\n\n\n\n\npost.php\n: Non mandatory file. Will only be executed when a POST\n    request is done (e.g., when a HTML form was send using the POST\n    protocol). Will be executed after \npre.php\n, but before the\n    rendering process (and thus, before \nbody.php\n and\n    \njavascript.php\n). Should handle the posted input, e.g., file\n    uploads.\n\n\n\n\n\n\nIt is also recommended to add the newly created folder to the mapping in\nthe \nroutes.php\n config file. This way it can also be requested by the\nshortened version of the URL. (Note that we deliberately avoided to\nauto-load all pages into this file using a directory scan, as this makes\nthe webplatform slow. )\n\n\nFor more information, see [[URL Mapping]].",
            "title": "WebApp"
        },
        {
            "location": "/WebApp-(PHP)/#backend",
            "text": "The high-level architecture of the website, including the controllers for different parts of the website (REST API, html, ...) and connections to the database.",
            "title": "Backend"
        },
        {
            "location": "/WebApp-(PHP)/#code",
            "text": "The source code is available in the 'website' repository: https://github.com/openml/website",
            "title": "Code"
        },
        {
            "location": "/WebApp-(PHP)/#important-files-and-folders",
            "text": "In this section we go through all important files and folder of the\nsystem.",
            "title": "Important files and folders"
        },
        {
            "location": "/WebApp-(PHP)/#root-directory",
            "text": "The root directory of OpenML contains the following files and folders.    system : This folder contains all files provided by\n    CodeIgniter 2.1.3. The contents of this folder is\n    beyond the scope of this document, and not relevant for extending\n    OpenML. All the files in this folder are in the same state as they\n    were provided by Ellislabs, and none of these files should ever be\n    changed.    sparks : Sparks is a package management system for\n    Codeigniter that allows for instant installation of libraries into\n    the application. This folder contains two libraries provided by\n    third party software developers, oauth1 (based on version 1 the\n    oauth protocol) and oauth2 (similarly, based on version 2 of the\n    oauth protocol). The exact contents of this folder is beyond the\n    scope of this document and not relevant for extending OpenML.    openml_OS : All files in this folder are written specifically\n    for OpenML. When extending the functionality OpenML, usually one of\n    the files in this folder needs to be adjusted. As a thorough\n    understanding of the contents of this folder is vital for extending\n    OpenML, we will discuss the contents of this folder in\n    [[URL Mapping]] in more detail.    index.php : This is the \u201cbootstrap\u201d file of the system.\n    Basically, every page request on OpenML goes through this file (with\n    the css, images and javascript files as only exception). It then\n    determines which CodeIgniter and OpenML files need to be included.\n    This file should not be edited.    .htaccess : This file (which configures the Apache Rewrite\n    Engine) makes sure that all URL requests will be directed to\n     index.php . Without this file, we would need to include  index.php \n    explicitly in every URL request. This file makes sure that all other\n    URL requests without  index.php  embedded in it automatically will\n    be transformed to  index.php . Eg.,\n     http://www.openml.org/frontend/page/home  will be rewritten to\n     http://www.openml.org/index.php/frontend/page/home . This will be\n    explained in detail in [[URL Mapping]].    css : A folder containing all stylesheets. These are important\n    for the layout of OpenML.    data : A folder containing data files, e.g., datasets,\n    implementation files, uploaded content. Please note that this folder\n    does not necessarily needs to be present in the root directory. The\n    OpenML Base Config file determines the\n    exact location of this folder.    downloads : Another data folder, containing files like the most\n    recent database snapshot.    img : A folder containing all static images shown on the webpage.    js : A folder containing all used Javascript files and libraries,\n    including third party libraries like jQuery and datatables.    Various other files, like .gitignore, favicon.ico, etc.",
            "title": "Root directory"
        },
        {
            "location": "/WebApp-(PHP)/#openml_os",
            "text": "This folder is (in CodeIgniter jargon) the \u201cApplication folder\u201d, and\ncontains all files relevant to OpenML. Within this folder, the following\nfolders should be present: (And also some other folders, but these are\nnot used by OpenML)    config : A folder containing all config files. Most notably, it\n    contains the file  BASE_CONFIG.php , in which all system\n    specific variables are set; the config items within this file\n    differs over various installations (e.g., on localhost,\n     openml.org ). Most other config files, like\n     database.php , will receive their values from\n     BASE_CONFIG.php . Other important config files are\n     autoload.php , determining which CodeIgniter / OpenML\n    files will be loaded on any request,  openML.php ,\n    containing config items specific to OpenML, and\n     routes.php , which will be explained in\n    [[URL Mapping]].    controllers : In the Model/View/Controller design pattern, all\n    user interaction goes through controllers. In a webapplication\n    setting this means that every time a URL gets requested, exactly one\n    controller gets invoked. The exact dynamics of this will be\n    explained in [[URL Mapping]].    core : A folder that contains CodeIgniter specific files. These\n    are not relevant for the understanding of OpenML.    helpers : This folder contains many convenience functions.\n    Wikipedia states: \u201cA convenience function is a non-essential\n    subroutine in a programming library or framework which is intended\n    to ease commonly performed tasks\u201d. For example the\n     file_upload_helper.php  contains many functions that\n    assist with uploading of files. Please note that a helper function\n    must be explicitly loaded in either the autoload config or the files\n    that uses its functions.    libraries : Similar to sparks, this folder contains libraries\n    specifically written for CodeIgniter. For example, the library used\n    for all user management routines is in this folder.    models : In the Model/View/Controller design pattern, models\n    represent the state of the system. In a webapplication setting, you\n    could say that a model is the link to the database. In OpenML,\n    almost all tables of the database are represented by a model. Each\n    model has general functionality applicable to all models (e.g.,\n    retrieve all records, retrieve record with constraints, insert\n    record) and functionality specific to that model (e.g., retrieve a\n    dataset that has certain data properties). Most models extend an\n    (abstract) base class, located in the  abstract  folder.\n    This way, all general functionality is programmed and maintained in\n    one place.    third_party : Although the name might suggests differently, this\n    folder contains all OpenML Java libraries.    views : In the Model/View/Controller design pattern, the views\n    are the way information is presented on the screen. In a\n    webapplication setting, a view usually is a block of (PHP generated)\n    HTML code. The most notable view is  frontend_main.php ,\n    which is the template file determining the main look and feel of\n    OpenML. Every single page also has its own specific view (which is\n    parsed within  frontend_main.php ). These pages can be\n    found (categorized by controller and name) in the  pages \n    folder. More about this structure is explained in\n    [[URL Mapping]].",
            "title": "openml_OS"
        },
        {
            "location": "/WebApp-(PHP)/#frontend",
            "text": "Architecture and libraries involved in generating the frontend functions.",
            "title": "Frontend"
        },
        {
            "location": "/WebApp-(PHP)/#code_1",
            "text": "https://github.com/openml/website/tree/master/openml_OS/views",
            "title": "Code"
        },
        {
            "location": "/WebApp-(PHP)/#high-level",
            "text": "All pages are generated by first loading  frontend_main.php . This creates the 'shell' in which the content is loaded. It loads all css and javascript libraries, and contains the html for displaying headers and footers.",
            "title": "High-level"
        },
        {
            "location": "/WebApp-(PHP)/#create-new-page",
            "text": "The preferred method is creating a new folder into the folder <root_directory>/openml_OS/views/pages/frontend \nThis page can be requested by http://www.openml.org/frontend/page/<folder_name> \nor just http://www.openml.org/<folder_name> \nThis method is preferred for human readable webpages, where the internal\nactions are simple, and the output is complex. We will describe the\nfiles that can be in this folder.    pre.php : Mandatory file. Will be executed first. Do not make\n    this file produce any output! Can be used to pre-render data, or set\n    some variables that are used in other files.    body.php : Highly recommended file. Intended for displaying the\n    main content of this file. Will be rendered at the right location\n    within the template file ( frontend_main.php ).    javascript.php : Non-mandatory file. Intended for javascript\n    function on which  body.php  relies. Will be rendered within a\n    javascript block in the header of the page.    post.php : Non mandatory file. Will only be executed when a POST\n    request is done (e.g., when a HTML form was send using the POST\n    protocol). Will be executed after  pre.php , but before the\n    rendering process (and thus, before  body.php  and\n     javascript.php ). Should handle the posted input, e.g., file\n    uploads.    It is also recommended to add the newly created folder to the mapping in\nthe  routes.php  config file. This way it can also be requested by the\nshortened version of the URL. (Note that we deliberately avoided to\nauto-load all pages into this file using a directory scan, as this makes\nthe webplatform slow. )  For more information, see [[URL Mapping]].",
            "title": "Create new page"
        },
        {
            "location": "/URL-Mapping/",
            "text": "URL to Page Mapping\n\u00b6\n\n\nMost pages in OpenML are represented by a folder in\n\n/openml_OS/views/pages/frontend\nThe contents of this folder will be parsed in the template\n\nfrontend_main.php\n template, as described in [[backend]]. In\nthis section we explain the way an URL is mapped to a certain OpenML\npage.\n\n\nURL Anatomy\n\u00b6\n\n\nBy default, CodeIgniter (and OpenML) accepts a URL in the following\nform:\n\nhttp://www.openml.org/index.php/<controller>/<function>/<p1>/<pN>/<free>\n\nThe various parts in the URL are divided by slashes. Every URL starts\nwith the protocol and server name (in the case of OpenML this is\n\nhttp://www.openml.org/\n). This is followed by the bootstrap file, which\nis always the same, i.e., \nindex.php\n. The next part indicates the\ncontroller that needs to be invoked; typically this is \nfrontend\n,\n\nrest_api\n or \ndata\n, but it can be any file from the \nopenml_OS\n folder\n\ncontrollers\n. Note that the suffix \n.php\n should not be included in the\nURL.\n\n\nThe next part indicates which function of the controller should be\ninvoked. This should be a existing, public function from the controller\nthat is indicated in the controller part. These functions might have one\nor more parameters that need to be set. This is the following part of\nthe URL (indicated by \np1\n and \npN\n). The parameters can be followed by\nanything in free format. Typically, this free format is used to pass on\nadditional parameters in \nname\n - \nvalue\n format, or just a way of\nadding a human readable string to the URL for SEO purposes.\n\n\nFor example, the following URL\n\nhttp://www.openml.org/index.php/frontend/page/home\n invokes\nthe function \npage\n from the \nfrontend\n controller and sets the only\nparameter of this function, \n$indicator\n, to value \nhome\n. The function\n\npage\n loads the content of the specified folder (\n$indicator\n) into the\nmain template. In this sense, the function \npage\n can be seen as some\nsort of specialized page loader.\n\n\nURL Shortening\n\u00b6\n\n\nSince it is good practice to have URL\u2019s as short as possible, we have\nintroduced some logic that shortens the URL\u2019s. Most importantly, the URL\npart that invokes \nindex.php\n can be removed at no cost, since this file\nis \nalways\n invoked. For this, we use Apache\u2019s rewrite engine. Rules\nfor rewriting URL\u2019s can be found in the \n.htaccess\n file, but is\nsuffices to say that any URL in the following format\n\nhttp://www.openml.org/index.php/<controller>/<function>/<params>\n\ncan due to the rewrite engine also be requested with\n\nhttp://www.openml.org/<controller>/<function>/<params>\n\n\nFurthermore, since most of the pages are invoked by the function \npage\n\nof the \nfrontend\n controller (hence, they come with the suffix\n\nfrontend/page/page_name\n) we also created a mapping that maps URL\u2019s in\nthe following form\n\nhttp://www.openml.org/<page_name>\n \nto\n\nhttp://www.openml.org/frontend/page/<page_name>\n\nNote that Apache\u2019s rewrite engine will also add \nindex.php\n to this. The\nexact mapping can be found in \nroutes.php\n config file.\n\n\nAdditional Mappings\n\u00b6\n\n\nAdditionally, a mapping is created from the following type of URL:\n\nhttp://www.openml.org/api/<any_query_string>\n\nto\n\nhttp://www.openml.org/rest_api/<any_query_string>\n\nThis was done for backwards compatibility. Many plugins make calls to\nthe not-existing \napi\n controller, which are automatically redirected to\nthe \nrest_api\n controller.\n\n\nExceptions\n\u00b6\n\n\nIt is important to note that not all pages do have a specific page\nfolder. The page folders are a good way of structuring complex GUI\u2019s\nthat need to be presented to the user, but in cases where the internal\nstate changes are more important than the GUI\u2019s, it might be preferable\nto make the controller function print the output directly. This happens\nfor example in the functions of \nrest_api.php\n and \nfree_query.php\n\n(although the former still has some files in the views folder that it\nrefers to).",
            "title": "URL Mapping"
        },
        {
            "location": "/URL-Mapping/#url-to-page-mapping",
            "text": "Most pages in OpenML are represented by a folder in /openml_OS/views/pages/frontend\nThe contents of this folder will be parsed in the template frontend_main.php  template, as described in [[backend]]. In\nthis section we explain the way an URL is mapped to a certain OpenML\npage.",
            "title": "URL to Page Mapping"
        },
        {
            "location": "/URL-Mapping/#url-anatomy",
            "text": "By default, CodeIgniter (and OpenML) accepts a URL in the following\nform: http://www.openml.org/index.php/<controller>/<function>/<p1>/<pN>/<free> \nThe various parts in the URL are divided by slashes. Every URL starts\nwith the protocol and server name (in the case of OpenML this is http://www.openml.org/ ). This is followed by the bootstrap file, which\nis always the same, i.e.,  index.php . The next part indicates the\ncontroller that needs to be invoked; typically this is  frontend , rest_api  or  data , but it can be any file from the  openml_OS  folder controllers . Note that the suffix  .php  should not be included in the\nURL.  The next part indicates which function of the controller should be\ninvoked. This should be a existing, public function from the controller\nthat is indicated in the controller part. These functions might have one\nor more parameters that need to be set. This is the following part of\nthe URL (indicated by  p1  and  pN ). The parameters can be followed by\nanything in free format. Typically, this free format is used to pass on\nadditional parameters in  name  -  value  format, or just a way of\nadding a human readable string to the URL for SEO purposes.  For example, the following URL http://www.openml.org/index.php/frontend/page/home  invokes\nthe function  page  from the  frontend  controller and sets the only\nparameter of this function,  $indicator , to value  home . The function page  loads the content of the specified folder ( $indicator ) into the\nmain template. In this sense, the function  page  can be seen as some\nsort of specialized page loader.",
            "title": "URL Anatomy"
        },
        {
            "location": "/URL-Mapping/#url-shortening",
            "text": "Since it is good practice to have URL\u2019s as short as possible, we have\nintroduced some logic that shortens the URL\u2019s. Most importantly, the URL\npart that invokes  index.php  can be removed at no cost, since this file\nis  always  invoked. For this, we use Apache\u2019s rewrite engine. Rules\nfor rewriting URL\u2019s can be found in the  .htaccess  file, but is\nsuffices to say that any URL in the following format http://www.openml.org/index.php/<controller>/<function>/<params> \ncan due to the rewrite engine also be requested with http://www.openml.org/<controller>/<function>/<params>  Furthermore, since most of the pages are invoked by the function  page \nof the  frontend  controller (hence, they come with the suffix frontend/page/page_name ) we also created a mapping that maps URL\u2019s in\nthe following form http://www.openml.org/<page_name>  \nto http://www.openml.org/frontend/page/<page_name> \nNote that Apache\u2019s rewrite engine will also add  index.php  to this. The\nexact mapping can be found in  routes.php  config file.",
            "title": "URL Shortening"
        },
        {
            "location": "/URL-Mapping/#additional-mappings",
            "text": "Additionally, a mapping is created from the following type of URL: http://www.openml.org/api/<any_query_string> \nto http://www.openml.org/rest_api/<any_query_string> \nThis was done for backwards compatibility. Many plugins make calls to\nthe not-existing  api  controller, which are automatically redirected to\nthe  rest_api  controller.",
            "title": "Additional Mappings"
        },
        {
            "location": "/URL-Mapping/#exceptions",
            "text": "It is important to note that not all pages do have a specific page\nfolder. The page folders are a good way of structuring complex GUI\u2019s\nthat need to be presented to the user, but in cases where the internal\nstate changes are more important than the GUI\u2019s, it might be preferable\nto make the controller function print the output directly. This happens\nfor example in the functions of  rest_api.php  and  free_query.php \n(although the former still has some files in the views folder that it\nrefers to).",
            "title": "Exceptions"
        },
        {
            "location": "/Java-App/",
            "text": "The Java App is used for a number of OpenML components, such as the ARFF parser and Evaluation engine, which depend on the Weka API. It is invoked from the OpenML API by means of a CLI interface. Typically, a call looks like this:\n\n\njava -jar webapplication.jar -config \"api_key=S3CR3T_AP1_K3Y\" -f evaluate_run -r 500\n\n\nWhich in this case executes the webapplication jar, invokes the function \"evaluate run\" and gives it parameter run id 500. The config parameter can be used to set some config items, in this case the api_key is mandatory. Every OpenML user has an api_key, which can be downloaded from their \nOpenML profile page\n. The response of this function is a call to the OpenML API uploading evaluation results to the OpenML database. Note that in this case the PHP website invokes the Java webapplication, which makes a call to the PHP website again, albeit another endpoint. \n\n\nThe webapplication does not have direct writing rights into the database. All communication to the database goes by means of the \nOpenML Connector\n, which communicates with the OpenML API. As a consequence, the webapplication could run on any system, i.e., there is no formal need for the webapplication to be on the same server as the website code. This is important, since this created modularity, and not all servers provide a command line interface to PHP scripts.\n\n\nAnother example is the following:\n\n\njava -jar webapplication -config \"api_key=S3CR3T_AP1_K3Y\" -f all_wrong -r 81,161 -t 59\n\n\nWhich takes a comma separated list of run ids (no spaces) and a task id as input and outputs the test examples on the dataset on which all algorithms used in the runs produced wrong examples (in this case, weka.BayesNet_K2 and weka.SMO, respectively). An error will be displayed if there are runs not consistent with the task id in there. \n\n\nExtending the Java App\n\u00b6\n\n\nThe bootstrap class of the webapplication is\n\n\norg.openml.webapplication.Main\n\n\nIt automatically checks authentication settings (such as api_key) and the determines which function to invoke. \n\n\nIt uses a switch-like if - else contruction to facilitate the functionalities of the various functions. Additional functions can be added to this freely. From there on, it is easy to add functionality to the webapplication. \n\n\nParameters are handled using the Apache Commons CommandLineParser class, which makes sure that the passed parameters are available to the program. \n\n\nIn order to make new functionalities available to the website, there also needs to be programmed an interface to the function, somewhere in the website. The next section details on that. \n\n\nInterfacing from the OpenML API\n\u00b6\n\n\nBy design, the REST API is not allowed to communicate with the Java App. All interfaces with the Java webapplication should go through other controllers of the PHP CodeIgniter framework., for example api_splits. Currently, the website features two main API's. These are represented by a Controller. Controllers can be found in the folder openml_OS/controllers. Here we see:\n* api_new.php, representing the REST API\n* api_splits.php, representing an API interfacing to the Java webapplication.",
            "title": "Java Backend"
        },
        {
            "location": "/Java-App/#extending-the-java-app",
            "text": "The bootstrap class of the webapplication is  org.openml.webapplication.Main  It automatically checks authentication settings (such as api_key) and the determines which function to invoke.   It uses a switch-like if - else contruction to facilitate the functionalities of the various functions. Additional functions can be added to this freely. From there on, it is easy to add functionality to the webapplication.   Parameters are handled using the Apache Commons CommandLineParser class, which makes sure that the passed parameters are available to the program.   In order to make new functionalities available to the website, there also needs to be programmed an interface to the function, somewhere in the website. The next section details on that.",
            "title": "Extending the Java App"
        },
        {
            "location": "/Java-App/#interfacing-from-the-openml-api",
            "text": "By design, the REST API is not allowed to communicate with the Java App. All interfaces with the Java webapplication should go through other controllers of the PHP CodeIgniter framework., for example api_splits. Currently, the website features two main API's. These are represented by a Controller. Controllers can be found in the folder openml_OS/controllers. Here we see:\n* api_new.php, representing the REST API\n* api_splits.php, representing an API interfacing to the Java webapplication.",
            "title": "Interfacing from the OpenML API"
        },
        {
            "location": "/Helper-functions/",
            "text": "Mostly written in Java, these functions build search indexes, compute dataset characteristics, generate tasks and evaluate the results of certain tasks.\n\n\nCode\n\u00b6\n\n\nThe Java code is available in the 'OpenML' repository: \nhttps://github.com/openml/OpenML/tree/master/Java\n\n\nComponents\n\u00b6\n\n\nGeneral:\n- \nOpenML\n: Building Lucene search index and smaller tools, e.g. extracting documentation from WEKA source files and ARFF files\n- \ngenerateApiDocs\n: Generates API HTML Documentation\n- \nhttp_post_file\n: Example how to post files to the api using Java.\n\n\nSupport for tasks:\n- \nfoldgeneration\n: Java code for generating cross-validation folds. Can be used from command line.\n- \nsplitgeneration\n: Split generator for cross validation and holdout. Unsure what's the difference with the previous?\n- \ngenerate_predictions\n: Helper class to build prediction files based on WEKA output. Move to WEKA repository?\n- \nevaluate_predictions\n: The evaluation engine computing evaluation scores based on submitted predictions",
            "title": "Helper functions"
        },
        {
            "location": "/Helper-functions/#code",
            "text": "The Java code is available in the 'OpenML' repository:  https://github.com/openml/OpenML/tree/master/Java",
            "title": "Code"
        },
        {
            "location": "/Helper-functions/#components",
            "text": "General:\n-  OpenML : Building Lucene search index and smaller tools, e.g. extracting documentation from WEKA source files and ARFF files\n-  generateApiDocs : Generates API HTML Documentation\n-  http_post_file : Example how to post files to the api using Java.  Support for tasks:\n-  foldgeneration : Java code for generating cross-validation folds. Can be used from command line.\n-  splitgeneration : Split generator for cross validation and holdout. Unsure what's the difference with the previous?\n-  generate_predictions : Helper class to build prediction files based on WEKA output. Move to WEKA repository?\n-  evaluate_predictions : The evaluation engine computing evaluation scores based on submitted predictions",
            "title": "Components"
        }
    ]
}